{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 113558,
          "databundleVersionId": 14878066,
          "sourceType": "competition"
        },
        {
          "sourceId": 14407528,
          "sourceType": "datasetVersion",
          "datasetId": 9153851
        },
        {
          "sourceId": 4534,
          "sourceType": "modelInstanceVersion",
          "modelInstanceId": 3326,
          "modelId": 986
        }
      ],
      "dockerImageVersionId": 31234,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "papermill": {
      "default_parameters": {},
      "duration": 4803.853475,
      "end_time": "2026-01-04T03:07:58.189377",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2026-01-04T01:47:54.335902",
      "version": "2.6.0"
    },
    "colab": {
      "name": "Scientific-Forensics-DINOv2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "86ef3cf9c99c47a3b62d30eb032eaa02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e65f469d47fa474b98e1d4b9e8f70622",
              "IPY_MODEL_222b8a86f7a545c6b914e793235ebff3",
              "IPY_MODEL_2792d9dff5da4fee84ede7e9c93725f5",
              "IPY_MODEL_bb2b970b24854fd98370dba937c76403",
              "IPY_MODEL_0a65c3370f8048a68de657a1c6ec9732"
            ],
            "layout": "IPY_MODEL_b46c1583be6e49da82315715deb40c44"
          }
        },
        "e65f469d47fa474b98e1d4b9e8f70622": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0db2fc41e77d420897578c56909563bb",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_6d5194394d594a0c8620a10852da45f6",
            "value": "<center> <img\nsrc=https://www.kaggle.com/static/images/site-logo.png\nalt='Kaggle'> <br> Create an API token from <a\nhref=\"https://www.kaggle.com/settings/account\" target=\"_blank\">your Kaggle\nsettings page</a> and paste it below along with your Kaggle username. <br> </center>"
          }
        },
        "222b8a86f7a545c6b914e793235ebff3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "Username:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_ecd9948cd9214012b42a15ea91e789a3",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_5bde63b3e8ef45f8802dcac5c08f11c2",
            "value": ""
          }
        },
        "2792d9dff5da4fee84ede7e9c93725f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_7359266d848742c68f7aef70856a6b5b",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_cf54a06e16b34308a8ae8fb4c134b1b0",
            "value": ""
          }
        },
        "bb2b970b24854fd98370dba937c76403": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_03b70c404a3e43dd894a8f0e97afc6ae",
            "style": "IPY_MODEL_c8cccf4107544a4b808f1c6ca5b0cafe",
            "tooltip": ""
          }
        },
        "0a65c3370f8048a68de657a1c6ec9732": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_399114883067423f8e23f4082db18cae",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_0edbc71064c340e4815190f49b917fda",
            "value": "\n<b>Thank You</b></center>"
          }
        },
        "b46c1583be6e49da82315715deb40c44": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "0db2fc41e77d420897578c56909563bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d5194394d594a0c8620a10852da45f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ecd9948cd9214012b42a15ea91e789a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5bde63b3e8ef45f8802dcac5c08f11c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7359266d848742c68f7aef70856a6b5b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf54a06e16b34308a8ae8fb4c134b1b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "03b70c404a3e43dd894a8f0e97afc6ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8cccf4107544a4b808f1c6ca5b0cafe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "399114883067423f8e23f4082db18cae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0edbc71064c340e4815190f49b917fda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samyakrajbayar/16-9-Image-Converter/blob/main/Scientific_Forensics_DINOv2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340,
          "referenced_widgets": [
            "86ef3cf9c99c47a3b62d30eb032eaa02",
            "e65f469d47fa474b98e1d4b9e8f70622",
            "222b8a86f7a545c6b914e793235ebff3",
            "2792d9dff5da4fee84ede7e9c93725f5",
            "bb2b970b24854fd98370dba937c76403",
            "0a65c3370f8048a68de657a1c6ec9732",
            "b46c1583be6e49da82315715deb40c44",
            "0db2fc41e77d420897578c56909563bb",
            "6d5194394d594a0c8620a10852da45f6",
            "ecd9948cd9214012b42a15ea91e789a3",
            "5bde63b3e8ef45f8802dcac5c08f11c2",
            "7359266d848742c68f7aef70856a6b5b",
            "cf54a06e16b34308a8ae8fb4c134b1b0",
            "03b70c404a3e43dd894a8f0e97afc6ae",
            "c8cccf4107544a4b808f1c6ca5b0cafe",
            "399114883067423f8e23f4082db18cae",
            "0edbc71064c340e4815190f49b917fda"
          ]
        },
        "id": "aooYS7254ZsS",
        "outputId": "c6e92678-4245-4c8b-bb59-a7ed0ece368c"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://www.kaggle.com/static/images/site-logo.png\\nalt=\\'Kaggleâ€¦"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "86ef3cf9c99c47a3b62d30eb032eaa02"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": 7
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "recodai_luc_scientific_image_forgery_detection_path = kagglehub.competition_download('recodai-luc-scientific-image-forgery-detection')\n",
        "pankajiitr_cnndinov2_pbd_path = kagglehub.dataset_download('pankajiitr/cnndinov2-pbd')\n",
        "metaresearch_dinov2_pytorch_base_1_path = kagglehub.model_download('metaresearch/dinov2/PyTorch/base/1')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 758
        },
        "id": "m63MTQec4ZsV",
        "outputId": "ba32c28b-9aa9-4eda-c067-6a10a49f59bb"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "error",
          "ename": "KaggleApiHTTPError",
          "evalue": "401 Client Error.\n\nYou don't have permission to access resource at URL: https://www.kaggle.com/competitions/recodai-luc-scientific-image-forgery-detection\nPlease make sure you are authenticated and have accepted the competition rules which can be found at this location: https://www.kaggle.com/competitions/recodai-luc-scientific-image-forgery-detection/rules",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/kagglehub/exceptions.py\u001b[0m in \u001b[0;36mkaggle_api_raise_for_status\u001b[0;34m(response, resource_handle)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://www.kaggle.com/api/v1/competitions/data/download-all/recodai-luc-scientific-image-forgery-detection",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKaggleApiHTTPError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1863615656.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# NOTEBOOK.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mrecodai_luc_scientific_image_forgery_detection_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkagglehub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompetition_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'recodai-luc-scientific-image-forgery-detection'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mpankajiitr_cnndinov2_pbd_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkagglehub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pankajiitr/cnndinov2-pbd'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmetaresearch_dinov2_pytorch_base_1_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkagglehub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'metaresearch/dinov2/PyTorch/base/1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/kagglehub/competition.py\u001b[0m in \u001b[0;36mcompetition_download\u001b[0;34m(handle, path, force_download)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_competition_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Downloading competition: {h.to_url()} ...\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mEXTRA_CONSOLE_BLOCK\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompetition_resolver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_download\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/kagglehub/registry.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mimpl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_impls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mimpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_supported\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mimpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0mfails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/kagglehub/resolver.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, handle, path, force_download)\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mSome\u001b[0m \u001b[0mcases\u001b[0m \u001b[0mwhere\u001b[0m \u001b[0mversion\u001b[0m \u001b[0mnumber\u001b[0m \u001b[0mmight\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCompetition\u001b[0m \u001b[0mdatasource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAPI\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mbased\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \"\"\"\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_resolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_download\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# Note handles are immutable, so _resolve() could not have altered our reference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/kagglehub/http_resolver.py\u001b[0m in \u001b[0;36m_resolve\u001b[0;34m(self, h, path, force_download)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                 \u001b[0mdownload_needed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marchive_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcached_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcached_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/kagglehub/clients.py\u001b[0m in \u001b[0;36mdownload_file\u001b[0;34m(self, path, out_file, resource_handle, cached_path, extract_auto_compressed_file)\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEFAULT_CONNECT_TIMEOUT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEFAULT_READ_TIMEOUT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         ) as response:\n\u001b[0;32m--> 182\u001b[0;31m             \u001b[0mkaggle_api_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresource_handle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mtotal_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Content-Length\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"Content-Length\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/kagglehub/exceptions.py\u001b[0m in \u001b[0;36mkaggle_api_raise_for_status\u001b[0;34m(response, resource_handle)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;31m# Default handling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mKaggleApiHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKaggleApiHTTPError\u001b[0m: 401 Client Error.\n\nYou don't have permission to access resource at URL: https://www.kaggle.com/competitions/recodai-luc-scientific-image-forgery-detection\nPlease make sure you are authenticated and have accepted the competition rules which can be found at this location: https://www.kaggle.com/competitions/recodai-luc-scientific-image-forgery-detection/rules"
          ]
        }
      ],
      "execution_count": 8
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "YbUqkuVT48OK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, cv2, json, math, random, torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn, torch.nn.functional as F, torch.optim as optim\n",
        "from transformers import AutoImageProcessor, AutoModel\n",
        "\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    # This forces CUDA to use deterministic algorithms (slower but consistent)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed_everything(42)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "BASE_DIR  = \"/kaggle/input/recodai-luc-scientific-image-forgery-detection\"\n",
        "AUTH_DIR  = f\"{BASE_DIR}/train_images/authentic\"\n",
        "FORG_DIR  = f\"{BASE_DIR}/train_images/forged\"\n",
        "MASK_DIR  = f\"{BASE_DIR}/train_masks\"\n",
        "TEST_DIR  = f\"{BASE_DIR}/test_images\"\n",
        "DINO_PATH = \"/kaggle/input/dinov2/pytorch/base/1\"\n",
        "\n",
        "IMG_SIZE = 518\n",
        "BATCH_SIZE = 2\n",
        "# MODEL_LOC = '/kaggle/input/cnndinov2-pbd/CNNDINOv2-U54/CNNDINOv2-U54/model_seg_final.pt'  # 0.310\n",
        "# MODEL_LOC = '/kaggle/input/cnndinov2-pbd/CNNDINOv2-U52/CNNDINOv2-U52/model_seg_final.pt'  # 0.321\n",
        "MODEL_LOC = '/kaggle/input/cnndinov2-pbd/CNNDINOv2-U52/CNNDINOv2-U52/model_seg_final.pt'  # 0.321\n",
        "\n",
        "# INFERENCE UTILS\n",
        "AREA_THR = 200\n",
        "MEAN_THR = 0.22\n",
        "USE_TTA = True\n",
        "GRID_SEARCH = True\n",
        "\n",
        "class ForgerySegDataset(Dataset):\n",
        "    def __init__(self, auth_paths, forg_paths, mask_dir, img_size=IMG_SIZE):\n",
        "        self.samples = []\n",
        "        for p in forg_paths:\n",
        "            m = os.path.join(mask_dir, Path(p).stem + \".npy\")\n",
        "            if os.path.exists(m):\n",
        "                self.samples.append((p, m))\n",
        "        for p in auth_paths:\n",
        "            self.samples.append((p, None))\n",
        "        self.img_size = img_size\n",
        "    def __len__(self): return len(self.samples)\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, mask_path = self.samples[idx]\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        w, h = img.size\n",
        "        if mask_path is None:\n",
        "            mask = np.zeros((h, w), np.uint8)\n",
        "        else:\n",
        "            m = np.load(mask_path)\n",
        "            if m.ndim == 3: m = np.max(m, axis=0)\n",
        "            mask = (m > 0).astype(np.uint8)\n",
        "        img_r = img.resize((IMG_SIZE, IMG_SIZE))\n",
        "        mask_r = cv2.resize(mask, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_NEAREST)\n",
        "        img_t = torch.from_numpy(np.array(img_r, np.float32)/255.).permute(2,0,1)\n",
        "        mask_t = torch.from_numpy(mask_r[None, ...].astype(np.float32))\n",
        "        return img_t, mask_t\n",
        "\n",
        "\n",
        "#  MODEL (DINOv2 + Decoder)\n",
        "\n",
        "from transformers import AutoImageProcessor, AutoModel\n",
        "processor = AutoImageProcessor.from_pretrained(DINO_PATH, local_files_only=True, use_fast=False)\n",
        "encoder = AutoModel.from_pretrained(DINO_PATH, local_files_only=True).eval().to(device)\n",
        "\n",
        "class DinoTinyDecoder(nn.Module):\n",
        "    def __init__(self, in_ch=768, out_ch=1):\n",
        "        super().__init__()\n",
        "        # Block 1: 768 -> 384\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, 384, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout2d(0.1)\n",
        "        )\n",
        "        # Block 2: 384 -> 192\n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.Conv2d(384, 192, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout2d(0.1)\n",
        "        )\n",
        "        # Block 3: 192 -> 96\n",
        "        self.block3 = nn.Sequential(\n",
        "            nn.Conv2d(192, 96, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        # Final Output: 96 -> 1\n",
        "        self.conv_out = nn.Conv2d(96, out_ch, kernel_size=1)\n",
        "\n",
        "    def forward(self, f, target_size):\n",
        "        # f: [B, 768, 37, 37]\n",
        "\n",
        "        # Step 1: Up to ~74x74\n",
        "        x = F.interpolate(self.block1(f), size=(74, 74), mode='bilinear', align_corners=False)\n",
        "\n",
        "        # Step 2: Up to ~148x148\n",
        "        x = F.interpolate(self.block2(x), size=(148, 148), mode='bilinear', align_corners=False)\n",
        "\n",
        "        # Step 3: Up to ~296x296\n",
        "        x = F.interpolate(self.block3(x), size=(296, 296), mode='bilinear', align_corners=False)\n",
        "\n",
        "        # Step 4: Final jump to 518x518\n",
        "        x = self.conv_out(x)\n",
        "        x = F.interpolate(x, size=target_size, mode='bilinear', align_corners=False)\n",
        "\n",
        "        return x\n",
        "\n",
        "class DinoSegmenter(nn.Module):\n",
        "    def __init__(self, encoder, processor):\n",
        "        super().__init__()\n",
        "        self.encoder, self.processor = encoder, processor\n",
        "        for p in self.encoder.parameters(): p.requires_grad = False\n",
        "        self.seg_head = DinoTinyDecoder(768,1)\n",
        "    def forward_features(self,x):\n",
        "        imgs = (x*255).clamp(0,255).byte().permute(0,2,3,1).cpu().numpy()\n",
        "        inputs = self.processor(images=list(imgs), return_tensors=\"pt\").to(x.device)\n",
        "        # with torch.no_grad():\n",
        "        #     feats = self.encoder(**inputs).last_hidden_state\n",
        "        feats = self.encoder(**inputs).last_hidden_state\n",
        "        B,N,C = feats.shape\n",
        "        fmap = feats[:,1:,:].permute(0,2,1)\n",
        "        s = int(math.sqrt(N-1))\n",
        "        fmap = fmap.reshape(B,C,s,s)\n",
        "        return fmap\n",
        "    def forward_seg(self,x):\n",
        "        fmap = self.forward_features(x)\n",
        "        return self.seg_head(fmap,(IMG_SIZE,IMG_SIZE))\n",
        "\n",
        "\n",
        "auth_imgs = sorted([str(Path(AUTH_DIR)/f) for f in os.listdir(AUTH_DIR)])\n",
        "forg_imgs = sorted([str(Path(FORG_DIR)/f) for f in os.listdir(FORG_DIR)])\n",
        "train_auth, val_auth = train_test_split(auth_imgs, test_size=0.2, random_state=42)\n",
        "train_forg, val_forg = train_test_split(forg_imgs, test_size=0.2, random_state=42)\n",
        "\n",
        "train_loader = DataLoader(ForgerySegDataset(train_auth, train_forg, MASK_DIR),\n",
        "                          batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(ForgerySegDataset(val_auth, val_forg, MASK_DIR),\n",
        "                        batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "model_seg = DinoSegmenter(encoder, processor).to(device)\n",
        "\n",
        "# Load pretrained weights if MODEL_LOC is specified\n",
        "if MODEL_LOC is not None and os.path.exists(MODEL_LOC):\n",
        "    model_seg.load_state_dict(torch.load(MODEL_LOC, map_location=device))\n",
        "    print(f\"âœ… Loaded pretrained model from: {MODEL_LOC}\")\n",
        "    model_seg.eval()  # Set model to evaluation mode\n",
        "\n",
        "@torch.no_grad()\n",
        "def segment_prob_map(pil):\n",
        "    x = torch.from_numpy(np.array(pil.resize((IMG_SIZE, IMG_SIZE)), np.float32)/255.).permute(2,0,1)[None].to(device)\n",
        "    prob = torch.sigmoid(model_seg.forward_seg(x))[0,0].cpu().numpy()\n",
        "    return prob\n",
        "\n",
        "@torch.no_grad()\n",
        "def segment_prob_map_with_tta(pil):\n",
        "    # 1. Preprocessing: Resize, Normalize, and move to Device\n",
        "    x = torch.from_numpy(np.array(pil.resize((IMG_SIZE, IMG_SIZE)), np.float32)/255.).permute(2,0,1)[None].to(device)\n",
        "\n",
        "    predictions = []\n",
        "\n",
        "    # 2. Original Prediction\n",
        "    pred_orig = torch.sigmoid(model_seg.forward_seg(x))\n",
        "    predictions.append(pred_orig)\n",
        "\n",
        "    # 3. Horizontal Flip TTA (dim 3)\n",
        "    # Flip input -> Predict -> Flip output back\n",
        "    pred_h = torch.sigmoid(model_seg.forward_seg(torch.flip(x, dims=[3])))\n",
        "    predictions.append(torch.flip(pred_h, dims=[3]))\n",
        "\n",
        "    # 4. Vertical Flip TTA (dim 2)\n",
        "    # Flip input -> Predict -> Flip output back\n",
        "    pred_v = torch.sigmoid(model_seg.forward_seg(torch.flip(x, dims=[2])))\n",
        "    predictions.append(torch.flip(pred_v, dims=[2]))\n",
        "\n",
        "    # 5. Average the predictions and format as numpy\n",
        "    # We stack the 3 predictions and take the mean across the stack dimension (0)\n",
        "    prob = torch.stack(predictions).mean(0)[0, 0].cpu().numpy()\n",
        "\n",
        "    return prob\n",
        "\n",
        "def enhanced_adaptive_mask(prob, alpha_grad=0.45):\n",
        "    gx = cv2.Sobel(prob, cv2.CV_32F, 1, 0, ksize=3)\n",
        "    gy = cv2.Sobel(prob, cv2.CV_32F, 0, 1, ksize=3)\n",
        "    grad_mag = np.sqrt(gx**2 + gy**2)\n",
        "    grad_norm = grad_mag / (grad_mag.max() + 1e-6)\n",
        "    enhanced = (1 - alpha_grad) * prob + alpha_grad * grad_norm\n",
        "    enhanced = cv2.GaussianBlur(enhanced, (3,3), 0)\n",
        "    thr = np.mean(enhanced) + 0.3 * np.std(enhanced)\n",
        "    mask = (enhanced > thr).astype(np.uint8)\n",
        "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, np.ones((5,5), np.uint8))\n",
        "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, np.ones((3,3), np.uint8))\n",
        "    return mask, thr\n",
        "\n",
        "def finalize_mask(prob, orig_size):\n",
        "    mask, thr = enhanced_adaptive_mask(prob)\n",
        "    mask = cv2.resize(mask, orig_size, interpolation=cv2.INTER_NEAREST)\n",
        "    return mask, thr\n",
        "\n",
        "def pipeline_final(pil):\n",
        "    if USE_TTA:\n",
        "        prob = segment_prob_map_with_tta(pil)\n",
        "    else:\n",
        "        prob = segment_prob_map(pil)\n",
        "    mask, thr = finalize_mask(prob, pil.size)\n",
        "    area = int(mask.sum())\n",
        "    mean_inside = float(prob[cv2.resize(mask,(IMG_SIZE,IMG_SIZE),interpolation=cv2.INTER_NEAREST)==1].mean()) if area>0 else 0.0\n",
        "    if area < AREA_THR or mean_inside < MEAN_THR:\n",
        "        return \"authentic\", None, {\"area\": area, \"mean_inside\": mean_inside, \"thr\": thr}\n",
        "    return \"forged\", mask, {\"area\": area, \"mean_inside\": mean_inside, \"thr\": thr}\n",
        "\n",
        "import itertools\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def grid_search_area_mean(forg_paths, auth_paths, mask_dir):\n",
        "    mean_range = [round(x, 2) for x in np.arange(0.20, 0.291, 0.01)]\n",
        "    area_range = [200]\n",
        "    # 1. Use ALL images from both paths to maximize robustness\n",
        "    val_set = [(p, \"forged\") for p in forg_paths] + [(p, \"authentic\") for p in auth_paths]\n",
        "\n",
        "    print(f\"ðŸš€ Step 1: Caching probability maps for ALL {len(val_set)} images...\")\n",
        "    cache = []\n",
        "    for p, label in tqdm(val_set):\n",
        "        pil = Image.open(p).convert(\"RGB\")\n",
        "        w, h = pil.size\n",
        "\n",
        "        # Get raw probability map\n",
        "        prob = segment_prob_map_with_tta(pil) if USE_TTA else segment_prob_map(pil)\n",
        "\n",
        "        # USE OLD MASK LOGIC: mean + 0.3*std\n",
        "        mask_raw, _ = enhanced_adaptive_mask(prob) # Your function using np.mean + 0.3*np.std\n",
        "        mask_resized = cv2.resize(mask_raw, (w, h), interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "        # Handle Ground Truth\n",
        "        if label == \"forged\":\n",
        "            m_gt = np.load(Path(mask_dir)/f\"{Path(p).stem}.npy\")\n",
        "            if m_gt.ndim == 3: m_gt = np.max(m_gt, axis=0)\n",
        "            m_gt = (m_gt > 0).astype(np.uint8)\n",
        "        else:\n",
        "            m_gt = np.zeros((h, w), np.uint8) # Authentic = blank GT\n",
        "\n",
        "        cache.append({\"prob\": prob, \"mask\": mask_resized, \"gt\": m_gt, \"label\": label})\n",
        "\n",
        "    # 2. Sweep thresholds\n",
        "    best_f1 = -1\n",
        "    best_params = {}\n",
        "    combinations = list(itertools.product(area_range, mean_range))\n",
        "\n",
        "    for a_thr, m_thr in combinations:\n",
        "        current_f1s = []\n",
        "        for item in cache:\n",
        "            mask = item[\"mask\"]\n",
        "            area = int(mask.sum()) # OLD AREA LOGIC\n",
        "\n",
        "            # OLD MEAN LOGIC\n",
        "            mask_small = cv2.resize(mask, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_NEAREST)\n",
        "            mean_in = float(item[\"prob\"][mask_small == 1].mean()) if area > 0 else 0.0\n",
        "\n",
        "            # Pipeline decision\n",
        "            is_forged = (area >= a_thr and mean_in >= m_thr)\n",
        "            m_pred = (mask > 0).astype(np.uint8) if is_forged else np.zeros_like(item[\"gt\"])\n",
        "\n",
        "            # F1 Calculation (Authentic silence = 1.0, noisy prediction = 0.0)\n",
        "            f1 = f1_score(item[\"gt\"].flatten(), m_pred.flatten(),\n",
        "                          zero_division=1 if item[\"label\"] == \"authentic\" else 0)\n",
        "            current_f1s.append(f1)\n",
        "\n",
        "        avg_f1 = np.mean(current_f1s)\n",
        "        if avg_f1 > best_f1:\n",
        "            best_f1 = avg_f1\n",
        "            best_params = {\"AREA_THR\": a_thr, \"MEAN_THR\": m_thr}\n",
        "            print(f\"â­ New Best F1: {best_f1:.4f} -> AREA: {a_thr}, MEAN: {m_thr}\")\n",
        "\n",
        "    return best_params\n",
        "\n",
        "if GRID_SEARCH:\n",
        "    best_cfg = grid_search_area_mean(val_forg, val_auth, MASK_DIR)\n",
        "    AREA_THR = best_cfg['AREA_THR']\n",
        "    MEAN_THR = best_cfg['MEAN_THR']\n",
        "\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "val_items = [(p, 1) for p in val_forg[:10]]\n",
        "results = []\n",
        "for p,_ in tqdm(val_items, desc=\"Validation forged-only\"):\n",
        "    pil = Image.open(p).convert(\"RGB\")\n",
        "    label, m_pred, dbg = pipeline_final(pil)\n",
        "    m_gt = np.load(Path(MASK_DIR)/f\"{Path(p).stem}.npy\")\n",
        "    if m_gt.ndim==3: m_gt=np.max(m_gt,axis=0)\n",
        "    m_gt=(m_gt>0).astype(np.uint8)\n",
        "    m_pred=(m_pred>0).astype(np.uint8) if m_pred is not None else np.zeros_like(m_gt)\n",
        "    f1 = f1_score(m_gt.flatten(), m_pred.flatten(), zero_division=0)\n",
        "    results.append((Path(p).stem, f1, dbg))\n",
        "print(\"\\n F1-score par image falsifiÃ©e:\\n\")\n",
        "for cid,f1,dbg in results:\n",
        "    print(f\"{cid} â€” F1={f1:.4f} | area={dbg['area']} mean={dbg['mean_inside']:.3f} thr={dbg['thr']:.3f}\")\n",
        "print(f\"\\n Moyenne F1 (falsifiÃ©es) = {np.mean([r[1] for r in results]):.4f}\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-04T01:47:59.539237Z",
          "iopub.status.busy": "2026-01-04T01:47:59.538554Z",
          "iopub.status.idle": "2026-01-04T03:07:41.66768Z",
          "shell.execute_reply": "2026-01-04T03:07:41.666621Z"
        },
        "papermill": {
          "duration": 4782.135448,
          "end_time": "2026-01-04T03:07:41.669605",
          "exception": false,
          "start_time": "2026-01-04T01:47:59.534157",
          "status": "completed"
        },
        "tags": [],
        "id": "HLCSDp1l4ZsW"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os, json, cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- RLE Encoder for Kaggle Submission ---\n",
        "def rle_encode(mask: np.ndarray, fg_val: int = 1) -> str:\n",
        "    pixels = mask.T.flatten()\n",
        "    dots = np.where(pixels == fg_val)[0]\n",
        "    if len(dots) == 0:\n",
        "        return \"authentic\"\n",
        "    run_lengths = []\n",
        "    prev = -2\n",
        "    for b in dots:\n",
        "        if b > prev + 1:\n",
        "            run_lengths.extend((b + 1, 0))\n",
        "        run_lengths[-1] += 1\n",
        "        prev = b\n",
        "    return json.dumps([int(x) for x in run_lengths])\n",
        "\n",
        "# --- Paths ---\n",
        "TEST_DIR = \"/kaggle/input/recodai-luc-scientific-image-forgery-detection/test_images\"\n",
        "SAMPLE_SUB = \"/kaggle/input/recodai-luc-scientific-image-forgery-detection/sample_submission.csv\"\n",
        "OUT_PATH = \"submission.csv\"\n",
        "\n",
        "rows = []\n",
        "for f in tqdm(sorted(os.listdir(TEST_DIR)), desc=\"Inference on Test Set\"):\n",
        "    pil = Image.open(Path(TEST_DIR)/f).convert(\"RGB\")\n",
        "    label, mask, dbg = pipeline_final(pil)  # utilise la version amÃ©liorÃ©e\n",
        "\n",
        "    # SÃ©curisation masque\n",
        "    if mask is None:\n",
        "        mask = np.zeros(pil.size[::-1], np.uint8)\n",
        "    else:\n",
        "        mask = np.array(mask, dtype=np.uint8)\n",
        "\n",
        "    # Annotation finale\n",
        "    if label == \"authentic\":\n",
        "        annot = \"authentic\"\n",
        "    else:\n",
        "        annot = rle_encode((mask > 0).astype(np.uint8))\n",
        "\n",
        "    rows.append({\n",
        "        \"case_id\": Path(f).stem,\n",
        "        \"annotation\": annot,\n",
        "        \"area\": int(dbg.get(\"area\", mask.sum())),\n",
        "        \"mean\": float(dbg.get(\"mean_inside\", 0.0)),\n",
        "        \"thr\": float(dbg.get(\"thr\", 0.0))\n",
        "    })\n",
        "\n",
        "\n",
        "sub = pd.DataFrame(rows)\n",
        "ss = pd.read_csv(SAMPLE_SUB)\n",
        "ss[\"case_id\"] = ss[\"case_id\"].astype(str)\n",
        "sub[\"case_id\"] = sub[\"case_id\"].astype(str)\n",
        "final = ss[[\"case_id\"]].merge(sub, on=\"case_id\", how=\"left\")\n",
        "final[\"annotation\"] = final[\"annotation\"].fillna(\"authentic\")\n",
        "final[[\"case_id\", \"annotation\"]].to_csv(OUT_PATH, index=False)\n",
        "\n",
        "print(f\"\\nâœ… Saved submission file: {OUT_PATH}\")\n",
        "print(final.head(10))\n",
        "\n",
        "\n",
        "sample_files = sorted(os.listdir(TEST_DIR))[:5]\n",
        "for f in sample_files:\n",
        "    pil = Image.open(Path(TEST_DIR)/f).convert(\"RGB\")\n",
        "    label, mask, dbg = pipeline_final(pil)\n",
        "    mask = np.array(mask, dtype=np.uint8) if mask is not None else np.zeros(pil.size[::-1], np.uint8)\n",
        "\n",
        "    print(f\"{'ðŸ”´' if label=='forged' else 'ðŸŸ¢'} {f}: {label} | area={mask.sum()} mean={dbg.get('mean_inside', 0):.3f}\")\n",
        "\n",
        "    if label == \"authentic\":\n",
        "        plt.figure(figsize=(5,5))\n",
        "        plt.imshow(pil)\n",
        "        plt.title(f\"{f} â€” Authentic\")\n",
        "        plt.axis(\"off\")\n",
        "        plt.show()\n",
        "    else:\n",
        "        plt.figure(figsize=(10,5))\n",
        "        plt.subplot(1,2,1)\n",
        "        plt.imshow(pil)\n",
        "        plt.title(\"Original Image\")\n",
        "        plt.axis(\"off\")\n",
        "        plt.subplot(1,2,2)\n",
        "        plt.imshow(pil)\n",
        "        plt.imshow(mask, alpha=0.45, cmap=\"Reds\")\n",
        "        plt.title(f\"Predicted Forged Mask\\nArea={mask.sum()} | Mean={dbg.get('mean_inside', 0):.3f}\")\n",
        "        plt.axis(\"off\")\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-04T03:07:41.875556Z",
          "iopub.status.busy": "2026-01-04T03:07:41.875183Z",
          "iopub.status.idle": "2026-01-04T03:07:44.024852Z",
          "shell.execute_reply": "2026-01-04T03:07:44.023891Z"
        },
        "papermill": {
          "duration": 2.205312,
          "end_time": "2026-01-04T03:07:44.02708",
          "exception": false,
          "start_time": "2026-01-04T03:07:41.821768",
          "status": "completed"
        },
        "tags": [],
        "id": "Go4fumPR4ZsY"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch, cv2, math, numpy as np, matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Use global IMG_SIZE from cell 2 (518)\n",
        "\n",
        "# 1ï¸ Predict probability map (from model)\n",
        "@torch.no_grad()\n",
        "def predict_prob_map(pil):\n",
        "    \"\"\"Return DINOv2 segmentation probability map [0,1].\"\"\"\n",
        "    img = pil.resize((IMG_SIZE, IMG_SIZE))\n",
        "    x = torch.from_numpy(np.array(img, np.float32) / 255.).permute(2, 0, 1)[None].to(device)\n",
        "    logits = model_seg.forward_seg(x)\n",
        "    prob = torch.sigmoid(logits)[0, 0].cpu().numpy()\n",
        "    return prob\n",
        "\n",
        "\n",
        "# 2ï¸ Post-processing consistent with pipeline_final\n",
        "def adaptive_mask(prob, alpha_grad=0.35):\n",
        "    \"\"\"Adaptive enhancement + morphological refinement.\"\"\"\n",
        "    gx = cv2.Sobel(prob, cv2.CV_32F, 1, 0, ksize=3)\n",
        "    gy = cv2.Sobel(prob, cv2.CV_32F, 0, 1, ksize=3)\n",
        "    grad_mag = np.sqrt(gx**2 + gy**2)\n",
        "    grad_norm = grad_mag / (grad_mag.max() + 1e-6)\n",
        "\n",
        "    enhanced = (1 - alpha_grad) * prob + alpha_grad * grad_norm\n",
        "    enhanced = cv2.GaussianBlur(enhanced, (3, 3), 0)\n",
        "\n",
        "    thr = np.mean(enhanced) + 0.3 * np.std(enhanced)\n",
        "    mask = (enhanced > thr).astype(np.uint8)\n",
        "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, np.ones((5, 5), np.uint8))\n",
        "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, np.ones((3, 3), np.uint8))\n",
        "    return mask, float(thr)\n",
        "\n",
        "\n",
        "# 3ï¸ Unified visualization pipeline (uses same filtering logic as pipeline_final)\n",
        "def pipeline_visual(pil):\n",
        "    if USE_TTA:\n",
        "        prob = segment_prob_map_with_tta(pil)\n",
        "    else:\n",
        "        prob = predict_prob_map(pil)\n",
        "    mask, thr = adaptive_mask(prob)\n",
        "    area = int(mask.sum())\n",
        "    mean_inside = float(prob[mask == 1].mean()) if area > 0 else 0.0\n",
        "\n",
        "    # âœ… FIXED: Use same decision rule as pipeline_final for consistency\n",
        "    if area < AREA_THR or mean_inside < MEAN_THR:\n",
        "        label = \"authentic\"\n",
        "    else:\n",
        "        label = \"forged\"\n",
        "    return label, mask, thr, area, mean_inside\n",
        "\n",
        "\n",
        "# 4ï¸ Visualization (for validation forged samples)\n",
        "sample_forged = val_forg[:5]\n",
        "n = len(sample_forged)\n",
        "fig, axes = plt.subplots(n, 3, figsize=(12, n * 3))\n",
        "if n == 1:\n",
        "    axes = np.expand_dims(axes, axis=0)\n",
        "\n",
        "for i, p in enumerate(sample_forged):\n",
        "    pil = Image.open(p).convert(\"RGB\")\n",
        "    label, m_pred, thr, area, mean = pipeline_visual(pil)\n",
        "\n",
        "    # Ground Truth mask\n",
        "    m_gt = np.load(Path(MASK_DIR)/f\"{Path(p).stem}.npy\")\n",
        "    if m_gt.ndim == 3: m_gt = np.max(m_gt, axis=0)\n",
        "    m_gt = (m_gt > 0).astype(np.uint8)\n",
        "\n",
        "    # Resize all for consistency\n",
        "    img_disp = cv2.resize(np.array(pil), (IMG_SIZE, IMG_SIZE))\n",
        "    gt_disp  = cv2.resize(m_gt, (IMG_SIZE, IMG_SIZE))\n",
        "    pr_disp  = cv2.resize(m_pred, (IMG_SIZE, IMG_SIZE))\n",
        "\n",
        "    # === Column 1: Original ===\n",
        "    axes[i, 0].imshow(img_disp)\n",
        "    axes[i, 0].set_title(\"ðŸ–¼ï¸ Original Image\", fontsize=11, weight=\"bold\")\n",
        "    axes[i, 0].axis(\"off\")\n",
        "\n",
        "    # === Column 2: Ground Truth ===\n",
        "    axes[i, 1].imshow(gt_disp, cmap=\"gray\")\n",
        "    axes[i, 1].set_title(\"âœ… Ground Truth\", fontsize=11, weight=\"bold\")\n",
        "    axes[i, 1].axis(\"off\")\n",
        "\n",
        "    # === Column 3: Predicted Mask ===\n",
        "    axes[i, 2].imshow(img_disp)\n",
        "    axes[i, 2].imshow(pr_disp, cmap=\"coolwarm\", alpha=0.45)\n",
        "    axes[i, 2].set_title(f\"ðŸ”® Predicted ({label})\\nThr={thr:.3f} | Area={area} | Mean={mean:.3f}\",\n",
        "                         fontsize=10)\n",
        "    axes[i, 2].axis(\"off\")\n",
        "\n",
        "plt.subplots_adjust(top=0.92, hspace=0.35)\n",
        "fig.suptitle(\"ðŸ” Segmentation of Forged Samples â€” CNNâ€“DINOv2 Hybrid\",\n",
        "             fontsize=16, fontweight=\"bold\", color=\"#b30000\")\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-04T03:07:44.246586Z",
          "iopub.status.busy": "2026-01-04T03:07:44.24619Z",
          "iopub.status.idle": "2026-01-04T03:07:50.057691Z",
          "shell.execute_reply": "2026-01-04T03:07:50.056644Z"
        },
        "papermill": {
          "duration": 5.877977,
          "end_time": "2026-01-04T03:07:50.069292",
          "exception": false,
          "start_time": "2026-01-04T03:07:44.191315",
          "status": "completed"
        },
        "tags": [],
        "id": "864hmInY4ZsZ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2, numpy as np\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "\n",
        "# Select a few authentic examples\n",
        "sample_auth = val_auth[:5]\n",
        "n = len(sample_auth)\n",
        "\n",
        "fig, axes = plt.subplots(n, 2, figsize=(9, n * 3))\n",
        "if n == 1:\n",
        "    axes = np.expand_dims(axes, axis=0)\n",
        "\n",
        "for i, p in enumerate(sample_auth):\n",
        "    pil = Image.open(p).convert(\"RGB\")\n",
        "    label, m_pred, thr, area, mean = pipeline_visual(pil)  # <-- version alignÃ©e avec ta nouvelle pipeline\n",
        "\n",
        "    # Predicted mask (should be empty for authentic images)\n",
        "    m_pred = (m_pred > 0).astype(np.uint8) if m_pred is not None else np.zeros((IMG_SIZE, IMG_SIZE))\n",
        "\n",
        "    # Resize for consistent display\n",
        "    img_disp = cv2.resize(np.array(pil), (IMG_SIZE, IMG_SIZE))\n",
        "    pr_disp  = cv2.resize(m_pred, (IMG_SIZE, IMG_SIZE))\n",
        "\n",
        "    # === Column 1: Original Image ===\n",
        "    axes[i, 0].imshow(img_disp)\n",
        "    axes[i, 0].set_title(\"ðŸ–¼ï¸ Original Image\", fontsize=11, weight=\"bold\")\n",
        "    axes[i, 0].axis(\"off\")\n",
        "\n",
        "    # === Column 2: Predicted Mask ===\n",
        "    axes[i, 1].imshow(img_disp)\n",
        "    axes[i, 1].imshow(pr_disp, cmap=\"coolwarm\", alpha=0.45)\n",
        "    axes[i, 1].set_title(\n",
        "        f\"ðŸŸ¢ Predicted: {label.upper()}\\nArea={area} | Mean={mean:.3f} | Thr={thr:.3f}\",\n",
        "        fontsize=10\n",
        "    )\n",
        "    axes[i, 1].axis(\"off\")\n",
        "\n",
        "    for j in range(2):\n",
        "        axes[i, j].set_aspect(\"equal\")\n",
        "\n",
        "plt.subplots_adjust(top=0.90, hspace=0.35)\n",
        "fig.suptitle(\"ðŸŸ¢ Segmentation of Authentic Images â€” CNNâ€“DINOv2 Hybrid\",\n",
        "             fontsize=16, fontweight=\"bold\", color=\"#009933\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-04T03:07:50.336756Z",
          "iopub.status.busy": "2026-01-04T03:07:50.336399Z",
          "iopub.status.idle": "2026-01-04T03:07:55.079414Z",
          "shell.execute_reply": "2026-01-04T03:07:55.078553Z"
        },
        "papermill": {
          "duration": 4.818679,
          "end_time": "2026-01-04T03:07:55.088316",
          "exception": false,
          "start_time": "2026-01-04T03:07:50.269637",
          "status": "completed"
        },
        "tags": [],
        "id": "d-whYUnx4Zsa"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25f35117"
      },
      "source": [
        "# Task\n",
        "Extract the `dinov2-pytorch-base-v1.tar.gz` and `CNNDINOv2-A78.zip` archives to local directories, then update the `DINO_PATH` and `MODEL_LOC` variables in the code to reference these new local paths, and finally load the DINOv2 and CNNDINOv2 model components using the updated paths."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fa12039d"
      },
      "source": [
        "## Extract DINOv2 Base Model\n",
        "\n",
        "### Subtask:\n",
        "Extract the `dinov2-pytorch-base-v1.tar.gz` file to a local directory.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b087340"
      },
      "source": [
        "**Reasoning**:\n",
        "To extract the `dinov2-pytorch-base-v1.tar.gz` file, I need to create a target directory first and then use the `tar` command to decompress and extract the archive into that directory. Shell commands starting with `!` can be executed within a Python code cell.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ef549a8e",
        "outputId": "6b747d4d-b249-404f-ff90-13846ee71989"
      },
      "source": [
        "import os\n",
        "\n",
        "# Define the paths\n",
        "archive_path = '/content/dinov2-pytorch-base-v1.tar.gz'\n",
        "extraction_dir = 'dinov2_base'\n",
        "\n",
        "# 1. Create a directory named 'dinov2_base'\n",
        "if not os.path.exists(extraction_dir):\n",
        "    os.makedirs(extraction_dir)\n",
        "    print(f\"Directory '{extraction_dir}' created.\")\n",
        "else:\n",
        "    print(f\"Directory '{extraction_dir}' already exists.\")\n",
        "\n",
        "# 2. Extract the contents of the archive into the newly created directory\n",
        "# The -C flag specifies the directory to extract to.\n",
        "!tar -xzf {archive_path} -C {extraction_dir}\n",
        "\n",
        "print(f\"Successfully extracted '{archive_path}' to '{extraction_dir}'.\")\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory 'dinov2_base' created.\n",
            "Successfully extracted '/content/dinov2-pytorch-base-v1.tar.gz' to 'dinov2_base'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07668698"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous step successfully extracted the `dinov2-pytorch-base-v1.tar.gz` archive. The next part of the subtask is to extract the `CNNDINOv2-A78.zip` archive to a local directory.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c905ad3b",
        "outputId": "9367e2ba-0266-45e9-8c7a-ccd3719d9432"
      },
      "source": [
        "import os\n",
        "\n",
        "# Define the paths for the CNNDINOv2 archive\n",
        "cnndinov2_archive_path = '/kaggle/input/cnndinov2-pbd/CNNDINOv2-U52/CNNDINOv2-U52.zip' # This path is based on the MODEL_LOC in a previous cell\n",
        "cnndinov2_extraction_dir = 'cnndinov2_model'\n",
        "\n",
        "# 1. Create a directory for CNNDINOv2 model\n",
        "if not os.path.exists(cnndinov2_extraction_dir):\n",
        "    os.makedirs(cnndinov2_extraction_dir)\n",
        "    print(f\"Directory '{cnndinov2_extraction_dir}' created.\")\n",
        "else:\n",
        "    print(f\"Directory '{cnndinov2_extraction_dir}' already exists.\")\n",
        "\n",
        "# 2. Extract the contents of the CNNDINOv2 archive into the newly created directory\n",
        "# The -d flag specifies the directory to extract to.\n",
        "!unzip -q {cnndinov2_archive_path} -d {cnndinov2_extraction_dir}\n",
        "\n",
        "print(f\"Successfully extracted '{cnndinov2_archive_path}' to '{cnndinov2_extraction_dir}'.\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory 'cnndinov2_model' created.\n",
            "unzip:  cannot find or open /kaggle/input/cnndinov2-pbd/CNNDINOv2-U52/CNNDINOv2-U52.zip, /kaggle/input/cnndinov2-pbd/CNNDINOv2-U52/CNNDINOv2-U52.zip.zip or /kaggle/input/cnndinov2-pbd/CNNDINOv2-U52/CNNDINOv2-U52.zip.ZIP.\n",
            "Successfully extracted '/kaggle/input/cnndinov2-pbd/CNNDINOv2-U52/CNNDINOv2-U52.zip' to 'cnndinov2_model'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7de14899"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt to extract `CNNDINOv2-U52.zip` failed because the file was not found. This indicates that the model files for `CNNDINOv2` are likely already extracted within the Kaggle input directory structure. To fulfill the task's requirement of having the model in a local directory and updating `MODEL_LOC`, I will copy the existing `model_seg_final.pt` file from its Kaggle input path to the `cnndinov2_model` directory.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0f4c8988",
        "outputId": "f12133d2-aa1f-4a69-90bb-3fd6f9b0693c"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Define the paths for the CNNDINOv2 model file\n",
        "original_model_loc = '/kaggle/input/cnndinov2-pbd/CNNDINOv2-U52/CNNDINOv2-U52/model_seg_final.pt'\n",
        "cnndinov2_extraction_dir = 'cnndinov2_model'\n",
        "local_model_path = os.path.join(cnndinov2_extraction_dir, 'model_seg_final.pt')\n",
        "\n",
        "# 1. Create a directory for CNNDINOv2 model if it doesn't exist\n",
        "if not os.path.exists(cnndinov2_extraction_dir):\n",
        "    os.makedirs(cnndinov2_extraction_dir)\n",
        "    print(f\"Directory '{cnndinov2_extraction_dir}' created.\")\n",
        "else:\n",
        "    print(f\"Directory '{cnndinov2_extraction_dir}' already exists.\")\n",
        "\n",
        "# 2. Copy the model file from its Kaggle input path to the local directory\n",
        "if os.path.exists(original_model_loc):\n",
        "    shutil.copy(original_model_loc, local_model_path)\n",
        "    print(f\"Successfully copied '{original_model_loc}' to '{local_model_path}'.\")\n",
        "else:\n",
        "    print(f\"Error: Original model file not found at '{original_model_loc}'. Skipping copy.\")\n",
        "\n",
        "# Update the global MODEL_LOC to point to the new local path\n",
        "# This variable will be used in a later step to modify the original notebook cell.\n",
        "# For now, just print it for confirmation.\n",
        "print(f\"Updated MODEL_LOC will be: {local_model_path}\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory 'cnndinov2_model' already exists.\n",
            "Error: Original model file not found at '/kaggle/input/cnndinov2-pbd/CNNDINOv2-U52/CNNDINOv2-U52/model_seg_final.pt'. Skipping copy.\n",
            "Updated MODEL_LOC will be: cnndinov2_model/model_seg_final.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bfbb879"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt to locate and copy the CNNDINOv2 model file failed because the source path was incorrect due to a Kaggle data source download error. The task specifies extracting `CNNDINOv2-A78.zip`. I will assume this archive is located in `/content/` (similar to how the DINOv2 archive was found there) and proceed to extract it. This will provide the local path for `MODEL_LOC` as per the task.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dc634eb4",
        "outputId": "eacc5d0b-b014-4999-fec7-ed0c2c61f722"
      },
      "source": [
        "import os\n",
        "\n",
        "# Define the path for the CNNDINOv2-A78 archive based on the task description\n",
        "cnndinov2_archive_path = '/content/CNNDINOv2-A78.zip'\n",
        "cnndinov2_extraction_dir = 'cnndinov2_a78_model'\n",
        "\n",
        "# 1. Create a directory for CNNDINOv2-A78 model\n",
        "if not os.path.exists(cnndinov2_extraction_dir):\n",
        "    os.makedirs(cnndinov2_extraction_dir)\n",
        "    print(f\"Directory '{cnndinov2_extraction_dir}' created.\")\n",
        "else:\n",
        "    print(f\"Directory '{cnndinov2_extraction_dir}' already exists.\")\n",
        "\n",
        "# 2. Extract the contents of the CNNDINOv2-A78 archive into the newly created directory\n",
        "# The -d flag specifies the directory to extract to.\n",
        "# Using -o to overwrite files without prompting if they exist.\n",
        "if os.path.exists(cnndinov2_archive_path):\n",
        "    !unzip -o -q {cnndinov2_archive_path} -d {cnndinov2_extraction_dir}\n",
        "    print(f\"Successfully extracted '{cnndinov2_archive_path}' to '{cnndinov2_extraction_dir}'.\")\n",
        "else:\n",
        "    print(f\"Error: CNNDINOv2-A78 archive not found at '{cnndinov2_archive_path}'. Please ensure it is present.\")\n",
        "\n",
        "# Assuming the extracted model file will be inside a subfolder, e.g., CNNDINOv2-A78/model_seg_final.pt\n",
        "# We'll construct the MODEL_LOC path based on this assumption.\n",
        "# If the exact structure differs, this path may need adjustment.\n",
        "MODEL_LOC_updated = os.path.join(cnndinov2_extraction_dir, 'CNNDINOv2-A78', 'model_seg_final.pt')\n",
        "print(f\"Tentative MODEL_LOC after extraction: {MODEL_LOC_updated}\")\n",
        "\n",
        "# The DINO_PATH was extracted to 'dinov2_base' in a previous step\n",
        "DINO_PATH_updated = 'dinov2_base'\n",
        "print(f\"DINO_PATH after extraction: {DINO_PATH_updated}\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory 'cnndinov2_a78_model' created.\n",
            "Successfully extracted '/content/CNNDINOv2-A78.zip' to 'cnndinov2_a78_model'.\n",
            "Tentative MODEL_LOC after extraction: cnndinov2_a78_model/CNNDINOv2-A78/model_seg_final.pt\n",
            "DINO_PATH after extraction: dinov2_base\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe615a4a"
      },
      "source": [
        "**Reasoning**:\n",
        "The archives have been extracted and the new local paths for `DINO_PATH` and `MODEL_LOC` are available in `DINO_PATH_updated` and `MODEL_LOC_updated` variables. The next logical step is to update the original notebook code in cell `HLCSDp1l4ZsW` to use these new local paths for the DINOv2 model and the CNNDINOv2 pretrained weights, and then to reload the model components.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "cd109df5",
        "outputId": "7b2734aa-8589-4880-8536-6f6f6b8d16aa"
      },
      "source": [
        "import os, cv2, json, math, random, torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn, torch.nn.functional as F, torch.optim as optim\n",
        "from transformers import AutoImageProcessor, AutoModel\n",
        "\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    # This forces CUDA to use deterministic algorithms (slower but consistent)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed_everything(42)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "BASE_DIR  = \"/kaggle/input/recodai-luc-scientific-image-forgery-detection\"\n",
        "AUTH_DIR  = f\"{BASE_DIR}/train_images/authentic\"\n",
        "FORG_DIR  = f\"{BASE_DIR}/train_images/forged\"\n",
        "MASK_DIR  = f\"{BASE_DIR}/train_masks\"\n",
        "TEST_DIR  = f\"{BASE_DIR}/test_images\"\n",
        "\n",
        "# Updated paths from previous extraction steps\n",
        "DINO_PATH = 'dinov2_base' # Updated to local extraction directory\n",
        "MODEL_LOC = 'cnndinov2_a78_model/CNNDINOv2-A78/model_seg_final.pt' # Updated to local extraction directory\n",
        "\n",
        "IMG_SIZE = 518\n",
        "BATCH_SIZE = 2\n",
        "\n",
        "# INFERENCE UTILS\n",
        "AREA_THR = 200\n",
        "MEAN_THR = 0.22\n",
        "USE_TTA = True\n",
        "GRID_SEARCH = True\n",
        "\n",
        "class ForgerySegDataset(Dataset):\n",
        "    def __init__(self, auth_paths, forg_paths, mask_dir, img_size=IMG_SIZE):\n",
        "        self.samples = []\n",
        "        for p in forg_paths:\n",
        "            m = os.path.join(mask_dir, Path(p).stem + \".npy\")\n",
        "            if os.path.exists(m):\n",
        "                self.samples.append((p, m))\n",
        "        for p in auth_paths:\n",
        "            self.samples.append((p, None))\n",
        "        self.img_size = img_size\n",
        "    def __len__(self): return len(self.samples)\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, mask_path = self.samples[idx]\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        w, h = img.size\n",
        "        if mask_path is None:\n",
        "            mask = np.zeros((h, w), np.uint8)\n",
        "        else:\n",
        "            m = np.load(mask_path)\n",
        "            if m.ndim == 3: m = np.max(m, axis=0)\n",
        "            mask = (m > 0).astype(np.uint8)\n",
        "        img_r = img.resize((IMG_SIZE, IMG_SIZE))\n",
        "        mask_r = cv2.resize(mask, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_NEAREST)\n",
        "        img_t = torch.from_numpy(np.array(img_r, np.float32)/255.).permute(2,0,1)\n",
        "        mask_t = torch.from_numpy(mask_r[None, ...].astype(np.float32))\n",
        "        return img_t, mask_t\n",
        "\n",
        "\n",
        "#  MODEL (DINOv2 + Decoder)\n",
        "\n",
        "from transformers import AutoImageProcessor, AutoModel\n",
        "processor = AutoImageProcessor.from_pretrained(DINO_PATH, local_files_only=True, use_fast=False)\n",
        "encoder = AutoModel.from_pretrained(DINO_PATH, local_files_only=True).eval().to(device)\n",
        "\n",
        "class DinoTinyDecoder(nn.Module):\n",
        "    def __init__(self, in_ch=768, out_ch=1):\n",
        "        super().__init__()\n",
        "        # Block 1: 768 -> 384\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, 384, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout2d(0.1)\n",
        "        )\n",
        "        # Block 2: 384 -> 192\n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.Conv2d(384, 192, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout2d(0.1)\n",
        "        )\n",
        "        # Block 3: 192 -> 96\n",
        "        self.block3 = nn.Sequential(\n",
        "            nn.Conv2d(192, 96, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        # Final Output: 96 -> 1\n",
        "        self.conv_out = nn.Conv2d(96, out_ch, kernel_size=1)\n",
        "\n",
        "    def forward(self, f, target_size):\n",
        "        # f: [B, 768, 37, 37]\n",
        "\n",
        "        # Step 1: Up to ~74x74\n",
        "        x = F.interpolate(self.block1(f), size=(74, 74), mode='bilinear', align_corners=False)\n",
        "\n",
        "        # Step 2: Up to ~148x148\n",
        "        x = F.interpolate(self.block2(x), size=(148, 148), mode='bilinear', align_corners=False)\n",
        "\n",
        "        # Step 3: Up to ~296x296\n",
        "        x = F.interpolate(self.block3(x), size=(296, 296), mode='bilinear', align_corners=False)\n",
        "\n",
        "        # Step 4: Final jump to 518x518\n",
        "        x = self.conv_out(x)\n",
        "        x = F.interpolate(x, size=target_size, mode='bilinear', align_corners=False)\n",
        "\n",
        "        return x\n",
        "\n",
        "class DinoSegmenter(nn.Module):\n",
        "    def __init__(self, encoder, processor):\n",
        "        super().__init__()\n",
        "        self.encoder, self.processor = encoder, processor\n",
        "        for p in self.encoder.parameters(): p.requires_grad = False\n",
        "        self.seg_head = DinoTinyDecoder(768,1)\n",
        "    def forward_features(self,x):\n",
        "        imgs = (x*255).clamp(0,255).byte().permute(0,2,3,1).cpu().numpy()\n",
        "        inputs = self.processor(images=list(imgs), return_tensors=\"pt\").to(x.device)\n",
        "        # with torch.no_grad():\n",
        "        #     feats = self.encoder(**inputs).last_hidden_state\n",
        "        feats = self.encoder(**inputs).last_hidden_state\n",
        "        B,N,C = feats.shape\n",
        "        fmap = feats[:,1:,:].permute(0,2,1)\n",
        "        s = int(math.sqrt(N-1))\n",
        "        fmap = fmap.reshape(B,C,s,s)\n",
        "        return fmap\n",
        "    def forward_seg(self,x):\n",
        "        fmap = self.forward_features(x)\n",
        "        return self.seg_head(fmap,(IMG_SIZE,IMG_SIZE))\n",
        "\n",
        "\n",
        "auth_imgs = sorted([str(Path(AUTH_DIR)/f) for f in os.listdir(AUTH_DIR)])\n",
        "forg_imgs = sorted([str(Path(FORG_DIR)/f) for f in os.listdir(FORG_DIR)])\n",
        "train_auth, val_auth = train_test_split(auth_imgs, test_size=0.2, random_state=42)\n",
        "train_forg, val_forg = train_test_split(forg_imgs, test_size=0.2, random_state=42)\n",
        "\n",
        "train_loader = DataLoader(ForgerySegDataset(train_auth, train_forg, MASK_DIR),\n",
        "                          batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(ForgerySegDataset(val_auth, val_forg, MASK_DIR),\n",
        "                        batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "model_seg = DinoSegmenter(encoder, processor).to(device)\n",
        "\n",
        "# Load pretrained weights if MODEL_LOC is specified\n",
        "if MODEL_LOC is not None and os.path.exists(MODEL_LOC):\n",
        "    model_seg.load_state_dict(torch.load(MODEL_LOC, map_location=device))\n",
        "    print(f\"âœ… Loaded pretrained model from: {MODEL_LOC}\")\n",
        "    model_seg.eval()  # Set model to evaluation mode\n",
        "\n",
        "@torch.no_grad()\n",
        "def segment_prob_map(pil):\n",
        "    x = torch.from_numpy(np.array(pil.resize((IMG_SIZE, IMG_SIZE)), np.float32)/255.).permute(2,0,1)[None].to(device)\n",
        "    prob = torch.sigmoid(model_seg.forward_seg(x))[0,0].cpu().numpy()\n",
        "    return prob\n",
        "\n",
        "@torch.no_grad()\n",
        "def segment_prob_map_with_tta(pil):\n",
        "    # 1. Preprocessing: Resize, Normalize, and move to Device\n",
        "    x = torch.from_numpy(np.array(pil.resize((IMG_SIZE, IMG_SIZE)), np.float32)/255.).permute(2,0,1)[None].to(device)\n",
        "\n",
        "    predictions = []\n",
        "\n",
        "    # 2. Original Prediction\n",
        "    pred_orig = torch.sigmoid(model_seg.forward_seg(x))\n",
        "    predictions.append(pred_orig)\n",
        "\n",
        "    # 3. Horizontal Flip TTA (dim 3)\n",
        "    # Flip input -> Predict -> Flip output back\n",
        "    pred_h = torch.sigmoid(model_seg.forward_seg(torch.flip(x, dims=[3])))\n",
        "    predictions.append(torch.flip(pred_h, dims=[3]))\n",
        "\n",
        "    # 4. Vertical Flip TTA (dim 2)\n",
        "    # Flip input -> Predict -> Flip output back\n",
        "    pred_v = torch.sigmoid(model_seg.forward_seg(torch.flip(x, dims=[2])))\n",
        "    predictions.append(torch.flip(pred_v, dims=[2]))\n",
        "\n",
        "    # 5. Average the predictions and format as numpy\n",
        "    # We stack the 3 predictions and take the mean across the stack dimension (0)\n",
        "    prob = torch.stack(predictions).mean(0)[0, 0].cpu().numpy()\n",
        "\n",
        "    return prob\n",
        "\n",
        "def enhanced_adaptive_mask(prob, alpha_grad=0.45):\n",
        "    gx = cv2.Sobel(prob, cv2.CV_32F, 1, 0, ksize=3)\n",
        "    gy = cv2.Sobel(prob, cv2.CV_32F, 0, 1, ksize=3)\n",
        "    grad_mag = np.sqrt(gx**2 + gy**2)\n",
        "    grad_norm = grad_mag / (grad_mag.max() + 1e-6)\n",
        "    enhanced = (1 - alpha_grad) * prob + alpha_grad * grad_norm\n",
        "    enhanced = cv2.GaussianBlur(enhanced, (3,3), 0)\n",
        "    thr = np.mean(enhanced) + 0.3 * np.std(enhanced)\n",
        "    mask = (enhanced > thr).astype(np.uint8)\n",
        "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, np.ones((5,5), np.uint8))\n",
        "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, np.ones((3,3), np.uint8))\n",
        "    return mask, thr\n",
        "\n",
        "def finalize_mask(prob, orig_size):\n",
        "    mask, thr = enhanced_adaptive_mask(prob)\n",
        "    mask = cv2.resize(mask, orig_size, interpolation=cv2.INTER_NEAREST)\n",
        "    return mask, thr\n",
        "\n",
        "def pipeline_final(pil):\n",
        "    if USE_TTA:\n",
        "        prob = segment_prob_map_with_tta(pil)\n",
        "    else:\n",
        "        prob = segment_prob_map(pil)\n",
        "    mask, thr = finalize_mask(prob, pil.size)\n",
        "    area = int(mask.sum())\n",
        "    mean_inside = float(prob[cv2.resize(mask,(IMG_SIZE,IMG_SIZE),interpolation=cv2.INTER_NEAREST)==1].mean()) if area>0 else 0.0\n",
        "    if area < AREA_THR or mean_inside < MEAN_THR:\n",
        "        return \"authentic\", None, {\"area\": area, \"mean_inside\": mean_inside, \"thr\": thr}\n",
        "    return \"forged\", mask, {\"area\": area, \"mean_inside\": mean_inside, \"thr\": thr}\n",
        "\n",
        "import itertools\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def grid_search_area_mean(forg_paths, auth_paths, mask_dir):\n",
        "    mean_range = [round(x, 2) for x in np.arange(0.20, 0.291, 0.01)]\n",
        "    area_range = [200]\n",
        "    # 1. Use ALL images from both paths to maximize robustness\n",
        "    val_set = [(p, \"forged\") for p in forg_paths] + [(p, \"authentic\") for p in auth_paths]\n",
        "\n",
        "    print(f\"ðŸš€ Step 1: Caching probability maps for ALL {len(val_set)} images...\")\n",
        "    cache = []\n",
        "    for p, label in tqdm(val_set):\n",
        "        pil = Image.open(p).convert(\"RGB\")\n",
        "        w, h = pil.size\n",
        "\n",
        "        # Get raw probability map\n",
        "        prob = segment_prob_map_with_tta(pil) if USE_TTA else segment_prob_map(pil)\n",
        "\n",
        "        # USE OLD MASK LOGIC: mean + 0.3*std\n",
        "        mask_raw, _ = enhanced_adaptive_mask(prob) # Your function using np.mean + 0.3*np.std\n",
        "        mask_resized = cv2.resize(mask_raw, (w, h), interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "        # Handle Ground Truth\n",
        "        if label == \"forged\":\n",
        "            m_gt = np.load(Path(mask_dir)/f\"{Path(p).stem}.npy\")\n",
        "            if m_gt.ndim == 3: m_gt = np.max(m_gt, axis=0)\n",
        "            m_gt = (m_gt > 0).astype(np.uint8)\n",
        "        else:\n",
        "            m_gt = np.zeros((h, w), np.uint8) # Authentic = blank GT\n",
        "\n",
        "        cache.append({\"prob\": prob, \"mask\": mask_resized, \"gt\": m_gt, \"label\": label})\n",
        "\n",
        "    # 2. Sweep thresholds\n",
        "    best_f1 = -1\n",
        "    best_params = {}\n",
        "    combinations = list(itertools.product(area_range, mean_range))\n",
        "\n",
        "    for a_thr, m_thr in combinations:\n",
        "        current_f1s = []\n",
        "        for item in cache:\n",
        "            mask = item[\"mask\"]\n",
        "            area = int(mask.sum()) # OLD AREA LOGIC\n",
        "\n",
        "            # OLD MEAN LOGIC\n",
        "            mask_small = cv2.resize(mask, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_NEAREST)\n",
        "            mean_in = float(item[\"prob\"][mask_small == 1].mean()) if area > 0 else 0.0\n",
        "\n",
        "            # Pipeline decision\n",
        "            is_forged = (area >= a_thr and mean_in >= m_thr)\n",
        "            m_pred = (mask > 0).astype(np.uint8) if is_forged else np.zeros_like(item[\"gt\"])\n",
        "\n",
        "            # F1 Calculation (Authentic silence = 1.0, noisy prediction = 0.0)\n",
        "            f1 = f1_score(item[\"gt\"].flatten(), m_pred.flatten(),\n",
        "                          zero_division=1 if item[\"label\"] == \"authentic\" else 0)\n",
        "            current_f1s.append(f1)\n",
        "\n",
        "        avg_f1 = np.mean(current_f1s)\n",
        "        if avg_f1 > best_f1:\n",
        "            best_f1 = avg_f1\n",
        "            best_params = {\"AREA_THR\": a_thr, \"MEAN_THR\": m_thr}\n",
        "            print(f\"â­ New Best F1: {best_f1:.4f} -> AREA: {a_thr}, MEAN: {m_thr}\")\n",
        "\n",
        "    return best_params\n",
        "\n",
        "if GRID_SEARCH:\n",
        "    best_cfg = grid_search_area_mean(val_forg, val_auth, MASK_DIR)\n",
        "    AREA_THR = best_cfg['AREA_THR']\n",
        "    MEAN_THR = best_cfg['MEAN_THR']\n",
        "\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "val_items = [(p, 1) for p in val_forg[:10]]\n",
        "results = []\n",
        "for p,_ in tqdm(val_items, desc=\"Validation forged-only\"):\n",
        "    pil = Image.open(p).convert(\"RGB\")\n",
        "    label, m_pred, dbg = pipeline_final(pil)\n",
        "    m_gt = np.load(Path(MASK_DIR)/f\"{Path(p).stem}.npy\")\n",
        "    if m_gt.ndim==3: m_gt=np.max(m_gt,axis=0)\n",
        "    m_gt=(m_gt>0).astype(np.uint8)\n",
        "    m_pred=(m_pred>0).astype(np.uint8) if m_pred is not None else np.zeros_like(m_gt)\n",
        "    f1 = f1_score(m_gt.flatten(), m_pred.flatten(), zero_division=0)\n",
        "    results.append((Path(p).stem, f1, dbg))\n",
        "print(\"\\n F1-score par image falsifiÃ©e:\\n\")\n",
        "for cid,f1,dbg in results:\n",
        "    print(f\"{cid} â€” F1={f1:.4f} | area={dbg['area']} mean={dbg['mean_inside']:.3f} thr={dbg['thr']:.3f}\")\n",
        "print(f\"\\n Moyenne F1 (falsifiÃ©es) = {np.mean([r[1] for r in results]):.4f}\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/kaggle/input/recodai-luc-scientific-image-forgery-detection/train_images/authentic'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4228142316.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m \u001b[0mauth_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAUTH_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAUTH_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0mforg_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFORG_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFORG_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0mtrain_auth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_auth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauth_imgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/recodai-luc-scientific-image-forgery-detection/train_images/authentic'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "323ae43c"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous execution failed with a `FileNotFoundError` because the `BASE_DIR` (`/kaggle/input/recodai-luc-scientific-image-forgery-detection`) and its subdirectories, specifically `AUTH_DIR`, were not found. This is a consequence of the `kagglehub.competition_download` failing earlier. To fix this, I will add a check for `BASE_DIR`'s existence. If the competition data is not available, the code will skip data loading, splitting, DataLoader creation, grid search, and F1-score calculation, but it will still proceed with loading the DINOv2 and CNNDINOv2 model components from their locally extracted paths as per the task requirements.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4a71b5c",
        "outputId": "a7319062-5c30-4164-b825-2fc4fb133082"
      },
      "source": [
        "import os, cv2, json, math, random, torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn, torch.nn.functional as F, torch.optim as optim\n",
        "from transformers import AutoImageProcessor, AutoModel\n",
        "\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    # This forces CUDA to use deterministic algorithms (slower but consistent)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed_everything(42)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "BASE_DIR  = \"/kaggle/input/recodai-luc-scientific-image-forgery-detection\"\n",
        "AUTH_DIR  = f\"{BASE_DIR}/train_images/authentic\"\n",
        "FORG_DIR  = f\"{BASE_DIR}/train_images/forged\"\n",
        "MASK_DIR  = f\"{BASE_DIR}/train_masks\"\n",
        "TEST_DIR  = f\"{BASE_DIR}/test_images\"\n",
        "\n",
        "# Updated paths from previous extraction steps\n",
        "DINO_PATH = 'dinov2_base' # Updated to local extraction directory\n",
        "MODEL_LOC = 'cnndinov2_a78_model/CNNDINOv2-A78/model_seg_final.pt' # Updated to local extraction directory\n",
        "\n",
        "IMG_SIZE = 518\n",
        "BATCH_SIZE = 2\n",
        "\n",
        "# INFERENCE UTILS\n",
        "AREA_THR = 200\n",
        "MEAN_THR = 0.22\n",
        "USE_TTA = True\n",
        "GRID_SEARCH_ENABLED = True # Renamed to avoid conflict if GRID_SEARCH is set to False later\n",
        "\n",
        "class ForgerySegDataset(Dataset):\n",
        "    def __init__(self, auth_paths, forg_paths, mask_dir, img_size=IMG_SIZE):\n",
        "        self.samples = []\n",
        "        for p in forg_paths:\n",
        "            m = os.path.join(mask_dir, Path(p).stem + \".npy\")\n",
        "            if os.path.exists(m):\n",
        "                self.samples.append((p, m))\n",
        "        for p in auth_paths:\n",
        "            self.samples.append((p, None))\n",
        "        self.img_size = img_size\n",
        "    def __len__(self): return len(self.samples)\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, mask_path = self.samples[idx]\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        w, h = img.size\n",
        "        if mask_path is None:\n",
        "            mask = np.zeros((h, w), np.uint8)\n",
        "        else:\n",
        "            m = np.load(mask_path)\n",
        "            if m.ndim == 3: m = np.max(m, axis=0)\n",
        "            mask = (m > 0).astype(np.uint8)\n",
        "        img_r = img.resize((IMG_SIZE, IMG_SIZE))\n",
        "        mask_r = cv2.resize(mask, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_NEAREST)\n",
        "        img_t = torch.from_numpy(np.array(img_r, np.float32)/255.).permute(2,0,1)\n",
        "        mask_t = torch.from_numpy(mask_r[None, ...].astype(np.float32))\n",
        "        return img_t, mask_t\n",
        "\n",
        "\n",
        "#  MODEL (DINOv2 + Decoder)\n",
        "\n",
        "from transformers import AutoImageProcessor, AutoModel\n",
        "processor = AutoImageProcessor.from_pretrained(DINO_PATH, local_files_only=True, use_fast=False)\n",
        "encoder = AutoModel.from_pretrained(DINO_PATH, local_files_only=True).eval().to(device)\n",
        "\n",
        "class DinoTinyDecoder(nn.Module):\n",
        "    def __init__(self, in_ch=768, out_ch=1):\n",
        "        super().__init__()\n",
        "        # Block 1: 768 -> 384\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, 384, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout2d(0.1)\n",
        "        )\n",
        "        # Block 2: 384 -> 192\n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.Conv2d(384, 192, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout2d(0.1)\n",
        "        )\n",
        "        # Block 3: 192 -> 96\n",
        "        self.block3 = nn.Sequential(\n",
        "            nn.Conv2d(192, 96, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        # Final Output: 96 -> 1\n",
        "        self.conv_out = nn.Conv2d(96, out_ch, kernel_size=1)\n",
        "\n",
        "    def forward(self, f, target_size):\n",
        "        # f: [B, 768, 37, 37]\n",
        "\n",
        "        # Step 1: Up to ~74x74\n",
        "        x = F.interpolate(self.block1(f), size=(74, 74), mode='bilinear', align_corners=False)\n",
        "\n",
        "        # Step 2: Up to ~148x148\n",
        "        x = F.interpolate(self.block2(x), size=(148, 148), mode='bilinear', align_corners=False)\n",
        "\n",
        "        # Step 3: Up to ~296x296\n",
        "        x = F.interpolate(self.block3(x), size=(296, 296), mode='bilinear', align_corners=False)\n",
        "\n",
        "        # Step 4: Final jump to 518x518\n",
        "        x = self.conv_out(x)\n",
        "        x = F.interpolate(x, size=target_size, mode='bilinear', align_corners=False)\n",
        "\n",
        "        return x\n",
        "\n",
        "class DinoSegmenter(nn.Module):\n",
        "    def __init__(self, encoder, processor):\n",
        "        super().__init__()\n",
        "        self.encoder, self.processor = encoder, processor\n",
        "        for p in self.encoder.parameters(): p.requires_grad = False\n",
        "        self.seg_head = DinoTinyDecoder(768,1)\n",
        "    def forward_features(self,x):\n",
        "        imgs = (x*255).clamp(0,255).byte().permute(0,2,3,1).cpu().numpy()\n",
        "        inputs = self.processor(images=list(imgs), return_tensors=\"pt\").to(x.device)\n",
        "        # with torch.no_grad():\n",
        "        #     feats = self.encoder(**inputs).last_hidden_state\n",
        "        feats = self.encoder(**inputs).last_hidden_state\n",
        "        B,N,C = feats.shape\n",
        "        fmap = feats[:,1:,:].permute(0,2,1)\n",
        "        s = int(math.sqrt(N-1))\n",
        "        fmap = fmap.reshape(B,C,s,s)\n",
        "        return fmap\n",
        "    def forward_seg(self,x):\n",
        "        fmap = self.forward_features(x)\n",
        "        return self.seg_head(fmap,(IMG_SIZE,IMG_SIZE))\n",
        "\n",
        "\n",
        "# --- MODIFIED PART TO HANDLE MISSING BASE_DIR ---\n",
        "# Initialize data-related variables as empty lists or None\n",
        "auth_imgs, forg_imgs, train_auth, val_auth, train_forg, val_forg = [], [], [], [], [], []\n",
        "train_loader, val_loader = None, None\n",
        "\n",
        "if not os.path.exists(BASE_DIR):\n",
        "    print(f\"Warning: Competition data not found at '{BASE_DIR}'. Skipping data loading, training setup, and grid search.\")\n",
        "    print(\"Please ensure the 'recodai-luc-scientific-image-forgery-detection' dataset is available or adjust BASE_DIR.\")\n",
        "    GRID_SEARCH_ENABLED = False # Disable grid search if data is not available\n",
        "else:\n",
        "    auth_imgs = sorted([str(Path(AUTH_DIR)/f) for f in os.listdir(AUTH_DIR)])\n",
        "    forg_imgs = sorted([str(Path(FORG_DIR)/f) for f in os.listdir(FORG_DIR)])\n",
        "    train_auth, val_auth = train_test_split(auth_imgs, test_size=0.2, random_state=42)\n",
        "    train_forg, val_forg = train_test_split(forg_imgs, test_size=0.2, random_state=42)\n",
        "\n",
        "    train_loader = DataLoader(ForgerySegDataset(train_auth, train_forg, MASK_DIR),\n",
        "                              batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(ForgerySegDataset(val_auth, val_forg, MASK_DIR),\n",
        "                            batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "model_seg = DinoSegmenter(encoder, processor).to(device)\n",
        "\n",
        "# Load pretrained weights if MODEL_LOC is specified\n",
        "if MODEL_LOC is not None and os.path.exists(MODEL_LOC):\n",
        "    model_seg.load_state_dict(torch.load(MODEL_LOC, map_location=device))\n",
        "    print(f\"âœ… Loaded pretrained model from: {MODEL_LOC}\")\n",
        "    model_seg.eval()  # Set model to evaluation mode\n",
        "else:\n",
        "    print(f\"âš ï¸ Warning: Model weights not found at '{MODEL_LOC}'. Model will use randomly initialized weights (if applicable).\")\n",
        "\n",
        "@torch.no_grad()\n",
        "def segment_prob_map(pil):\n",
        "    x = torch.from_numpy(np.array(pil.resize((IMG_SIZE, IMG_SIZE)), np.float32)/255.).permute(2,0,1)[None].to(device)\n",
        "    prob = torch.sigmoid(model_seg.forward_seg(x))[0,0].cpu().numpy()\n",
        "    return prob\n",
        "\n",
        "@torch.no_grad()\n",
        "def segment_prob_map_with_tta(pil):\n",
        "    # 1. Preprocessing: Resize, Normalize, and move to Device\n",
        "    x = torch.from_numpy(np.array(pil.resize((IMG_SIZE, IMG_SIZE)), np.float32)/255.).permute(2,0,1)[None].to(device)\n",
        "\n",
        "    predictions = []\n",
        "\n",
        "    # 2. Original Prediction\n",
        "    pred_orig = torch.sigmoid(model_seg.forward_seg(x))\n",
        "    predictions.append(pred_orig)\n",
        "\n",
        "    # 3. Horizontal Flip TTA (dim 3)\n",
        "    # Flip input -> Predict -> Flip output back\n",
        "    pred_h = torch.sigmoid(model_seg.forward_seg(torch.flip(x, dims=[3])))\n",
        "    predictions.append(torch.flip(pred_h, dims=[3]))\n",
        "\n",
        "    # 4. Vertical Flip TTA (dim 2)\n",
        "    # Flip input -> Predict -> Flip output back\n",
        "    pred_v = torch.sigmoid(model_seg.forward_seg(torch.flip(x, dims=[2])))\n",
        "    predictions.append(torch.flip(pred_v, dims=[2]))\n",
        "\n",
        "    # 5. Average the predictions and format as numpy\n",
        "    # We stack the 3 predictions and take the mean across the stack dimension (0)\n",
        "    prob = torch.stack(predictions).mean(0)[0, 0].cpu().numpy()\n",
        "\n",
        "    return prob\n",
        "\n",
        "def enhanced_adaptive_mask(prob, alpha_grad=0.45):\n",
        "    gx = cv2.Sobel(prob, cv2.CV_32F, 1, 0, ksize=3)\n",
        "    gy = cv2.Sobel(prob, cv2.CV_32F, 0, 1, ksize=3)\n",
        "    grad_mag = np.sqrt(gx**2 + gy**2)\n",
        "    grad_norm = grad_mag / (grad_mag.max() + 1e-6)\n",
        "    enhanced = (1 - alpha_grad) * prob + alpha_grad * grad_norm\n",
        "    enhanced = cv2.GaussianBlur(enhanced, (3,3), 0)\n",
        "    thr = np.mean(enhanced) + 0.3 * np.std(enhanced)\n",
        "    mask = (enhanced > thr).astype(np.uint8)\n",
        "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, np.ones((5,5), np.uint8))\n",
        "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, np.ones((3,3), np.uint8))\n",
        "    return mask, thr\n",
        "\n",
        "def finalize_mask(prob, orig_size):\n",
        "    mask, thr = enhanced_adaptive_mask(prob)\n",
        "    mask = cv2.resize(mask, orig_size, interpolation=cv2.INTER_NEAREST)\n",
        "    return mask, thr\n",
        "\n",
        "def pipeline_final(pil):\n",
        "    if USE_TTA:\n",
        "        prob = segment_prob_map_with_tta(pil)\n",
        "    else:\n",
        "        prob = segment_prob_map(pil)\n",
        "    mask, thr = finalize_mask(prob, pil.size)\n",
        "    area = int(mask.sum())\n",
        "    mean_inside = float(prob[cv2.resize(mask,(IMG_SIZE,IMG_SIZE),interpolation=cv2.INTER_NEAREST)==1].mean()) if area>0 else 0.0\n",
        "    if area < AREA_THR or mean_inside < MEAN_THR:\n",
        "        return \"authentic\", None, {\"area\": area, \"mean_inside\": mean_inside, \"thr\": thr}\n",
        "    return \"forged\", mask, {\"area\": area, \"mean_inside\": mean_inside, \"thr\": thr}\n",
        "\n",
        "import itertools\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def grid_search_area_mean(forg_paths, auth_paths, mask_dir):\n",
        "    # Return default values and print message if no valid data for grid search\n",
        "    if not forg_paths and not auth_paths:\n",
        "        print(\"Skipping grid search as validation data is not available.\")\n",
        "        return {'AREA_THR': 200, 'MEAN_THR': 0.22}\n",
        "\n",
        "    mean_range = [round(x, 2) for x in np.arange(0.20, 0.291, 0.01)]\n",
        "    area_range = [200]\n",
        "    val_set = [(p, \"forged\") for p in forg_paths] + [(p, \"authentic\") for p in auth_paths]\n",
        "\n",
        "    if not val_set:\n",
        "        print(\"Skipping grid search as validation set is empty.\")\n",
        "        return {'AREA_THR': 200, 'MEAN_THR': 0.22}\n",
        "\n",
        "    print(f\"ðŸš€ Step 1: Caching probability maps for ALL {len(val_set)} images...\")\n",
        "    cache = []\n",
        "    for p, label in tqdm(val_set):\n",
        "        try:\n",
        "            pil = Image.open(p).convert(\"RGB\")\n",
        "            w, h = pil.size\n",
        "\n",
        "            prob = segment_prob_map_with_tta(pil) if USE_TTA else segment_prob_map(pil)\n",
        "\n",
        "            mask_raw, _ = enhanced_adaptive_mask(prob)\n",
        "            mask_resized = cv2.resize(mask_raw, (w, h), interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "            if label == \"forged\":\n",
        "                m_gt = np.load(Path(mask_dir)/f\"{Path(p).stem}.npy\")\n",
        "                if m_gt.ndim == 3: m_gt = np.max(m_gt, axis=0)\n",
        "                m_gt = (m_gt > 0).astype(np.uint8)\n",
        "            else:\n",
        "                m_gt = np.zeros((h, w), np.uint8)\n",
        "\n",
        "            cache.append({\"prob\": prob, \"mask\": mask_resized, \"gt\": m_gt, \"label\": label})\n",
        "        except FileNotFoundError as e:\n",
        "            print(f\"Skipping {p} due to FileNotFoundError during caching: {e}\")\n",
        "            continue\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping {p} due to unexpected error during caching: {e}\")\n",
        "            continue\n",
        "\n",
        "    if not cache: # If no items were successfully cached, return default params\n",
        "        print(\"No valid images found for grid search caching. Returning default parameters.\")\n",
        "        return {'AREA_THR': 200, 'MEAN_THR': 0.22}\n",
        "\n",
        "    best_f1 = -1\n",
        "    best_params = {}\n",
        "    combinations = list(itertools.product(area_range, mean_range))\n",
        "\n",
        "    for a_thr, m_thr in combinations:\n",
        "        current_f1s = []\n",
        "        for item in cache:\n",
        "            mask = item[\"mask\"]\n",
        "            area = int(mask.sum())\n",
        "\n",
        "            mask_small = cv2.resize(mask, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_NEAREST)\n",
        "            mean_in = float(item[\"prob\"][mask_small == 1].mean()) if area > 0 else 0.0\n",
        "\n",
        "            is_forged = (area >= a_thr and mean_in >= m_thr)\n",
        "            m_pred = (mask > 0).astype(np.uint8) if is_forged else np.zeros_like(item[\"gt\"])\n",
        "\n",
        "            f1 = f1_score(item[\"gt\"].flatten(), m_pred.flatten(),\n",
        "                          zero_division=1 if item[\"label\"] == \"authentic\" else 0)\n",
        "            current_f1s.append(f1)\n",
        "\n",
        "        if current_f1s: # Only calculate average if any F1 scores were computed\n",
        "            avg_f1 = np.mean(current_f1s)\n",
        "            if avg_f1 > best_f1:\n",
        "                best_f1 = avg_f1\n",
        "                best_params = {\"AREA_THR\": a_thr, \"MEAN_THR\": m_thr}\n",
        "                print(f\"â­ New Best F1: {best_f1:.4f} -> AREA: {a_thr}, MEAN: {m_thr}\")\n",
        "\n",
        "    if not best_params: # If no suitable parameters found, return defaults\n",
        "        print(\"No optimal parameters found during grid search. Returning default parameters.\")\n",
        "        return {'AREA_THR': 200, 'MEAN_THR': 0.22}\n",
        "\n",
        "    return best_params\n",
        "\n",
        "if GRID_SEARCH_ENABLED:\n",
        "    if val_forg or val_auth: # Only run grid search if there's actual validation data\n",
        "        print(\"Starting grid search for optimal parameters...\")\n",
        "        best_cfg = grid_search_area_mean(val_forg, val_auth, MASK_DIR)\n",
        "        AREA_THR = best_cfg['AREA_THR']\n",
        "        MEAN_THR = best_cfg['MEAN_THR']\n",
        "        print(f\"Final Grid Search Parameters: AREA_THR={AREA_THR}, MEAN_THR={MEAN_THR}\")\n",
        "    else:\n",
        "        print(\"Skipping grid search because validation data lists are empty.\")\n",
        "else:\n",
        "    print(\"Skipping grid search because GRID_SEARCH_ENABLED flag is False.\")\n",
        "\n",
        "\n",
        "# --- FURTHER MODIFICATION FOR VALIDATION F1 SCORE CALCULATION ---\n",
        "from sklearn.metrics import f1_score\n",
        "val_items = [(p, 1) for p in val_forg[:10]] # This will be empty if val_forg is empty\n",
        "results = []\n",
        "if val_items:\n",
        "    print(\"\\nCalculating F1-score for validation forged images:\")\n",
        "    for p,_ in tqdm(val_items, desc=\"Validation forged-only\"):\n",
        "        try:\n",
        "            pil = Image.open(p).convert(\"RGB\")\n",
        "            label, m_pred, dbg = pipeline_final(pil)\n",
        "            m_gt = np.load(Path(MASK_DIR)/f\"{Path(p).stem}.npy\")\n",
        "            if m_gt.ndim==3: m_gt=np.max(m_gt,axis=0)\n",
        "            m_gt=(m_gt>0).astype(np.uint8)\n",
        "            m_pred=(m_pred>0).astype(np.uint8) if m_pred is not None else np.zeros_like(m_gt)\n",
        "            f1 = f1_score(m_gt.flatten(), m_pred.flatten(), zero_division=0)\n",
        "            results.append((Path(p).stem, f1, dbg))\n",
        "        except FileNotFoundError as e:\n",
        "            print(f\"Skipping {p} in validation F1 calculation due to FileNotFoundError: {e}\")\n",
        "            continue\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping {p} in validation F1 calculation due to unexpected error: {e}\")\n",
        "            continue\n",
        "\n",
        "    if results:\n",
        "        print(\"\\n F1-score par image falsifiÃ©e:\\n\")\n",
        "        for cid,f1,dbg in results:\n",
        "            print(f\"{cid} â€” F1={f1:.4f} | area={dbg['area']} mean={dbg['mean_inside']:.3f} thr={dbg['thr']:.3f}\")\n",
        "        print(f\"\\n Moyenne F1 (falsifiÃ©es) = {np.mean([r[1] for r in results]):.4f}\")\n",
        "    else:\n",
        "        print(\"\\n No results to display for validation F1-score (forged-only).\")\n",
        "else:\n",
        "    print(\"Skipping validation F1-score calculation (forged-only) because validation data is not available.\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Competition data not found at '/kaggle/input/recodai-luc-scientific-image-forgery-detection'. Skipping data loading, training setup, and grid search.\n",
            "Please ensure the 'recodai-luc-scientific-image-forgery-detection' dataset is available or adjust BASE_DIR.\n",
            "âš ï¸ Warning: Model weights not found at 'cnndinov2_a78_model/CNNDINOv2-A78/model_seg_final.pt'. Model will use randomly initialized weights (if applicable).\n",
            "Skipping grid search because GRID_SEARCH_ENABLED flag is False.\n",
            "Skipping validation F1-score calculation (forged-only) because validation data is not available.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "294a57bb"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous step updated `MODEL_LOC` to a path within `cnndinov2_a78_model`, but the model weights were still not found. This suggests the assumed subfolder structure (`CNNDINOv2-A78/model_seg_final.pt`) might be incorrect. To debug this, I need to inspect the contents of the `cnndinov2_a78_model` directory to identify the actual path of `model_seg_final.pt`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3874ef2f",
        "outputId": "43a689a5-9e03-4e58-901b-b915894bdf61"
      },
      "source": [
        "import os\n",
        "\n",
        "# List the contents of the CNNDINOv2 extraction directory\n",
        "cnndinov2_extraction_dir = 'cnndinov2_a78_model'\n",
        "\n",
        "print(f\"Contents of '{cnndinov2_extraction_dir}':\")\n",
        "if os.path.exists(cnndinov2_extraction_dir):\n",
        "    for root, dirs, files in os.walk(cnndinov2_extraction_dir):\n",
        "        level = root.replace(cnndinov2_extraction_dir, '').count(os.sep)\n",
        "        indent = ' ' * 4 * (level)\n",
        "        print(f'{indent}{os.path.basename(root)}/')\n",
        "        subindent = ' ' * 4 * (level + 1)\n",
        "        for f in files:\n",
        "            print(f'{subindent}{f}')\n",
        "else:\n",
        "    print(f\"Directory '{cnndinov2_extraction_dir}' does not exist.\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contents of 'cnndinov2_a78_model':\n",
            "cnndinov2_a78_model/\n",
            "    submission.csv\n",
            "    model_seg_final.pt\n",
            "    __results___files/\n",
            "        __results___11_1.png\n",
            "        __results___5_2.png\n",
            "        __results___8_1.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdaa1765"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous step identified that `model_seg_final.pt` is directly under `cnndinov2_a78_model/`, not in a subfolder. I will now update `MODEL_LOC` with the correct path and re-execute the code block that initializes and loads the models, ensuring that the model weights are found and loaded correctly.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c588ad3c",
        "outputId": "1f4e8125-8908-4b3e-ce1c-f18fc4f69e7d"
      },
      "source": [
        "import os, cv2, json, math, random, torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn, torch.nn.functional as F, torch.optim as optim\n",
        "from transformers import AutoImageProcessor, AutoModel\n",
        "\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    # This forces CUDA to use deterministic algorithms (slower but consistent)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed_everything(42)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "BASE_DIR  = \"/kaggle/input/recodai-luc-scientific-image-forgery-detection\"\n",
        "AUTH_DIR  = f\"{BASE_DIR}/train_images/authentic\"\n",
        "FORG_DIR  = f\"{BASE_DIR}/train_images/forged\"\n",
        "MASK_DIR  = f\"{BASE_DIR}/train_masks\"\n",
        "TEST_DIR  = f\"{BASE_DIR}/test_images\"\n",
        "\n",
        "# Updated paths from previous extraction steps\n",
        "DINO_PATH = 'dinov2_base' # Updated to local extraction directory\n",
        "MODEL_LOC = 'cnndinov2_a78_model/model_seg_final.pt' # Corrected path based on directory listing\n",
        "\n",
        "IMG_SIZE = 518\n",
        "BATCH_SIZE = 2\n",
        "\n",
        "# INFERENCE UTILS\n",
        "AREA_THR = 200\n",
        "MEAN_THR = 0.22\n",
        "USE_TTA = True\n",
        "GRID_SEARCH_ENABLED = True # Renamed to avoid conflict if GRID_SEARCH is set to False later\n",
        "\n",
        "class ForgerySegDataset(Dataset):\n",
        "    def __init__(self, auth_paths, forg_paths, mask_dir, img_size=IMG_SIZE):\n",
        "        self.samples = []\n",
        "        for p in forg_paths:\n",
        "            m = os.path.join(mask_dir, Path(p).stem + \".npy\")\n",
        "            if os.path.exists(m):\n",
        "                self.samples.append((p, m))\n",
        "        for p in auth_paths:\n",
        "            self.samples.append((p, None))\n",
        "        self.img_size = img_size\n",
        "    def __len__(self): return len(self.samples)\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, mask_path = self.samples[idx]\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        w, h = img.size\n",
        "        if mask_path is None:\n",
        "            mask = np.zeros((h, w), np.uint8)\n",
        "        else:\n",
        "            m = np.load(mask_path)\n",
        "            if m.ndim == 3: m = np.max(m, axis=0)\n",
        "            mask = (m > 0).astype(np.uint8)\n",
        "        img_r = img.resize((IMG_SIZE, IMG_SIZE))\n",
        "        mask_r = cv2.resize(mask, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_NEAREST)\n",
        "        img_t = torch.from_numpy(np.array(img_r, np.float32)/255.).permute(2,0,1)\n",
        "        mask_t = torch.from_numpy(mask_r[None, ...].astype(np.float32))\n",
        "        return img_t, mask_t\n",
        "\n",
        "\n",
        "#  MODEL (DINOv2 + Decoder)\n",
        "\n",
        "from transformers import AutoImageProcessor, AutoModel\n",
        "processor = AutoImageProcessor.from_pretrained(DINO_PATH, local_files_only=True, use_fast=False)\n",
        "encoder = AutoModel.from_pretrained(DINO_PATH, local_files_only=True).eval().to(device)\n",
        "\n",
        "class DinoTinyDecoder(nn.Module):\n",
        "    def __init__(self, in_ch=768, out_ch=1):\n",
        "        super().__init__()\n",
        "        # Block 1: 768 -> 384\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, 384, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout2d(0.1)\n",
        "        )\n",
        "        # Block 2: 384 -> 192\n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.Conv2d(384, 192, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout2d(0.1)\n",
        "        )\n",
        "        # Block 3: 192 -> 96\n",
        "        self.block3 = nn.Sequential(\n",
        "            nn.Conv2d(192, 96, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        # Final Output: 96 -> 1\n",
        "        self.conv_out = nn.Conv2d(96, out_ch, kernel_size=1)\n",
        "\n",
        "    def forward(self, f, target_size):\n",
        "        # f: [B, 768, 37, 37]\n",
        "\n",
        "        # Step 1: Up to ~74x74\n",
        "        x = F.interpolate(self.block1(f), size=(74, 74), mode='bilinear', align_corners=False)\n",
        "\n",
        "        # Step 2: Up to ~148x148\n",
        "        x = F.interpolate(self.block2(x), size=(148, 148), mode='bilinear', align_corners=False)\n",
        "\n",
        "        # Step 3: Up to ~296x296\n",
        "        x = F.interpolate(self.block3(x), size=(296, 296), mode='bilinear', align_corners=False)\n",
        "\n",
        "        # Step 4: Final jump to 518x518\n",
        "        x = self.conv_out(x)\n",
        "        x = F.interpolate(x, size=target_size, mode='bilinear', align_corners=False)\n",
        "\n",
        "        return x\n",
        "\n",
        "class DinoSegmenter(nn.Module):\n",
        "    def __init__(self, encoder, processor):\n",
        "        super().__init__()\n",
        "        self.encoder, self.processor = encoder, processor\n",
        "        for p in self.encoder.parameters(): p.requires_grad = False\n",
        "        self.seg_head = DinoTinyDecoder(768,1)\n",
        "    def forward_features(self,x):\n",
        "        imgs = (x*255).clamp(0,255).byte().permute(0,2,3,1).cpu().numpy()\n",
        "        inputs = self.processor(images=list(imgs), return_tensors=\"pt\").to(x.device)\n",
        "        # with torch.no_grad():\n",
        "        #     feats = self.encoder(**inputs).last_hidden_state\n",
        "        feats = self.encoder(**inputs).last_hidden_state\n",
        "        B,N,C = feats.shape\n",
        "        fmap = feats[:,1:,:].permute(0,2,1)\n",
        "        s = int(math.sqrt(N-1))\n",
        "        fmap = fmap.reshape(B,C,s,s)\n",
        "        return fmap\n",
        "    def forward_seg(self,x):\n",
        "        fmap = self.forward_features(x)\n",
        "        return self.seg_head(fmap,(IMG_SIZE,IMG_SIZE))\n",
        "\n",
        "\n",
        "# --- MODIFIED PART TO HANDLE MISSING BASE_DIR ---\n",
        "# Initialize data-related variables as empty lists or None\n",
        "auth_imgs, forg_imgs, train_auth, val_auth, train_forg, val_forg = [], [], [], [], [], []\n",
        "train_loader, val_loader = None, None\n",
        "\n",
        "if not os.path.exists(BASE_DIR):\n",
        "    print(f\"Warning: Competition data not found at '{BASE_DIR}'. Skipping data loading, training setup, and grid search.\")\n",
        "    print(\"Please ensure the 'recodai-luc-scientific-image-forgery-detection' dataset is available or adjust BASE_DIR.\")\n",
        "    GRID_SEARCH_ENABLED = False # Disable grid search if data is not available\n",
        "else:\n",
        "    auth_imgs = sorted([str(Path(AUTH_DIR)/f) for f in os.listdir(AUTH_DIR)])\n",
        "    forg_imgs = sorted([str(Path(FORG_DIR)/f) for f in os.listdir(FORG_DIR)])\n",
        "    train_auth, val_auth = train_test_split(auth_imgs, test_size=0.2, random_state=42)\n",
        "    train_forg, val_forg = train_test_split(forg_imgs, test_size=0.2, random_state=42)\n",
        "\n",
        "    train_loader = DataLoader(ForgerySegDataset(train_auth, train_forg, MASK_DIR),\n",
        "                              batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(ForgerySegDataset(val_auth, val_forg, MASK_DIR),\n",
        "                            batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "model_seg = DinoSegmenter(encoder, processor).to(device)\n",
        "\n",
        "# Load pretrained weights if MODEL_LOC is specified\n",
        "if MODEL_LOC is not None and os.path.exists(MODEL_LOC):\n",
        "    model_seg.load_state_dict(torch.load(MODEL_LOC, map_location=device))\n",
        "    print(f\"âœ… Loaded pretrained model from: {MODEL_LOC}\")\n",
        "    model_seg.eval()  # Set model to evaluation mode\n",
        "else:\n",
        "    print(f\"âš ï¸ Warning: Model weights not found at '{MODEL_LOC}'. Model will use randomly initialized weights (if applicable).\")\n",
        "\n",
        "@torch.no_grad()\n",
        "def segment_prob_map(pil):\n",
        "    x = torch.from_numpy(np.array(pil.resize((IMG_SIZE, IMG_SIZE)), np.float32)/255.).permute(2,0,1)[None].to(device)\n",
        "    prob = torch.sigmoid(model_seg.forward_seg(x))[0,0].cpu().numpy()\n",
        "    return prob\n",
        "\n",
        "@torch.no_grad()\n",
        "def segment_prob_map_with_tta(pil):\n",
        "    # 1. Preprocessing: Resize, Normalize, and move to Device\n",
        "    x = torch.from_numpy(np.array(pil.resize((IMG_SIZE, IMG_SIZE)), np.float32)/255.).permute(2,0,1)[None].to(device)\n",
        "\n",
        "    predictions = []\n",
        "\n",
        "    # 2. Original Prediction\n",
        "    pred_orig = torch.sigmoid(model_seg.forward_seg(x))\n",
        "    predictions.append(pred_orig)\n",
        "\n",
        "    # 3. Horizontal Flip TTA (dim 3)\n",
        "    # Flip input -> Predict -> Flip output back\n",
        "    pred_h = torch.sigmoid(model_seg.forward_seg(torch.flip(x, dims=[3])))\n",
        "    predictions.append(torch.flip(pred_h, dims=[3]))\n",
        "\n",
        "    # 4. Vertical Flip TTA (dim 2)\n",
        "    # Flip input -> Predict -> Flip output back\n",
        "    pred_v = torch.sigmoid(model_seg.forward_seg(torch.flip(x, dims=[2])))\n",
        "    predictions.append(torch.flip(pred_v, dims=[2]))\n",
        "\n",
        "    # 5. Average the predictions and format as numpy\n",
        "    # We stack the 3 predictions and take the mean across the stack dimension (0)\n",
        "    prob = torch.stack(predictions).mean(0)[0, 0].cpu().numpy()\n",
        "\n",
        "    return prob\n",
        "\n",
        "def enhanced_adaptive_mask(prob, alpha_grad=0.45):\n",
        "    gx = cv2.Sobel(prob, cv2.CV_32F, 1, 0, ksize=3)\n",
        "    gy = cv2.Sobel(prob, cv2.CV_32F, 0, 1, ksize=3)\n",
        "    grad_mag = np.sqrt(gx**2 + gy**2)\n",
        "    grad_norm = grad_mag / (grad_mag.max() + 1e-6)\n",
        "    enhanced = (1 - alpha_grad) * prob + alpha_grad * grad_norm\n",
        "    enhanced = cv2.GaussianBlur(enhanced, (3,3), 0)\n",
        "    thr = np.mean(enhanced) + 0.3 * np.std(enhanced)\n",
        "    mask = (enhanced > thr).astype(np.uint8)\n",
        "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, np.ones((5,5), np.uint8))\n",
        "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, np.ones((3,3), np.uint8))\n",
        "    return mask, thr\n",
        "\n",
        "def finalize_mask(prob, orig_size):\n",
        "    mask, thr = enhanced_adaptive_mask(prob)\n",
        "    mask = cv2.resize(mask, orig_size, interpolation=cv2.INTER_NEAREST)\n",
        "    return mask, thr\n",
        "\n",
        "def pipeline_final(pil):\n",
        "    if USE_TTA:\n",
        "        prob = segment_prob_map_with_tta(pil)\n",
        "    else:\n",
        "        prob = segment_prob_map(pil)\n",
        "    mask, thr = finalize_mask(prob, pil.size)\n",
        "    area = int(mask.sum())\n",
        "    mean_inside = float(prob[cv2.resize(mask,(IMG_SIZE,IMG_SIZE),interpolation=cv2.INTER_NEAREST)==1].mean()) if area>0 else 0.0\n",
        "    if area < AREA_THR or mean_inside < MEAN_THR:\n",
        "        return \"authentic\", None, {\"area\": area, \"mean_inside\": mean_inside, \"thr\": thr}\n",
        "    return \"forged\", mask, {\"area\": area, \"mean_inside\": mean_inside, \"thr\": thr}\n",
        "\n",
        "import itertools\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def grid_search_area_mean(forg_paths, auth_paths, mask_dir):\n",
        "    # Return default values and print message if no valid data for grid search\n",
        "    if not forg_paths and not auth_paths:\n",
        "        print(\"Skipping grid search as validation data is not available.\")\n",
        "        return {'AREA_THR': 200, 'MEAN_THR': 0.22}\n",
        "\n",
        "    mean_range = [round(x, 2) for x in np.arange(0.20, 0.291, 0.01)]\n",
        "    area_range = [200]\n",
        "    val_set = [(p, \"forged\") for p in forg_paths] + [(p, \"authentic\") for p in auth_paths]\n",
        "\n",
        "    if not val_set:\n",
        "        print(\"Skipping grid search as validation set is empty.\")\n",
        "        return {'AREA_THR': 200, 'MEAN_THR': 0.22}\n",
        "\n",
        "    print(f\"ðŸš€ Step 1: Caching probability maps for ALL {len(val_set)} images...\")\n",
        "    cache = []\n",
        "    for p, label in tqdm(val_set):\n",
        "        try:\n",
        "            pil = Image.open(p).convert(\"RGB\")\n",
        "            w, h = pil.size\n",
        "\n",
        "            prob = segment_prob_map_with_tta(pil) if USE_TTA else segment_prob_map(pil)\n",
        "\n",
        "            mask_raw, _ = enhanced_adaptive_mask(prob)\n",
        "            mask_resized = cv2.resize(mask_raw, (w, h), interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "            if label == \"forged\":\n",
        "                m_gt = np.load(Path(mask_dir)/f\"{Path(p).stem}.npy\")\n",
        "                if m_gt.ndim == 3: m_gt = np.max(m_gt, axis=0)\n",
        "                m_gt = (m_gt > 0).astype(np.uint8)\n",
        "            else:\n",
        "                m_gt = np.zeros((h, w), np.uint8)\n",
        "\n",
        "            cache.append({\"prob\": prob, \"mask\": mask_resized, \"gt\": m_gt, \"label\": label})\n",
        "        except FileNotFoundError as e:\n",
        "            print(f\"Skipping {p} due to FileNotFoundError during caching: {e}\")\n",
        "            continue\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping {p} due to unexpected error during caching: {e}\")\n",
        "            continue\n",
        "\n",
        "    if not cache: # If no items were successfully cached, return default params\n",
        "        print(\"No valid images found for grid search caching. Returning default parameters.\")\n",
        "        return {'AREA_THR': 200, 'MEAN_THR': 0.22}\n",
        "\n",
        "    best_f1 = -1\n",
        "    best_params = {}\n",
        "    combinations = list(itertools.product(area_range, mean_range))\n",
        "\n",
        "    for a_thr, m_thr in combinations:\n",
        "        current_f1s = []\n",
        "        for item in cache:\n",
        "            mask = item[\"mask\"]\n",
        "            area = int(mask.sum())\n",
        "\n",
        "            mask_small = cv2.resize(mask, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_NEAREST)\n",
        "            mean_in = float(item[\"prob\"][mask_small == 1].mean()) if area > 0 else 0.0\n",
        "\n",
        "            is_forged = (area >= a_thr and mean_in >= m_thr)\n",
        "            m_pred = (mask > 0).astype(np.uint8) if is_forged else np.zeros_like(item[\"gt\"])\n",
        "\n",
        "            f1 = f1_score(item[\"gt\"].flatten(), m_pred.flatten(),\n",
        "                          zero_division=1 if item[\"label\"] == \"authentic\" else 0)\n",
        "            current_f1s.append(f1)\n",
        "\n",
        "        if current_f1s: # Only calculate average if any F1 scores were computed\n",
        "            avg_f1 = np.mean(current_f1s)\n",
        "            if avg_f1 > best_f1:\n",
        "                best_f1 = avg_f1\n",
        "                best_params = {\"AREA_THR\": a_thr, \"MEAN_THR\": m_thr}\n",
        "                print(f\"â­ New Best F1: {best_f1:.4f} -> AREA: {a_thr}, MEAN: {m_thr}\")\n",
        "\n",
        "    if not best_params: # If no suitable parameters found, return defaults\n",
        "        print(\"No optimal parameters found during grid search. Returning default parameters.\")\n",
        "        return {'AREA_THR': 200, 'MEAN_THR': 0.22}\n",
        "\n",
        "    return best_params\n",
        "\n",
        "if GRID_SEARCH_ENABLED:\n",
        "    if val_forg or val_auth: # Only run grid search if there's actual validation data\n",
        "        print(\"Starting grid search for optimal parameters...\")\n",
        "        best_cfg = grid_search_area_mean(val_forg, val_auth, MASK_DIR)\n",
        "        AREA_THR = best_cfg['AREA_THR']\n",
        "        MEAN_THR = best_cfg['MEAN_THR']\n",
        "        print(f\"Final Grid Search Parameters: AREA_THR={AREA_THR}, MEAN_THR={MEAN_THR}\")\n",
        "    else:\n",
        "        print(\"Skipping grid search because validation data lists are empty.\")\n",
        "else:\n",
        "    print(\"Skipping grid search because GRID_SEARCH_ENABLED flag is False.\")\n",
        "\n",
        "\n",
        "# --- FURTHER MODIFICATION FOR VALIDATION F1 SCORE CALCULATION ---\n",
        "from sklearn.metrics import f1_score\n",
        "val_items = [(p, 1) for p in val_forg[:10]] # This will be empty if val_forg is empty\n",
        "results = []\n",
        "if val_items:\n",
        "    print(\"\\nCalculating F1-score for validation forged images:\")\n",
        "    for p,_ in tqdm(val_items, desc=\"Validation forged-only\"):\n",
        "        try:\n",
        "            pil = Image.open(p).convert(\"RGB\")\n",
        "            label, m_pred, dbg = pipeline_final(pil)\n",
        "            m_gt = np.load(Path(MASK_DIR)/f\"{Path(p).stem}.npy\")\n",
        "            if m_gt.ndim==3: m_gt=np.max(m_gt,axis=0)\n",
        "            m_gt=(m_gt>0).astype(np.uint8)\n",
        "            m_pred=(m_pred>0).astype(np.uint8) if m_pred is not None else np.zeros_like(m_gt)\n",
        "            f1 = f1_score(m_gt.flatten(), m_pred.flatten(), zero_division=0)\n",
        "            results.append((Path(p).stem, f1, dbg))\n",
        "        except FileNotFoundError as e:\n",
        "            print(f\"Skipping {p} in validation F1 calculation due to FileNotFoundError: {e}\")\n",
        "            continue\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping {p} in validation F1 calculation due to unexpected error: {e}\")\n",
        "            continue\n",
        "\n",
        "    if results:\n",
        "        print(\"\\n F1-score par image falsifiÃ©e:\\n\")\n",
        "        for cid,f1,dbg in results:\n",
        "            print(f\"{cid} â€” F1={f1:.4f} | area={dbg['area']} mean={dbg['mean_inside']:.3f} thr={dbg['thr']:.3f}\")\n",
        "        print(f\"\\n Moyenne F1 (falsifiÃ©es) = {np.mean([r[1] for r in results]):.4f}\")\n",
        "    else:\n",
        "        print(\"\\n No results to display for validation F1-score (forged-only).\")\n",
        "else:\n",
        "    print(\"Skipping validation F1-score calculation (forged-only) because validation data is not available.\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Competition data not found at '/kaggle/input/recodai-luc-scientific-image-forgery-detection'. Skipping data loading, training setup, and grid search.\n",
            "Please ensure the 'recodai-luc-scientific-image-forgery-detection' dataset is available or adjust BASE_DIR.\n",
            "âœ… Loaded pretrained model from: cnndinov2_a78_model/model_seg_final.pt\n",
            "Skipping grid search because GRID_SEARCH_ENABLED flag is False.\n",
            "Skipping validation F1-score calculation (forged-only) because validation data is not available.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "297cced1"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Confirm that the DINOv2 and CNNDINOv2 model components are successfully loaded from the provided local archives.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2df9a6d"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "Yes, the DINOv2 and CNNDINOv2 model components were successfully loaded from the provided local archives. The DINOv2 processor and encoder were loaded from `dinov2_base`, and the CNNDINOv2 model weights were loaded from `cnndinov2_a78_model/model_seg_final.pt` after correcting the path.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   The `dinov2-pytorch-base-v1.tar.gz` archive was successfully extracted to the `dinov2_base` directory.\n",
        "*   The `CNNDINOv2-A78.zip` archive was successfully extracted to the `cnndinov2_a78_model` directory.\n",
        "*   Initial attempts to set the `MODEL_LOC` for CNNDINOv2 were incorrect, assuming a nested directory structure (`cnndinov2_a78_model/CNNDINOv2-A78/model_seg_final.pt`).\n",
        "*   A directory listing confirmed that the `model_seg_final.pt` file was directly located at the root of the `cnndinov2_a78_model` extraction directory.\n",
        "*   The `MODEL_LOC` variable was corrected to `cnndinov2_a78_model/model_seg_final.pt`, allowing for the successful loading of the CNNDINOv2 model weights.\n",
        "*   The DINOv2 model components (processor and encoder) were successfully loaded using `DINO_PATH = 'dinov2_base'`.\n",
        "*   The code includes robust error handling for missing competition data, preventing crashes and allowing the model loading process to complete successfully, albeit without data-dependent operations like grid search or F1-score calculation.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   Always verify the internal structure of extracted archives immediately after extraction to ensure correct file path referencing.\n",
        "*   Implement explicit validation steps after critical file operations (like extraction or path updates) to confirm successful execution and correct configuration before proceeding to dependent steps.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24eee489"
      },
      "source": [
        "# Task\n",
        "**Task**: Modify the `DinoSegmenter` model by setting the `encoder` parameters to be trainable (i.e., `p.requires_grad = True` or remove the line entirely) and then implement a complete training loop with `BCEWithLogitsLoss` as the loss function, `Adam` as the optimizer, and a suitable number of training epochs. Ensure the training loop handles forward passes, backpropagation, and parameter updates for the `DinoSegmenter` model, saving the best model based on performance on the validation set. Additionally, integrate data augmentation techniques into the `ForgerySegDataset` to improve model generalization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09822e1b"
      },
      "source": [
        "## Enable Fine-tuning of DINOv2 Encoder\n",
        "\n",
        "### Subtask:\n",
        "Modify the model definition to allow the DINOv2 encoder's parameters to be updated during training.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5fe3e38"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires modifying the `DinoSegmenter` class to allow the DINOv2 encoder's parameters to be updated during training. This involves commenting out the line that freezes the encoder's parameters in the `__init__` method of `DinoSegmenter` in the specified cell.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8b96a897",
        "outputId": "d42c5b19-c8e6-42bc-d45b-3e25a4004850"
      },
      "source": [
        "import os, cv2, json, math, random, torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn, torch.nn.functional as F, torch.optim as optim\n",
        "from transformers import AutoImageProcessor, AutoModel\n",
        "\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    # This forces CUDA to use deterministic algorithms (slower but consistent)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed_everything(42)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "BASE_DIR  = \"/kaggle/input/recodai-luc-scientific-image-forgery-detection\"\n",
        "AUTH_DIR  = f\"{BASE_DIR}/train_images/authentic\"\n",
        "FORG_DIR  = f\"{BASE_DIR}/train_images/forged\"\n",
        "MASK_DIR  = f\"{BASE_DIR}/train_masks\"\n",
        "TEST_DIR  = f\"{BASE_DIR}/test_images\"\n",
        "\n",
        "# Updated paths from previous extraction steps\n",
        "DINO_PATH = 'dinov2_base' # Updated to local extraction directory\n",
        "MODEL_LOC = 'cnndinov2_a78_model/model_seg_final.pt' # Corrected path based on directory listing\n",
        "\n",
        "IMG_SIZE = 518\n",
        "BATCH_SIZE = 2\n",
        "\n",
        "# INFERENCE UTILS\n",
        "AREA_THR = 200\n",
        "MEAN_THR = 0.22\n",
        "USE_TTA = True\n",
        "GRID_SEARCH_ENABLED = True # Renamed to avoid conflict if GRID_SEARCH is set to False later\n",
        "\n",
        "class ForgerySegDataset(Dataset):\n",
        "    def __init__(self, auth_paths, forg_paths, mask_dir, img_size=IMG_SIZE):\n",
        "        self.samples = []\n",
        "        for p in forg_paths:\n",
        "            m = os.path.join(mask_dir, Path(p).stem + \".npy\")\n",
        "            if os.path.exists(m):\n",
        "                self.samples.append((p, m))\n",
        "        for p in auth_paths:\n",
        "            self.samples.append((p, None))\n",
        "        self.img_size = img_size\n",
        "    def __len__(self): return len(self.samples)\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, mask_path = self.samples[idx]\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        w, h = img.size\n",
        "        if mask_path is None:\n",
        "            mask = np.zeros((h, w), np.uint8)\n",
        "        else:\n",
        "            m = np.load(mask_path)\n",
        "            if m.ndim == 3: m = np.max(m, axis=0)\n",
        "            mask = (m > 0).astype(np.uint8)\n",
        "        img_r = img.resize((IMG_SIZE, IMG_SIZE))\n",
        "        mask_r = cv2.resize(mask, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_NEAREST)\n",
        "        img_t = torch.from_numpy(np.array(img_r, np.float32)/255.).permute(2,0,1)\n",
        "        mask_t = torch.from_numpy(mask_r[None, ...].astype(np.float32))\n",
        "        return img_t, mask_t\n",
        "\n",
        "\n",
        "#  MODEL (DINOv2 + Decoder)\n",
        "\n",
        "from transformers import AutoImageProcessor, AutoModel\n",
        "processor = AutoImageProcessor.from_pretrained(DINO_PATH, local_files_only=True, use_fast=False)\n",
        "encoder = AutoModel.from_pretrained(DINO_PATH, local_files_only=True).eval().to(device)\n",
        "\n",
        "class DinoTinyDecoder(nn.Module):\n",
        "    def __init__(self, in_ch=768, out_ch=1):\n",
        "        super().__init__()\n",
        "        # Block 1: 768 -> 384\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, 384, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout2d(0.1)\n",
        "        )\n",
        "        # Block 2: 384 -> 192\n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.Conv2d(384, 192, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout2d(0.1)\n",
        "        )\n",
        "        # Block 3: 192 -> 96\n",
        "        self.block3 = nn.Sequential(\n",
        "            nn.Conv2d(192, 96, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        # Final Output: 96 -> 1\n",
        "        self.conv_out = nn.Conv2d(96, out_ch, kernel_size=1)\n",
        "\n",
        "    def forward(self, f, target_size):\n",
        "        # f: [B, 768, 37, 37]\n",
        "\n",
        "        # Step 1: Up to ~74x74\n",
        "        x = F.interpolate(self.block1(f), size=(74, 74), mode='bilinear', align_corners=False)\n",
        "\n",
        "        # Step 2: Up to ~148x148\n",
        "        x = F.interpolate(self.block2(x), size=(148, 148), mode='bilinear', align_corners=False)\n",
        "\n",
        "        # Step 3: Up to ~296x296\n",
        "        x = F.interpolate(self.block3(x), size=(296, 296), mode='bilinear', align_corners=False)\n",
        "\n",
        "        # Step 4: Final jump to 518x518\n",
        "        x = self.conv_out(x)\n",
        "        x = F.interpolate(x, size=target_size, mode='bilinear', align_corners=False)\n",
        "\n",
        "        return x\n",
        "\n",
        "class DinoSegmenter(nn.Module):\n",
        "    def __init__(self, encoder, processor):\n",
        "        super().__init__()\n",
        "        self.encoder, self.processor = encoder, processor\n",
        "        # Removed or commented out the line freezing encoder parameters to enable fine-tuning\n",
        "        # for p in self.encoder.parameters(): p.requires_grad = False\n",
        "        self.seg_head = DinoTinyDecoder(768,1)\n",
        "    def forward_features(self,x):\n",
        "        imgs = (x*255).clamp(0,255).byte().permute(0,2,3,1).cpu().numpy()\n",
        "        inputs = self.processor(images=list(imgs), return_tensors=\"pt\").to(x.device)\n",
        "        # with torch.no_grad():\n",
        "        #     feats = self.encoder(**inputs).last_hidden_state\n",
        "        feats = self.encoder(**inputs).last_hidden_state\n",
        "        B,N,C = feats.shape\n",
        "        fmap = feats[:,1:,:].permute(0,2,1)\n",
        "        s = int(math.sqrt(N-1))\n",
        "        fmap = fmap.reshape(B,C,s,s)\n",
        "        return fmap\n",
        "    def forward_seg(self,x):\n",
        "        fmap = self.forward_features(x)\n",
        "        return self.seg_head(fmap,(IMG_SIZE,IMG_SIZE))\n",
        "\n",
        "\n",
        "# --- MODIFIED PART TO HANDLE MISSING BASE_DIR ---\n",
        "# Initialize data-related variables as empty lists or None\n",
        "auth_imgs, forg_imgs, train_auth, val_auth, train_forg, val_forg = [], [], [], [], [], []\n",
        "train_loader, val_loader = None, None\n",
        "\n",
        "if not os.path.exists(BASE_DIR):\n",
        "    print(f\"Warning: Competition data not found at '{BASE_DIR}'. Skipping data loading, training setup, and grid search.\")\n",
        "    print(\"Please ensure the 'recodai-luc-scientific-image-forgery-detection' dataset is available or adjust BASE_DIR.\")\n",
        "    GRID_SEARCH_ENABLED = False # Disable grid search if data is not available\n",
        "else:\n",
        "    auth_imgs = sorted([str(Path(AUTH_DIR)/f) for f in os.listdir(AUTH_DIR)])\n",
        "    forg_imgs = sorted([str(Path(FORG_DIR)/f) for f in os.listdir(FORG_DIR)])\n",
        "    train_auth, val_auth = train_test_split(auth_imgs, test_size=0.2, random_state=42)\n",
        "    train_forg, val_forg = train_test_split(forg_imgs, test_size=0.2, random_state=42)\n",
        "\n",
        "    train_loader = DataLoader(ForgerySegDataset(train_auth, train_forg, MASK_DIR),\n",
        "                              batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(ForgerySegDataset(val_auth, val_forg, MASK_DIR),\n",
        "                            batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "model_seg = DinoSegmenter(encoder, processor).to(device)\n",
        "\n",
        "# Load pretrained weights if MODEL_LOC is specified\n",
        "if MODEL_LOC is not None and os.path.exists(MODEL_LOC):\n",
        "    model_seg.load_state_dict(torch.load(MODEL_LOC, map_location=device))\n",
        "    print(f\"âœ… Loaded pretrained model from: {MODEL_LOC}\")\n",
        "    model_seg.eval()  # Set model to evaluation mode\n",
        "else:\n",
        "    print(f\"âš ï¸ Warning: Model weights not found at '{MODEL_LOC}'. Model will use randomly initialized weights (if applicable).\")\n",
        "\n",
        "@torch.no_grad()\n",
        "def segment_prob_map(pil):\n",
        "    x = torch.from_numpy(np.array(pil.resize((IMG_SIZE, IMG_SIZE)), np.float32)/255.).permute(2,0,1)[None].to(device)\n",
        "    prob = torch.sigmoid(model_seg.forward_seg(x))[0,0].cpu().numpy()\n",
        "    return prob\n",
        "\n",
        "@torch.no_grad()\n",
        "def segment_prob_map_with_tta(pil):\n",
        "    # 1. Preprocessing: Resize, Normalize, and move to Device\n",
        "    x = torch.from_numpy(np.array(pil.resize((IMG_SIZE, IMG_SIZE)), np.float32)/255.).permute(2,0,1)[None].to(device)\n",
        "\n",
        "    predictions = []\n",
        "\n",
        "    # 2. Original Prediction\n",
        "    pred_orig = torch.sigmoid(model_seg.forward_seg(x))\n",
        "    predictions.append(pred_orig)\n",
        "\n",
        "    # 3. Horizontal Flip TTA (dim 3)\n",
        "    # Flip input -> Predict -> Flip output back\n",
        "    pred_h = torch.sigmoid(model_seg.forward_seg(torch.flip(x, dims=[3])))\n",
        "    predictions.append(torch.flip(pred_h, dims=[3]))\n",
        "\n",
        "    # 4. Vertical Flip TTA (dim 2)\n",
        "    # Flip input -> Predict -> Flip output back\n",
        "    pred_v = torch.sigmoid(model_seg.forward_seg(torch.flip(x, dims=[2])))\n",
        "    predictions.append(torch.flip(pred_v, dims=[2]))\n",
        "\n",
        "    # 5. Average the predictions and format as numpy\n",
        "    # We stack the 3 predictions and take the mean across the stack dimension (0)\n",
        "    prob = torch.stack(predictions).mean(0)[0, 0].cpu().numpy()\n",
        "\n",
        "    return prob\n",
        "\n",
        "def enhanced_adaptive_mask(prob, alpha_grad=0.45):\n",
        "    gx = cv2.Sobel(prob, cv2.CV_32F, 1, 0, ksize=3)\n",
        "    gy = cv2.Sobel(prob, cv2.CV_32F, 0, 1, ksize=3)\n",
        "    grad_mag = np.sqrt(gx**2 + gy**2)\n",
        "    grad_norm = grad_mag / (grad_mag.max() + 1e-6)\n",
        "    enhanced = (1 - alpha_grad) * prob + alpha_grad * grad_norm\n",
        "    enhanced = cv2.GaussianBlur(enhanced, (3,3), 0)\n",
        "    thr = np.mean(enhanced) + 0.3 * np.std(enhanced)\n",
        "    mask = (enhanced > thr).astype(np.uint8)\n",
        "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, np.ones((5,5), np.uint8))\n",
        "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, np.ones((3,3), np.uint8))\n",
        "    return mask, thr\n",
        "\n",
        "def finalize_mask(prob, orig_size):\n",
        "    mask, thr = enhanced_adaptive_mask(prob)\n",
        "    mask = cv2.resize(mask, orig_size, interpolation=cv2.INTER_NEAREST)\n",
        "    return mask, thr\n",
        "\n",
        "def pipeline_final(pil):\n",
        "    if USE_TTA:\n",
        "        prob = segment_prob_map_with_tta(pil)\n",
        "    else:\n",
        "        prob = segment_prob_map(pil)\n",
        "    mask, thr = finalize_mask(prob, pil.size)\n",
        "    area = int(mask.sum())\n",
        "    mean_inside = float(prob[cv2.resize(mask,(IMG_SIZE,IMG_SIZE),interpolation=cv2.INTER_NEAREST)==1].mean()) if area>0 else 0.0\n",
        "    if area < AREA_THR or mean_inside < MEAN_THR:\n",
        "        return \"authentic\", None, {\"area\": area, \"mean_inside\": mean_inside, \"thr\": thr}\n",
        "    return \"forged\", mask, {\"area\": area, \"mean_inside\": mean_inside, \"thr\": thr}\n",
        "\n",
        "import itertools\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def grid_search_area_mean(forg_paths, auth_paths, mask_dir):\n",
        "    # Return default values and print message if no valid data for grid search\n",
        "    if not forg_paths and not auth_paths:\n",
        "        print(\"Skipping grid search as validation data is not available.\")\n",
        "        return {'AREA_THR': 200, 'MEAN_THR': 0.22}\n",
        "\n",
        "    mean_range = [round(x, 2) for x in np.arange(0.20, 0.291, 0.01)]\n",
        "    area_range = [200]\n",
        "    val_set = [(p, \"forged\") for p in forg_paths] + [(p, \"authentic\") for p in auth_paths]\n",
        "\n",
        "    if not val_set:\n",
        "        print(\"Skipping grid search as validation set is empty.\")\n",
        "        return {'AREA_THR': 200, 'MEAN_THR': 0.22}\n",
        "\n",
        "    print(f\"ðŸš€ Step 1: Caching probability maps for ALL {len(val_set)} images...\")\n",
        "    cache = []\n",
        "    for p, label in tqdm(val_set):\n",
        "        try:\n",
        "            pil = Image.open(p).convert(\"RGB\")\n",
        "            w, h = pil.size\n",
        "\n",
        "            prob = segment_prob_map_with_tta(pil) if USE_TTA else segment_prob_map(pil)\n",
        "\n",
        "            mask_raw, _ = enhanced_adaptive_mask(prob)\n",
        "            mask_resized = cv2.resize(mask_raw, (w, h), interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "            if label == \"forged\":\n",
        "                m_gt = np.load(Path(mask_dir)/f\"{Path(p).stem}.npy\")\n",
        "                if m_gt.ndim == 3: m_gt = np.max(m_gt, axis=0)\n",
        "                m_gt = (m_gt > 0).astype(np.uint8)\n",
        "            else:\n",
        "                m_gt = np.zeros((h, w), np.uint8)\n",
        "\n",
        "            cache.append({\"prob\": prob, \"mask\": mask_resized, \"gt\": m_gt, \"label\": label})\n",
        "        except FileNotFoundError as e:\n",
        "            print(f\"Skipping {p} due to FileNotFoundError during caching: {e}\")\n",
        "            continue\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping {p} due to unexpected error during caching: {e}\")\n",
        "            continue\n",
        "\n",
        "    if not cache: # If no items were successfully cached, return default params\n",
        "        print(\"No valid images found for grid search caching. Returning default parameters.\")\n",
        "        return {'AREA_THR': 200, 'MEAN_THR': 0.22}\n",
        "\n",
        "    best_f1 = -1\n",
        "    best_params = {}\n",
        "    combinations = list(itertools.product(area_range, mean_range))\n",
        "\n",
        "    for a_thr, m_thr in combinations:\n",
        "        current_f1s = []\n",
        "        for item in cache:\n",
        "            mask = item[\"mask\"]\n",
        "            area = int(mask.sum())\n",
        "\n",
        "            mask_small = cv2.resize(mask, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_NEAREST)\n",
        "            mean_in = float(item[\"prob\"][mask_small == 1].mean()) if area > 0 else 0.0\n",
        "\n",
        "            is_forged = (area >= a_thr and mean_in >= m_thr)\n",
        "            m_pred = (mask > 0).astype(np.uint8) if is_forged else np.zeros_like(item[\"gt\"])\n",
        "\n",
        "            f1 = f1_score(item[\"gt\"].flatten(), m_pred.flatten(),\n",
        "                          zero_division=1 if item[\"label\"] == \"authentic\" else 0)\n",
        "            current_f1s.append(f1)\n",
        "\n",
        "        if current_f1s: # Only calculate average if any F1 scores were computed\n",
        "            avg_f1 = np.mean(current_f1s)\n",
        "            if avg_f1 > best_f1:\n",
        "                best_f1 = avg_f1\n",
        "                best_params = {\"AREA_THR\": a_thr, \"MEAN_THR\": m_thr}\n",
        "                print(f\"â­ New Best F1: {best_f1:.4f} -> AREA: {a_thr}, MEAN: {m_thr}\")\n",
        "\n",
        "    if not best_params: # If no suitable parameters found, return defaults\n",
        "        print(\"No optimal parameters found during grid search. Returning default parameters.\")\n",
        "        return {'AREA_THR': 200, 'MEAN_THR': 0.22}\n",
        "\n",
        "    return best_params\n",
        "\n",
        "if GRID_SEARCH_ENABLED:\n",
        "    if val_forg or val_auth: # Only run grid search if there's actual validation data\n",
        "        print(\"Starting grid search for optimal parameters...\")\n",
        "        best_cfg = grid_search_area_mean(val_forg, val_auth, MASK_DIR)\n",
        "        AREA_THR = best_cfg['AREA_THR']\n",
        "        MEAN_THR = best_cfg['MEAN_THR']\n",
        "        print(f\"Final Grid Search Parameters: AREA_THR={AREA_THR}, MEAN_THR={MEAN_THR}\")\n",
        "    else:\n",
        "        print(\"Skipping grid search because validation data lists are empty.\")\n",
        "else:\n",
        "    print(\"Skipping grid search because GRID_SEARCH_ENABLED flag is False.\")\n",
        "\n",
        "\n",
        "# --- FURTHER MODIFICATION FOR VALIDATION F1 SCORE CALCULATION ---\n",
        "from sklearn.metrics import f1_score\n",
        "val_items = [(p, 1) for p in val_forg[:10]] # This will be empty if val_forg is empty\n",
        "results = []\n",
        "if val_items:\n",
        "    print(\"\\nCalculating F1-score for validation forged images:\")\n",
        "    for p,_ in tqdm(val_items, desc=\"Validation forged-only\"):\n",
        "        try:\n",
        "            pil = Image.open(p).convert(\"RGB\")\n",
        "            label, m_pred, dbg = pipeline_final(pil)\n",
        "            m_gt = np.load(Path(MASK_DIR)/f\"{Path(p).stem}.npy\")\n",
        "            if m_gt.ndim==3: m_gt=np.max(m_gt,axis=0)\n",
        "            m_gt=(m_gt>0).astype(np.uint8)\n",
        "            m_pred=(m_pred>0).astype(np.uint8) if m_pred is not None else np.zeros_like(m_gt)\n",
        "            f1 = f1_score(m_gt.flatten(), m_pred.flatten(), zero_division=0)\n",
        "            results.append((Path(p).stem, f1, dbg))\n",
        "        except FileNotFoundError as e:\n",
        "            print(f\"Skipping {p} in validation F1 calculation due to FileNotFoundError: {e}\")\n",
        "            continue\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping {p} in validation F1 calculation due to unexpected error: {e}\")\n",
        "            continue\n",
        "\n",
        "    if results:\n",
        "        print(\"\\n F1-score par image falsifiÃ©e:\\n\")\n",
        "        for cid,f1,dbg in results:\n",
        "            print(f\"{cid} â€” F1={f1:.4f} | area={dbg['area']} mean={dbg['mean_inside']:.3f} thr={dbg['thr']:.3f}\")\n",
        "        print(f\"\\n Moyenne F1 (falsifiÃ©es) = {np.mean([r[1] for r in results]):.4f}\")\n",
        "    else:\n",
        "        print(\"\\n No results to display for validation F1-score (forged-only).\")\n",
        "else:\n",
        "    print(\"Skipping validation F1-score calculation (forged-only) because validation data is not available.\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Competition data not found at '/kaggle/input/recodai-luc-scientific-image-forgery-detection'. Skipping data loading, training setup, and grid search.\n",
            "Please ensure the 'recodai-luc-scientific-image-forgery-detection' dataset is available or adjust BASE_DIR.\n",
            "âœ… Loaded pretrained model from: cnndinov2_a78_model/model_seg_final.pt\n",
            "Skipping grid search because GRID_SEARCH_ENABLED flag is False.\n",
            "Skipping validation F1-score calculation (forged-only) because validation data is not available.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9873727f"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask is to enable fine-tuning of the DINOv2 encoder by modifying the `DinoSegmenter` class. This involves commenting out the line that sets `requires_grad = False` for the encoder's parameters within the `__init__` method. The code block below reflects this change.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5eb59d1",
        "outputId": "ad894f1d-07ea-401a-c7b6-932b7ba387fe"
      },
      "source": [
        "import os, cv2, json, math, random, torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn, torch.nn.functional as F, torch.optim as optim\n",
        "from transformers import AutoImageProcessor, AutoModel\n",
        "\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    # This forces CUDA to use deterministic algorithms (slower but consistent)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed_everything(42)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "BASE_DIR  = \"/kaggle/input/recodai-luc-scientific-image-forgery-detection\"\n",
        "AUTH_DIR  = f\"{BASE_DIR}/train_images/authentic\"\n",
        "FORG_DIR  = f\"{BASE_DIR}/train_images/forged\"\n",
        "MASK_DIR  = f\"{BASE_DIR}/train_masks\"\n",
        "TEST_DIR  = f\"{BASE_DIR}/test_images\"\n",
        "\n",
        "# Updated paths from previous extraction steps\n",
        "DINO_PATH = 'dinov2_base' # Updated to local extraction directory\n",
        "MODEL_LOC = 'cnndinov2_a78_model/model_seg_final.pt' # Corrected path based on directory listing\n",
        "\n",
        "IMG_SIZE = 518\n",
        "BATCH_SIZE = 2\n",
        "\n",
        "# INFERENCE UTILS\n",
        "AREA_THR = 200\n",
        "MEAN_THR = 0.22\n",
        "USE_TTA = True\n",
        "GRID_SEARCH_ENABLED = True # Renamed to avoid conflict if GRID_SEARCH is set to False later\n",
        "\n",
        "class ForgerySegDataset(Dataset):\n",
        "    def __init__(self, auth_paths, forg_paths, mask_dir, img_size=IMG_SIZE):\n",
        "        self.samples = []\n",
        "        for p in forg_paths:\n",
        "            m = os.path.join(mask_dir, Path(p).stem + \".npy\")\n",
        "            if os.path.exists(m):\n",
        "                self.samples.append((p, m))\n",
        "        for p in auth_paths:\n",
        "            self.samples.append((p, None))\n",
        "        self.img_size = img_size\n",
        "    def __len__(self): return len(self.samples)\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, mask_path = self.samples[idx]\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        w, h = img.size\n",
        "        if mask_path is None:\n",
        "            mask = np.zeros((h, w), np.uint8)\n",
        "        else:\n",
        "            m = np.load(mask_path)\n",
        "            if m.ndim == 3: m = np.max(m, axis=0)\n",
        "            mask = (m > 0).astype(np.uint8)\n",
        "        img_r = img.resize((IMG_SIZE, IMG_SIZE))\n",
        "        mask_r = cv2.resize(mask, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_NEAREST)\n",
        "        img_t = torch.from_numpy(np.array(img_r, np.float32)/255.).permute(2,0,1)\n",
        "        mask_t = torch.from_numpy(mask_r[None, ...].astype(np.float32))\n",
        "        return img_t, mask_t\n",
        "\n",
        "\n",
        "#  MODEL (DINOv2 + Decoder)\n",
        "\n",
        "from transformers import AutoImageProcessor, AutoModel\n",
        "processor = AutoImageProcessor.from_pretrained(DINO_PATH, local_files_only=True, use_fast=False)\n",
        "encoder = AutoModel.from_pretrained(DINO_PATH, local_files_only=True).eval().to(device)\n",
        "\n",
        "class DinoTinyDecoder(nn.Module):\n",
        "    def __init__(self, in_ch=768, out_ch=1):\n",
        "        super().__init__()\n",
        "        # Block 1: 768 -> 384\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, 384, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout2d(0.1)\n",
        "        )\n",
        "        # Block 2: 384 -> 192\n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.Conv2d(384, 192, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout2d(0.1)\n",
        "        )\n",
        "        # Block 3: 192 -> 96\n",
        "        self.block3 = nn.Sequential(\n",
        "            nn.Conv2d(192, 96, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        # Final Output: 96 -> 1\n",
        "        self.conv_out = nn.Conv2d(96, out_ch, kernel_size=1)\n",
        "\n",
        "    def forward(self, f, target_size):\n",
        "        # f: [B, 768, 37, 37]\n",
        "\n",
        "        # Step 1: Up to ~74x74\n",
        "        x = F.interpolate(self.block1(f), size=(74, 74), mode='bilinear', align_corners=False)\n",
        "\n",
        "        # Step 2: Up to ~148x148\n",
        "        x = F.interpolate(self.block2(x), size=(148, 148), mode='bilinear', align_corners=False)\n",
        "\n",
        "        # Step 3: Up to ~296x296\n",
        "        x = F.interpolate(self.block3(x), size=(296, 296), mode='bilinear', align_corners=False)\n",
        "\n",
        "        # Step 4: Final jump to 518x518\n",
        "        x = self.conv_out(x)\n",
        "        x = F.interpolate(x, size=target_size, mode='bilinear', align_corners=False)\n",
        "\n",
        "        return x\n",
        "\n",
        "class DinoSegmenter(nn.Module):\n",
        "    def __init__(self, encoder, processor):\n",
        "        super().__init__()\n",
        "        self.encoder, self.processor = encoder, processor\n",
        "        # Removed or commented out the line freezing encoder parameters to enable fine-tuning\n",
        "        # for p in self.encoder.parameters(): p.requires_grad = False\n",
        "        self.seg_head = DinoTinyDecoder(768,1)\n",
        "    def forward_features(self,x):\n",
        "        imgs = (x*255).clamp(0,255).byte().permute(0,2,3,1).cpu().numpy()\n",
        "        inputs = self.processor(images=list(imgs), return_tensors=\"pt\").to(x.device)\n",
        "        # with torch.no_grad():\n",
        "        #     feats = self.encoder(**inputs).last_hidden_state\n",
        "        feats = self.encoder(**inputs).last_hidden_state\n",
        "        B,N,C = feats.shape\n",
        "        fmap = feats[:,1:,:].permute(0,2,1)\n",
        "        s = int(math.sqrt(N-1))\n",
        "        fmap = fmap.reshape(B,C,s,s)\n",
        "        return fmap\n",
        "    def forward_seg(self,x):\n",
        "        fmap = self.forward_features(x)\n",
        "        return self.seg_head(fmap,(IMG_SIZE,IMG_SIZE))\n",
        "\n",
        "\n",
        "# --- MODIFIED PART TO HANDLE MISSING BASE_DIR ---\n",
        "# Initialize data-related variables as empty lists or None\n",
        "auth_imgs, forg_imgs, train_auth, val_auth, train_forg, val_forg = [], [], [], [], [], []\n",
        "train_loader, val_loader = None, None\n",
        "\n",
        "if not os.path.exists(BASE_DIR):\n",
        "    print(f\"Warning: Competition data not found at '{BASE_DIR}'. Skipping data loading, training setup, and grid search.\")\n",
        "    print(\"Please ensure the 'recodai-luc-scientific-image-forgery-detection' dataset is available or adjust BASE_DIR.\")\n",
        "    GRID_SEARCH_ENABLED = False # Disable grid search if data is not available\n",
        "else:\n",
        "    auth_imgs = sorted([str(Path(AUTH_DIR)/f) for f in os.listdir(AUTH_DIR)])\n",
        "    forg_imgs = sorted([str(Path(FORG_DIR)/f) for f in os.listdir(FORG_DIR)])\n",
        "    train_auth, val_auth = train_test_split(auth_imgs, test_size=0.2, random_state=42)\n",
        "    train_forg, val_forg = train_test_split(forg_imgs, test_size=0.2, random_state=42)\n",
        "\n",
        "    train_loader = DataLoader(ForgerySegDataset(train_auth, train_forg, MASK_DIR),\n",
        "                              batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(ForgerySegDataset(val_auth, val_forg, MASK_DIR),\n",
        "                            batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "model_seg = DinoSegmenter(encoder, processor).to(device)\n",
        "\n",
        "# Load pretrained weights if MODEL_LOC is specified\n",
        "if MODEL_LOC is not None and os.path.exists(MODEL_LOC):\n",
        "    model_seg.load_state_dict(torch.load(MODEL_LOC, map_location=device))\n",
        "    print(f\"âœ… Loaded pretrained model from: {MODEL_LOC}\")\n",
        "    model_seg.eval()  # Set model to evaluation mode\n",
        "else:\n",
        "    print(f\"âš ï¸ Warning: Model weights not found at '{MODEL_LOC}'. Model will use randomly initialized weights (if applicable).\")\n",
        "\n",
        "@torch.no_grad()\n",
        "def segment_prob_map(pil):\n",
        "    x = torch.from_numpy(np.array(pil.resize((IMG_SIZE, IMG_SIZE)), np.float32)/255.).permute(2,0,1)[None].to(device)\n",
        "    prob = torch.sigmoid(model_seg.forward_seg(x))[0,0].cpu().numpy()\n",
        "    return prob\n",
        "\n",
        "@torch.no_grad()\n",
        "def segment_prob_map_with_tta(pil):\n",
        "    # 1. Preprocessing: Resize, Normalize, and move to Device\n",
        "    x = torch.from_numpy(np.array(pil.resize((IMG_SIZE, IMG_SIZE)), np.float32)/255.).permute(2,0,1)[None].to(device)\n",
        "\n",
        "    predictions = []\n",
        "\n",
        "    # 2. Original Prediction\n",
        "    pred_orig = torch.sigmoid(model_seg.forward_seg(x))\n",
        "    predictions.append(pred_orig)\n",
        "\n",
        "    # 3. Horizontal Flip TTA (dim 3)\n",
        "    # Flip input -> Predict -> Flip output back\n",
        "    pred_h = torch.sigmoid(model_seg.forward_seg(torch.flip(x, dims=[3])))\n",
        "    predictions.append(torch.flip(pred_h, dims=[3]))\n",
        "\n",
        "    # 4. Vertical Flip TTA (dim 2)\n",
        "    # Flip input -> Predict -> Flip output back\n",
        "    pred_v = torch.sigmoid(model_seg.forward_seg(torch.flip(x, dims=[2])))\n",
        "    predictions.append(torch.flip(pred_v, dims=[2]))\n",
        "\n",
        "    # 5. Average the predictions and format as numpy\n",
        "    # We stack the 3 predictions and take the mean across the stack dimension (0)\n",
        "    prob = torch.stack(predictions).mean(0)[0, 0].cpu().numpy()\n",
        "\n",
        "    return prob\n",
        "\n",
        "def enhanced_adaptive_mask(prob, alpha_grad=0.45):\n",
        "    gx = cv2.Sobel(prob, cv2.CV_32F, 1, 0, ksize=3)\n",
        "    gy = cv2.Sobel(prob, cv2.CV_32F, 0, 1, ksize=3)\n",
        "    grad_mag = np.sqrt(gx**2 + gy**2)\n",
        "    grad_norm = grad_mag / (grad_mag.max() + 1e-6)\n",
        "    enhanced = (1 - alpha_grad) * prob + alpha_grad * grad_norm\n",
        "    enhanced = cv2.GaussianBlur(enhanced, (3,3), 0)\n",
        "    thr = np.mean(enhanced) + 0.3 * np.std(enhanced)\n",
        "    mask = (enhanced > thr).astype(np.uint8)\n",
        "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, np.ones((5,5), np.uint8))\n",
        "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, np.ones((3,3), np.uint8))\n",
        "    return mask, thr\n",
        "\n",
        "def finalize_mask(prob, orig_size):\n",
        "    mask, thr = enhanced_adaptive_mask(prob)\n",
        "    mask = cv2.resize(mask, orig_size, interpolation=cv2.INTER_NEAREST)\n",
        "    return mask, thr\n",
        "\n",
        "def pipeline_final(pil):\n",
        "    if USE_TTA:\n",
        "        prob = segment_prob_map_with_tta(pil)\n",
        "    else:\n",
        "        prob = segment_prob_map(pil)\n",
        "    mask, thr = finalize_mask(prob, pil.size)\n",
        "    area = int(mask.sum())\n",
        "    mean_inside = float(prob[cv2.resize(mask,(IMG_SIZE,IMG_SIZE),interpolation=cv2.INTER_NEAREST)==1].mean()) if area>0 else 0.0\n",
        "    if area < AREA_THR or mean_inside < MEAN_THR:\n",
        "        return \"authentic\", None, {\"area\": area, \"mean_inside\": mean_inside, \"thr\": thr}\n",
        "    return \"forged\", mask, {\"area\": area, \"mean_inside\": mean_inside, \"thr\": thr}\n",
        "\n",
        "import itertools\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def grid_search_area_mean(forg_paths, auth_paths, mask_dir):\n",
        "    # Return default values and print message if no valid data for grid search\n",
        "    if not forg_paths and not auth_paths:\n",
        "        print(\"Skipping grid search as validation data is not available.\")\n",
        "        return {'AREA_THR': 200, 'MEAN_THR': 0.22}\n",
        "\n",
        "    mean_range = [round(x, 2) for x in np.arange(0.20, 0.291, 0.01)]\n",
        "    area_range = [200]\n",
        "    val_set = [(p, \"forged\") for p in forg_paths] + [(p, \"authentic\") for p in auth_paths]\n",
        "\n",
        "    if not val_set:\n",
        "        print(\"Skipping grid search as validation set is empty.\")\n",
        "        return {'AREA_THR': 200, 'MEAN_THR': 0.22}\n",
        "\n",
        "    print(f\"ðŸš€ Step 1: Caching probability maps for ALL {len(val_set)} images...\")\n",
        "    cache = []\n",
        "    for p, label in tqdm(val_set):\n",
        "        try:\n",
        "            pil = Image.open(p).convert(\"RGB\")\n",
        "            w, h = pil.size\n",
        "\n",
        "            prob = segment_prob_map_with_tta(pil) if USE_TTA else segment_prob_map(pil)\n",
        "\n",
        "            mask_raw, _ = enhanced_adaptive_mask(prob)\n",
        "            mask_resized = cv2.resize(mask_raw, (w, h), interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "            if label == \"forged\":\n",
        "                m_gt = np.load(Path(mask_dir)/f\"{Path(p).stem}.npy\")\n",
        "                if m_gt.ndim == 3: m_gt = np.max(m_gt, axis=0)\n",
        "                m_gt = (m_gt > 0).astype(np.uint8)\n",
        "            else:\n",
        "                m_gt = np.zeros((h, w), np.uint8)\n",
        "\n",
        "            cache.append({\"prob\": prob, \"mask\": mask_resized, \"gt\": m_gt, \"label\": label})\n",
        "        except FileNotFoundError as e:\n",
        "            print(f\"Skipping {p} due to FileNotFoundError during caching: {e}\")\n",
        "            continue\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping {p} due to unexpected error during caching: {e}\")\n",
        "            continue\n",
        "\n",
        "    if not cache: # If no items were successfully cached, return default params\n",
        "        print(\"No valid images found for grid search caching. Returning default parameters.\")\n",
        "        return {'AREA_THR': 200, 'MEAN_THR': 0.22}\n",
        "\n",
        "    best_f1 = -1\n",
        "    best_params = {}\n",
        "    combinations = list(itertools.product(area_range, mean_range))\n",
        "\n",
        "    for a_thr, m_thr in combinations:\n",
        "        current_f1s = []\n",
        "        for item in cache:\n",
        "            mask = item[\"mask\"]\n",
        "            area = int(mask.sum())\n",
        "\n",
        "            mask_small = cv2.resize(mask, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_NEAREST)\n",
        "            mean_in = float(item[\"prob\"][mask_small == 1].mean()) if area > 0 else 0.0\n",
        "\n",
        "            is_forged = (area >= a_thr and mean_in >= m_thr)\n",
        "            m_pred = (mask > 0).astype(np.uint8) if is_forged else np.zeros_like(item[\"gt\"])\n",
        "\n",
        "            f1 = f1_score(item[\"gt\"].flatten(), m_pred.flatten(),\n",
        "                          zero_division=1 if item[\"label\"] == \"authentic\" else 0)\n",
        "            current_f1s.append(f1)\n",
        "\n",
        "        if current_f1s: # Only calculate average if any F1 scores were computed\n",
        "            avg_f1 = np.mean(current_f1s)\n",
        "            if avg_f1 > best_f1:\n",
        "                best_f1 = avg_f1\n",
        "                best_params = {\"AREA_THR\": a_thr, \"MEAN_THR\": m_thr}\n",
        "                print(f\"â­ New Best F1: {best_f1:.4f} -> AREA: {a_thr}, MEAN: {m_thr}\")\n",
        "\n",
        "    if not best_params: # If no suitable parameters found, return defaults\n",
        "        print(\"No optimal parameters found during grid search. Returning default parameters.\")\n",
        "        return {'AREA_THR': 200, 'MEAN_THR': 0.22}\n",
        "\n",
        "    return best_params\n",
        "\n",
        "if GRID_SEARCH_ENABLED:\n",
        "    if val_forg or val_auth: # Only run grid search if there's actual validation data\n",
        "        print(\"Starting grid search for optimal parameters...\")\n",
        "        best_cfg = grid_search_area_mean(val_forg, val_auth, MASK_DIR)\n",
        "        AREA_THR = best_cfg['AREA_THR']\n",
        "        MEAN_THR = best_cfg['MEAN_THR']\n",
        "        print(f\"Final Grid Search Parameters: AREA_THR={AREA_THR}, MEAN_THR={MEAN_THR}\")\n",
        "    else:\n",
        "        print(\"Skipping grid search because validation data lists are empty.\")\n",
        "else:\n",
        "    print(\"Skipping grid search because GRID_SEARCH_ENABLED flag is False.\")\n",
        "\n",
        "\n",
        "# --- FURTHER MODIFICATION FOR VALIDATION F1 SCORE CALCULATION ---\n",
        "from sklearn.metrics import f1_score\n",
        "val_items = [(p, 1) for p in val_forg[:10]] # This will be empty if val_forg is empty\n",
        "results = []\n",
        "if val_items:\n",
        "    print(\"\\nCalculating F1-score for validation forged images:\")\n",
        "    for p,_ in tqdm(val_items, desc=\"Validation forged-only\"):\n",
        "        try:\n",
        "            pil = Image.open(p).convert(\"RGB\")\n",
        "            label, m_pred, dbg = pipeline_final(pil)\n",
        "            m_gt = np.load(Path(MASK_DIR)/f\"{Path(p).stem}.npy\")\n",
        "            if m_gt.ndim==3: m_gt=np.max(m_gt,axis=0)\n",
        "            m_gt=(m_gt>0).astype(np.uint8)\n",
        "            m_pred=(m_pred>0).astype(np.uint8) if m_pred is not None else np.zeros_like(m_gt)\n",
        "            f1 = f1_score(m_gt.flatten(), m_pred.flatten(), zero_division=0)\n",
        "            results.append((Path(p).stem, f1, dbg))\n",
        "        except FileNotFoundError as e:\n",
        "            print(f\"Skipping {p} in validation F1 calculation due to FileNotFoundError: {e}\")\n",
        "            continue\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping {p} in validation F1 calculation due to unexpected error: {e}\")\n",
        "            continue\n",
        "\n",
        "    if results:\n",
        "        print(\"\\n F1-score par image falsifiÃ©e:\\n\")\n",
        "        for cid,f1,dbg in results:\n",
        "            print(f\"{cid} â€” F1={f1:.4f} | area={dbg['area']} mean={dbg['mean_inside']:.3f} thr={dbg['thr']:.3f}\")\n",
        "        print(f\"\\n Moyenne F1 (falsifiÃ©es) = {np.mean([r[1] for r in results]):.4f}\")\n",
        "    else:\n",
        "        print(\"\\n No results to display for validation F1-score (forged-only).\")\n",
        "else:\n",
        "    print(\"Skipping validation F1-score calculation (forged-only) because validation data is not available.\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Competition data not found at '/kaggle/input/recodai-luc-scientific-image-forgery-detection'. Skipping data loading, training setup, and grid search.\n",
            "Please ensure the 'recodai-luc-scientific-image-forgery-detection' dataset is available or adjust BASE_DIR.\n",
            "âœ… Loaded pretrained model from: cnndinov2_a78_model/model_seg_final.pt\n",
            "Skipping grid search because GRID_SEARCH_ENABLED flag is False.\n",
            "Skipping validation F1-score calculation (forged-only) because validation data is not available.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "437116bb"
      },
      "source": [
        "**Reasoning**:\n",
        "The next step is to implement the training loop and integrate data augmentation as per the main task. This includes defining `ForgerySegDataset` with data augmentation, setting up the optimizer and loss function, and creating the training and validation loops, all while ensuring conditional execution based on the presence of the `BASE_DIR` to avoid `FileNotFoundError`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "bac9aead",
        "outputId": "7e38967b-04fa-48a3-8237-d45f2660d0b2"
      },
      "source": [
        "import os, cv2, json, math, random, torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn, torch.nn.functional as F, torch.optim as optim\n",
        "from transformers import AutoImageProcessor, AutoModel\n",
        "import torchvision.transforms as transforms # Import torchvision transforms\n",
        "\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    # This forces CUDA to use deterministic algorithms (slower but consistent)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed_everything(42)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "BASE_DIR  = \"/kaggle/input/recodai-luc-scientific-image-forgery-detection\"\n",
        "AUTH_DIR  = f\"{BASE_DIR}/train_images/authentic\"\n",
        "FORG_DIR  = f\"{BASE_DIR}/train_images/forged\"\n",
        "MASK_DIR  = f\"{BASE_DIR}/train_masks\"\n",
        "TEST_DIR  = f\"{BASE_DIR}/test_images\"\n",
        "\n",
        "# Updated paths from previous extraction steps\n",
        "DINO_PATH = 'dinov2_base' # Updated to local extraction directory\n",
        "MODEL_LOC = 'cnndinov2_a78_model/model_seg_final.pt' # Corrected path based on directory listing\n",
        "\n",
        "IMG_SIZE = 518\n",
        "BATCH_SIZE = 2\n",
        "\n",
        "# INFERENCE UTILS\n",
        "AREA_THR = 200\n",
        "MEAN_THR = 0.22\n",
        "USE_TTA = True\n",
        "GRID_SEARCH_ENABLED = True # Renamed to avoid conflict if GRID_SEARCH is set to False later\n",
        "\n",
        "# --- Data Augmentation Transforms ---\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomVerticalFlip(p=0.5),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.ToTensor(), # Converts PIL Image to Tensor, normalizes to [0,1]\n",
        "    # No normalization here as the original code scales to /255. manually\n",
        "])\n",
        "\n",
        "val_transforms = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "class ForgerySegDataset(Dataset):\n",
        "    def __init__(self, auth_paths, forg_paths, mask_dir, transform=None):\n",
        "        self.samples = []\n",
        "        for p in forg_paths:\n",
        "            m = os.path.join(mask_dir, Path(p).stem + \".npy\")\n",
        "            if os.path.exists(m):\n",
        "                self.samples.append((p, m, 'forged')) # Added 'forged' label\n",
        "            else:\n",
        "                # If mask is missing for a forged image, treat as authentic for now\n",
        "                # Or skip, depending on desired behavior. For training, we need masks.\n",
        "                # Here, we skip if no mask for forged image (as it would be useless for segmentation training)\n",
        "                print(f\"Warning: Mask not found for forged image {p}. Skipping this sample.\")\n",
        "        for p in auth_paths:\n",
        "            self.samples.append((p, None, 'authentic')) # Added 'authentic' label\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self): return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, mask_path, label_type = self.samples[idx]\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        if mask_path is None: # Authentic image\n",
        "            # Create an empty mask for authentic images, scaled to IMG_SIZE\n",
        "            mask = np.zeros((IMG_SIZE, IMG_SIZE), np.uint8)\n",
        "        else: # Forged image with a mask\n",
        "            m = np.load(mask_path)\n",
        "            if m.ndim == 3: m = np.max(m, axis=0)\n",
        "            mask = (m > 0).astype(np.uint8)\n",
        "            # Resize mask to IMG_SIZE before applying transforms if any spatial transforms are used\n",
        "            mask = Image.fromarray(mask).resize((IMG_SIZE, IMG_SIZE), Image.NEAREST)\n",
        "            mask = np.array(mask)\n",
        "\n",
        "        # Apply image transformations\n",
        "        # Note: torchvision.transforms expects PIL Image or Tensor.\n",
        "        # Custom transforms for both image and mask should be handled carefully\n",
        "        # Here, we apply image transforms and then process mask for consistency.\n",
        "        # For random operations, it's better to combine image and mask transforms.\n",
        "        # For simplicity, we resize mask earlier and then apply 'ToTensor' logic.\n",
        "\n",
        "        # Convert image to tensor using transform\n",
        "        if self.transform:\n",
        "            img_t = self.transform(img)\n",
        "        else:\n",
        "            # Default processing if no transform (e.g., for validation/inference)\n",
        "            img_r = img.resize((IMG_SIZE, IMG_SIZE))\n",
        "            img_t = torch.from_numpy(np.array(img_r, np.float32)/255.).permute(2,0,1)\n",
        "\n",
        "        # Convert mask to tensor (already resized to IMG_SIZE in previous logic)\n",
        "        mask_t = torch.from_numpy(mask[None, ...].astype(np.float32))\n",
        "\n",
        "        return img_t, mask_t\n",
        "\n",
        "\n",
        "\n",
        "#  MODEL (DINOv2 + Decoder)\n",
        "\n",
        "from transformers import AutoImageProcessor, AutoModel\n",
        "processor = AutoImageProcessor.from_pretrained(DINO_PATH, local_files_only=True, use_fast=False)\n",
        "encoder = AutoModel.from_pretrained(DINO_PATH, local_files_only=True).eval().to(device)\n",
        "\n",
        "class DinoTinyDecoder(nn.Module):\n",
        "    def __init__(self, in_ch=768, out_ch=1):\n",
        "        super().__init__()\n",
        "        # Block 1: 768 -> 384\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, 384, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout2d(0.1)\n",
        "        )\n",
        "        # Block 2: 384 -> 192\n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.Conv2d(384, 192, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout2d(0.1)\n",
        "        )\n",
        "        # Block 3: 192 -> 96\n",
        "        self.block3 = nn.Sequential(\n",
        "            nn.Conv2d(192, 96, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        # Final Output: 96 -> 1\n",
        "        self.conv_out = nn.Conv2d(96, out_ch, kernel_size=1)\n",
        "\n",
        "    def forward(self, f, target_size):\n",
        "        # f: [B, 768, 37, 37]\n",
        "\n",
        "        # Step 1: Up to ~74x74\n",
        "        x = F.interpolate(self.block1(f), size=(74, 74), mode='bilinear', align_corners=False)\n",
        "\n",
        "        # Step 2: Up to ~148x148\n",
        "        x = F.interpolate(self.block2(x), size=(148, 148), mode='bilinear', align_corners=False)\n",
        "\n",
        "        # Step 3: Up to ~296x296\n",
        "        x = F.interpolate(self.block3(x), size=(296, 296), mode='bilinear', align_corners=False)\n",
        "\n",
        "        # Step 4: Final jump to 518x518\n",
        "        x = self.conv_out(x)\n",
        "        x = F.interpolate(x, size=target_size, mode='bilinear', align_corners=False)\n",
        "\n",
        "        return x\n",
        "\n",
        "class DinoSegmenter(nn.Module):\n",
        "    def __init__(self, encoder, processor):\n",
        "        super().__init__()\n",
        "        self.encoder, self.processor = encoder, processor\n",
        "        # Removed or commented out the line freezing encoder parameters to enable fine-tuning\n",
        "        # for p in self.encoder.parameters(): p.requires_grad = False\n",
        "        self.seg_head = DinoTinyDecoder(768,1)\n",
        "    def forward_features(self,x):\n",
        "        # Ensure input to processor is a list of PIL Images (or similar format it expects)\n",
        "        # x is already a tensor [B, C, H, W] ranging from 0-1 from ToTensor()\n",
        "        # Need to convert it back to a list of PIL images or numpy arrays for the processor.\n",
        "        # This part requires careful handling based on AutoImageProcessor's specific expectations.\n",
        "        # Assuming processor can take a list of numpy arrays [H,W,C] or PIL images.\n",
        "        # The `x*255.clamp(0,255).byte().permute(0,2,3,1).cpu().numpy()` converts a batch of float tensors to a batch of uint8 numpy arrays.\n",
        "        imgs = (x*255).clamp(0,255).byte().permute(0,2,3,1).cpu().numpy()\n",
        "        # The processor expects a list of images (PIL Image, NumPy array, or PyTorch tensor)\n",
        "        inputs = self.processor(images=[Image.fromarray(img) for img in imgs], return_tensors=\"pt\").to(x.device)\n",
        "\n",
        "        feats = self.encoder(**inputs).last_hidden_state\n",
        "        B,N,C = feats.shape\n",
        "        fmap = feats[:,1:,:].permute(0,2,1)\n",
        "        s = int(math.sqrt(N-1))\n",
        "        fmap = fmap.reshape(B,C,s,s)\n",
        "        return fmap\n",
        "    def forward_seg(self,x):\n",
        "        fmap = self.forward_features(x)\n",
        "        return self.seg_head(fmap,(IMG_SIZE,IMG_SIZE))\n",
        "\n",
        "\n",
        "# --- MODIFIED PART TO HANDLE MISSING BASE_DIR AND TRAINING LOOP ---\n",
        "# Initialize data-related variables as empty lists or None\n",
        "auth_imgs, forg_imgs, train_auth, val_auth, train_forg, val_forg = [], [], [], [], [], []\n",
        "train_loader, val_loader = None, None\n",
        "\n",
        "if not os.path.exists(BASE_DIR):\n",
        "    print(f\"Warning: Competition data not found at '{BASE_DIR}'. Skipping data loading, training setup, and grid search.\")\n",
        "    print(\"Please ensure the 'recodai-luc-scientific-image-forgery-detection' dataset is available or adjust BASE_DIR.\")\n",
        "    GRID_SEARCH_ENABLED = False # Disable grid search if data is not available\n",
        "else:\n",
        "    auth_imgs = sorted([str(Path(AUTH_DIR)/f) for f in os.listdir(AUTH_DIR)])\n",
        "    forg_imgs = sorted([str(Path(FORG_DIR)/f) for f in os.listdir(FORG_DIR)])\n",
        "    train_auth, val_auth = train_test_split(auth_imgs, test_size=0.2, random_state=42)\n",
        "    train_forg, val_forg = train_test_split(forg_imgs, test_size=0.2, random_state=42)\n",
        "\n",
        "    train_loader = DataLoader(ForgerySegDataset(train_auth, train_forg, MASK_DIR, transform=train_transforms),\n",
        "                              batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
        "    val_loader = DataLoader(ForgerySegDataset(val_auth, val_forg, MASK_DIR, transform=val_transforms),\n",
        "                            batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "model_seg = DinoSegmenter(encoder, processor).to(device)\n",
        "\n",
        "# Load pretrained weights if MODEL_LOC is specified\n",
        "if MODEL_LOC is not None and os.path.exists(MODEL_LOC):\n",
        "    model_seg.load_state_dict(torch.load(MODEL_LOC, map_location=device))\n",
        "    print(f\"âœ… Loaded pretrained model from: {MODEL_LOC}\")\n",
        "    # model_seg.eval()  # Do not set to eval mode if we are training\n",
        "else:\n",
        "    print(f\"âš ï¸ Warning: Model weights not found at '{MODEL_LOC}'. Model will use randomly initialized weights (if applicable).\")\n",
        "\n",
        "\n",
        "# --- Training Loop Implementation ---\n",
        "if train_loader and val_loader and len(train_loader.dataset) > 0 and len(val_loader.dataset) > 0:\n",
        "    print(\"\\nStarting model training...\")\n",
        "    # Define Loss Function and Optimizer\n",
        "    criterion = nn.BCEWithLogitsLoss() # For binary segmentation\n",
        "    optimizer = optim.Adam(model_seg.parameters(), lr=1e-5) # Fine-tune encoder as well\n",
        "    num_epochs = 5 # Example number of epochs\n",
        "    best_val_f1 = -1.0\n",
        "    best_model_path = \"best_model_segmenter.pt\"\n",
        "\n",
        "    # Helper function to calculate F1-score for validation\n",
        "    def calculate_f1_score(preds, masks):\n",
        "        # preds: logits, masks: ground truth (0/1)\n",
        "        probs = torch.sigmoid(preds)\n",
        "        # Threshold at 0.5 for binary prediction\n",
        "        binary_preds = (probs > 0.5).float()\n",
        "        # Flatten tensors for F1 score calculation\n",
        "        return f1_score(masks.cpu().numpy().flatten(), binary_preds.cpu().numpy().flatten(), zero_division=0)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model_seg.train() # Set model to training mode\n",
        "        running_loss = 0.0\n",
        "        for batch_idx, (images, masks) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1} Training\")):\n",
        "            images, masks = images.to(device), masks.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model_seg.forward_seg(images)\n",
        "            loss = criterion(outputs, masks)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = running_loss / len(train_loader)\n",
        "\n",
        "        # Validation phase\n",
        "        model_seg.eval() # Set model to evaluation mode\n",
        "        val_loss = 0.0\n",
        "        val_f1 = 0.0\n",
        "        with torch.no_grad():\n",
        "            for images, masks in tqdm(val_loader, desc=f\"Epoch {epoch+1} Validation\")):\n",
        "                images, masks = images.to(device), masks.to(device)\n",
        "                outputs = model_seg.forward_seg(images)\n",
        "                loss = criterion(outputs, masks)\n",
        "                val_loss += loss.item()\n",
        "                val_f1 += calculate_f1_score(outputs, masks)\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        avg_val_f1 = val_f1 / len(val_loader)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val F1: {avg_val_f1:.4f}\")\n",
        "\n",
        "        # Save best model\n",
        "        if avg_val_f1 > best_val_f1:\n",
        "            best_val_f1 = avg_val_f1\n",
        "            torch.save(model_seg.state_dict(), best_model_path)\n",
        "            print(f\"â­ New best model saved with Val F1: {best_val_f1:.4f}\")\n",
        "    print(\"Training complete.\")\n",
        "else:\n",
        "    print(\"\\nSkipping model training due to missing data or empty data loaders.\")\n",
        "\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def segment_prob_map(pil):\n",
        "    x = torch.from_numpy(np.array(pil.resize((IMG_SIZE, IMG_SIZE)), np.float32)/255.).permute(2,0,1)[None].to(device)\n",
        "    prob = torch.sigmoid(model_seg.forward_seg(x))[0,0].cpu().numpy()\n",
        "    return prob\n",
        "\n",
        "@torch.no_grad()\n",
        "def segment_prob_map_with_tta(pil):\n",
        "    # 1. Preprocessing: Resize, Normalize, and move to Device\n",
        "    # Use val_transforms for consistency in inference preparation\n",
        "    # pil_tensor = val_transforms(pil).to(device)\n",
        "    # The original code's image preparation for inference: manual resize and to_tensor conversion\n",
        "    x = torch.from_numpy(np.array(pil.resize((IMG_SIZE, IMG_SIZE)), np.float32)/255.).permute(2,0,1)[None].to(device)\n",
        "\n",
        "    predictions = []\n",
        "\n",
        "    # 2. Original Prediction\n",
        "    pred_orig = torch.sigmoid(model_seg.forward_seg(x))\n",
        "    predictions.append(pred_orig)\n",
        "\n",
        "    # 3. Horizontal Flip TTA (dim 3)\n",
        "    # Flip input -> Predict -> Flip output back\n",
        "    pred_h = torch.sigmoid(model_seg.forward_seg(torch.flip(x, dims=[3])))\n",
        "    predictions.append(torch.flip(pred_h, dims=[3]))\n",
        "\n",
        "    # 4. Vertical Flip TTA (dim 2)\n",
        "    # Flip input -> Predict -> Flip output back\n",
        "    pred_v = torch.sigmoid(model_seg.forward_seg(torch.flip(x, dims=[2])))\n",
        "    predictions.append(torch.flip(pred_v, dims=[2]))\n",
        "\n",
        "    # 5. Average the predictions and format as numpy\n",
        "    # We stack the 3 predictions and take the mean across the stack dimension (0)\n",
        "    prob = torch.stack(predictions).mean(0)[0, 0].cpu().numpy()\n",
        "\n",
        "    return prob\n",
        "\n",
        "def enhanced_adaptive_mask(prob, alpha_grad=0.45):\n",
        "    gx = cv2.Sobel(prob, cv2.CV_32F, 1, 0, ksize=3)\n",
        "    gy = cv2.Sobel(prob, cv2.CV_32F, 0, 1, ksize=3)\n",
        "    grad_mag = np.sqrt(gx**2 + gy**2)\n",
        "    grad_norm = grad_mag / (grad_mag.max() + 1e-6)\n",
        "    enhanced = (1 - alpha_grad) * prob + alpha_grad * grad_norm\n",
        "    enhanced = cv2.GaussianBlur(enhanced, (3,3), 0)\n",
        "    thr = np.mean(enhanced) + 0.3 * np.std(enhanced)\n",
        "    mask = (enhanced > thr).astype(np.uint8)\n",
        "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, np.ones((5,5), np.uint8))\n",
        "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, np.ones((3,3), np.uint8))\n",
        "    return mask, thr\n",
        "\n",
        "def finalize_mask(prob, orig_size):\n",
        "    mask, thr = enhanced_adaptive_mask(prob)\n",
        "    mask = cv2.resize(mask, orig_size, interpolation=cv2.INTER_NEAREST)\n",
        "    return mask, thr\n",
        "\n",
        "def pipeline_final(pil):\n",
        "    model_seg.eval() # Ensure model is in eval mode for inference\n",
        "    if USE_TTA:\n",
        "        prob = segment_prob_map_with_tta(pil)\n",
        "    else:\n",
        "        prob = segment_prob_map(pil)\n",
        "    mask, thr = finalize_mask(prob, pil.size)\n",
        "    area = int(mask.sum())\n",
        "    # The mask used here for mean calculation should be resized to IMG_SIZE for proper indexing into 'prob'\n",
        "    mask_for_mean = cv2.resize(mask,(IMG_SIZE,IMG_SIZE),interpolation=cv2.INTER_NEAREST)\n",
        "    mean_inside = float(prob[mask_for_mean==1].mean()) if area>0 else 0.0\n",
        "    if area < AREA_THR or mean_inside < MEAN_THR:\n",
        "        return \"authentic\", None, {\"area\": area, \"mean_inside\": mean_inside, \"thr\": thr}\n",
        "    return \"forged\", mask, {\"area\": area, \"mean_inside\": mean_inside, \"thr\": thr}\n",
        "\n",
        "import itertools\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def grid_search_area_mean(forg_paths, auth_paths, mask_dir):\n",
        "    # Return default values and print message if no valid data for grid search\n",
        "    if not forg_paths and not auth_paths:\n",
        "        print(\"Skipping grid search as validation data is not available.\")\n",
        "        return {'AREA_THR': 200, 'MEAN_THR': 0.22}\n",
        "\n",
        "    mean_range = [round(x, 2) for x in np.arange(0.20, 0.291, 0.01)]\n",
        "    area_range = [200]\n",
        "    val_set = [(p, \"forged\") for p in forg_paths] + [(p, \"authentic\") for p in auth_paths]\n",
        "\n",
        "    if not val_set:\n",
        "        print(\"Skipping grid search as validation set is empty.\")\n",
        "        return {'AREA_THR': 200, 'MEAN_THR': 0.22}\n",
        "\n",
        "    print(f\"ðŸš€ Step 1: Caching probability maps for ALL {len(val_set)} images...\")\n",
        "    cache = []\n",
        "    for p, label in tqdm(val_set):\n",
        "        try:\n",
        "            pil = Image.open(p).convert(\"RGB\")\n",
        "            w, h = pil.size\n",
        "\n",
        "            # Ensure model is in eval mode for caching inference\n",
        "            model_seg.eval()\n",
        "            prob = segment_prob_map_with_tta(pil) if USE_TTA else segment_prob_map(pil)\n",
        "\n",
        "            mask_raw, _ = enhanced_adaptive_mask(prob)\n",
        "            mask_resized = cv2.resize(mask_raw, (w, h), interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "            if label == \"forged\":\n",
        "                m_gt = np.load(Path(mask_dir)/f\"{Path(p).stem}.npy\")\n",
        "                if m_gt.ndim == 3: m_gt = np.max(m_gt, axis=0)\n",
        "                m_gt = (m_gt > 0).astype(np.uint8)\n",
        "            else:\n",
        "                m_gt = np.zeros((h, w), np.uint8)\n",
        "\n",
        "            cache.append({\"prob\": prob, \"mask\": mask_resized, \"gt\": m_gt, \"label\": label})\n",
        "        except FileNotFoundError as e:\n",
        "            print(f\"Skipping {p} due to FileNotFoundError during caching: {e}\")\n",
        "            continue\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping {p} due to unexpected error during caching: {e}\")\n",
        "            continue\n",
        "\n",
        "    if not cache: # If no items were successfully cached, return default params\n",
        "        print(\"No valid images found for grid search caching. Returning default parameters.\")\n",
        "        return {'AREA_THR': 200, 'MEAN_THR': 0.22}\n",
        "\n",
        "    best_f1 = -1\n",
        "    best_params = {}\n",
        "    combinations = list(itertools.product(area_range, mean_range))\n",
        "\n",
        "    for a_thr, m_thr in combinations:\n",
        "        current_f1s = []\n",
        "        for item in cache:\n",
        "            mask = item[\"mask\"]\n",
        "            area = int(mask.sum())\n",
        "\n",
        "            mask_small = cv2.resize(mask, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_NEAREST)\n",
        "            mean_in = float(item[\"prob\"][mask_small == 1].mean()) if area > 0 else 0.0\n",
        "\n",
        "            is_forged = (area >= a_thr and mean_in >= m_thr)\n",
        "            m_pred = (mask > 0).astype(np.uint8) if is_forged else np.zeros_like(item[\"gt\"])\n",
        "\n",
        "            f1 = f1_score(item[\"gt\"].flatten(), m_pred.flatten(),\n",
        "                          zero_division=1 if item[\"label\"] == \"authentic\" else 0)\n",
        "            current_f1s.append(f1)\n",
        "\n",
        "        if current_f1s: # Only calculate average if any F1 scores were computed\n",
        "            avg_f1 = np.mean(current_f1s)\n",
        "            if avg_f1 > best_f1:\n",
        "                best_f1 = avg_f1\n",
        "                best_params = {\"AREA_THR\": a_thr, \"MEAN_THR\": m_thr}\n",
        "                print(f\"â­ New Best F1: {best_f1:.4f} -> AREA: {a_thr}, MEAN: {m_thr}\")\n",
        "\n",
        "    if not best_params: # If no suitable parameters found, return defaults\n",
        "        print(\"No optimal parameters found during grid search. Returning default parameters.\")\n",
        "        return {'AREA_THR': 200, 'MEAN_THR': 0.22}\n",
        "\n",
        "    return best_params\n",
        "\n",
        "if GRID_SEARCH_ENABLED:\n",
        "    if val_forg or val_auth: # Only run grid search if there's actual validation data\n",
        "        print(\"Starting grid search for optimal parameters...\")\n",
        "        # Temporarily set model to eval mode for grid search inference if it was in train mode\n",
        "        current_model_mode = model_seg.training\n",
        "        model_seg.eval()\n",
        "        best_cfg = grid_search_area_mean(val_forg, val_auth, MASK_DIR)\n",
        "        AREA_THR = best_cfg['AREA_THR']\n",
        "        MEAN_THR = best_cfg['MEAN_THR']\n",
        "        print(f\"Final Grid Search Parameters: AREA_THR={AREA_THR}, MEAN_THR={MEAN_THR}\")\n",
        "        # Restore model mode\n",
        "        if current_model_mode: model_seg.train()\n",
        "    else:\n",
        "        print(\"Skipping grid search because validation data lists are empty.\")\n",
        "else:\n",
        "    print(\"Skipping grid search because GRID_SEARCH_ENABLED flag is False.\")\n",
        "\n",
        "\n",
        "# --- FURTHER MODIFICATION FOR VALIDATION F1 SCORE CALCULATION ---\n",
        "from sklearn.metrics import f1_score\n",
        "val_items = [(p, 1) for p in val_forg[:10]] # This will be empty if val_forg is empty\n",
        "results = []\n",
        "if val_items:\n",
        "    print(\"\\nCalculating F1-score for validation forged images:\")\n",
        "    # Ensure model is in eval mode for validation inference\n",
        "    current_model_mode = model_seg.training\n",
        "    model_seg.eval()\n",
        "    for p,_ in tqdm(val_items, desc=\"Validation forged-only\"):\n",
        "        try:\n",
        "            pil = Image.open(p).convert(\"RGB\")\n",
        "            label, m_pred, dbg = pipeline_final(pil)\n",
        "            m_gt = np.load(Path(MASK_DIR)/f\"{Path(p).stem}.npy\")\n",
        "            if m_gt.ndim==3: m_gt=np.max(m_gt,axis=0)\n",
        "            m_gt=(m_gt>0).astype(np.uint8)\n",
        "            m_pred=(m_pred>0).astype(np.uint8) if m_pred is not None else np.zeros_like(m_gt)\n",
        "            f1 = f1_score(m_gt.flatten(), m_pred.flatten(), zero_division=0)\n",
        "            results.append((Path(p).stem, f1, dbg))\n",
        "        except FileNotFoundError as e:\n",
        "            print(f\"Skipping {p} in validation F1 calculation due to FileNotFoundError: {e}\")\n",
        "            continue\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping {p} in validation F1 calculation due to unexpected error: {e}\")\n",
        "            continue\n",
        "    # Restore model mode\n",
        "    if current_model_mode: model_seg.train()\n",
        "\n",
        "    if results:\n",
        "        print(\"\\n F1-score par image falsifiÃ©e:\\n\")\n",
        "        for cid,f1,dbg in results:\n",
        "            print(f\"{cid} â€” F1={f1:.4f} | area={dbg['area']} mean={dbg['mean_inside']:.3f} thr={dbg['thr']:.3f}\")\n",
        "        print(f\"\\n Moyenne F1 (falsifiÃ©es) = {np.mean([r[1] for r in results]):.4f}\")\n",
        "    else:\n",
        "        print(\"\\n No results to display for validation F1-score (forged-only).\")\n",
        "else:\n",
        "    print(\"Skipping validation F1-score calculation (forged-only) because validation data is not available.\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "unmatched ')' (ipython-input-2884228310.py, line 262)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-2884228310.py\"\u001b[0;36m, line \u001b[0;32m262\u001b[0m\n\u001b[0;31m    for images, masks in tqdm(val_loader, desc=f\"Epoch {epoch+1} Validation\")):\u001b[0m\n\u001b[0m                                                                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unmatched ')'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b58c129"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code block failed due to a `SyntaxError: unmatched ')'`. I have identified and removed the extra parenthesis in the `tqdm` loop for validation to fix this error.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4633118c",
        "outputId": "b96123ce-a377-4488-85ab-cc814616350d"
      },
      "source": [
        "import os, cv2, json, math, random, torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn, torch.nn.functional as F, torch.optim as optim\n",
        "from transformers import AutoImageProcessor, AutoModel\n",
        "import torchvision.transforms as transforms # Import torchvision transforms\n",
        "\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    # This forces CUDA to use deterministic algorithms (slower but consistent)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed_everything(42)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "BASE_DIR  = \"/kaggle/input/recodai-luc-scientific-image-forgery-detection\"\n",
        "AUTH_DIR  = f\"{BASE_DIR}/train_images/authentic\"\n",
        "FORG_DIR  = f\"{BASE_DIR}/train_images/forged\"\n",
        "MASK_DIR  = f\"{BASE_DIR}/train_masks\"\n",
        "TEST_DIR  = f\"{BASE_DIR}/test_images\"\n",
        "\n",
        "# Updated paths from previous extraction steps\n",
        "DINO_PATH = 'dinov2_base' # Updated to local extraction directory\n",
        "MODEL_LOC = 'cnndinov2_a78_model/model_seg_final.pt' # Corrected path based on directory listing\n",
        "\n",
        "IMG_SIZE = 518\n",
        "BATCH_SIZE = 2\n",
        "\n",
        "# INFERENCE UTILS\n",
        "AREA_THR = 200\n",
        "MEAN_THR = 0.22\n",
        "USE_TTA = True\n",
        "GRID_SEARCH_ENABLED = True # Renamed to avoid conflict if GRID_SEARCH is set to False later\n",
        "\n",
        "# --- Data Augmentation Transforms ---\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomVerticalFlip(p=0.5),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.ToTensor(), # Converts PIL Image to Tensor, normalizes to [0,1]\n",
        "    # No normalization here as the original code scales to /255. manually\n",
        "])\n",
        "\n",
        "val_transforms = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "class ForgerySegDataset(Dataset):\n",
        "    def __init__(self, auth_paths, forg_paths, mask_dir, transform=None):\n",
        "        self.samples = []\n",
        "        for p in forg_paths:\n",
        "            m = os.path.join(mask_dir, Path(p).stem + \".npy\")\n",
        "            if os.path.exists(m):\n",
        "                self.samples.append((p, m, 'forged')) # Added 'forged' label\n",
        "            else:\n",
        "                # If mask is missing for a forged image, treat as authentic for now\n",
        "                # Or skip, depending on desired behavior. For training, we need masks.\n",
        "                # Here, we skip if no mask for forged image (as it would be useless for segmentation training)\n",
        "                print(f\"Warning: Mask not found for forged image {p}. Skipping this sample.\")\n",
        "        for p in auth_paths:\n",
        "            self.samples.append((p, None, 'authentic')) # Added 'authentic' label\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self): return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, mask_path, label_type = self.samples[idx]\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        if mask_path is None: # Authentic image\n",
        "            # Create an empty mask for authentic images, scaled to IMG_SIZE\n",
        "            mask = np.zeros((IMG_SIZE, IMG_SIZE), np.uint8)\n",
        "        else: # Forged image with a mask\n",
        "            m = np.load(mask_path)\n",
        "            if m.ndim == 3: m = np.max(m, axis=0)\n",
        "            mask = (m > 0).astype(np.uint8)\n",
        "            # Resize mask to IMG_SIZE before applying transforms if any spatial transforms are used\n",
        "            mask = Image.fromarray(mask).resize((IMG_SIZE, IMG_SIZE), Image.NEAREST)\n",
        "            mask = np.array(mask)\n",
        "\n",
        "        # Apply image transformations\n",
        "        # Note: torchvision.transforms expects PIL Image or Tensor.\n",
        "        # Custom transforms for both image and mask should be handled carefully\n",
        "        # Here, we apply image transforms and then process mask for consistency.\n",
        "        # For random operations, it's better to combine image and mask transforms.\n",
        "        # For simplicity, we resize mask earlier and then apply 'ToTensor' logic.\n",
        "\n",
        "        # Convert image to tensor using transform\n",
        "        if self.transform:\n",
        "            img_t = self.transform(img)\n",
        "        else:\n",
        "            # Default processing if no transform (e.g., for validation/inference)\n",
        "            img_r = img.resize((IMG_SIZE, IMG_SIZE))\n",
        "            img_t = torch.from_numpy(np.array(img_r, np.float32)/255.).permute(2,0,1)\n",
        "\n",
        "        # Convert mask to tensor (already resized to IMG_SIZE in previous logic)\n",
        "        mask_t = torch.from_numpy(mask[None, ...].astype(np.float32))\n",
        "\n",
        "        return img_t, mask_t\n",
        "\n",
        "\n",
        "\n",
        "#  MODEL (DINOv2 + Decoder)\n",
        "\n",
        "from transformers import AutoImageProcessor, AutoModel\n",
        "processor = AutoImageProcessor.from_pretrained(DINO_PATH, local_files_only=True, use_fast=False)\n",
        "encoder = AutoModel.from_pretrained(DINO_PATH, local_files_only=True).eval().to(device)\n",
        "\n",
        "class DinoTinyDecoder(nn.Module):\n",
        "    def __init__(self, in_ch=768, out_ch=1):\n",
        "        super().__init__()\n",
        "        # Block 1: 768 -> 384\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, 384, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout2d(0.1)\n",
        "        )\n",
        "        # Block 2: 384 -> 192\n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.Conv2d(384, 192, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout2d(0.1)\n",
        "        )\n",
        "        # Block 3: 192 -> 96\n",
        "        self.block3 = nn.Sequential(\n",
        "            nn.Conv2d(192, 96, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        # Final Output: 96 -> 1\n",
        "        self.conv_out = nn.Conv2d(96, out_ch, kernel_size=1)\n",
        "\n",
        "    def forward(self, f, target_size):\n",
        "        # f: [B, 768, 37, 37]\n",
        "\n",
        "        # Step 1: Up to ~74x74\n",
        "        x = F.interpolate(self.block1(f), size=(74, 74), mode='bilinear', align_corners=False)\n",
        "\n",
        "        # Step 2: Up to ~148x148\n",
        "        x = F.interpolate(self.block2(x), size=(148, 148), mode='bilinear', align_corners=False)\n",
        "\n",
        "        # Step 3: Up to ~296x296\n",
        "        x = F.interpolate(self.block3(x), size=(296, 296), mode='bilinear', align_corners=False)\n",
        "\n",
        "        # Step 4: Final jump to 518x518\n",
        "        x = self.conv_out(x)\n",
        "        x = F.interpolate(x, size=target_size, mode='bilinear', align_corners=False)\n",
        "\n",
        "        return x\n",
        "\n",
        "class DinoSegmenter(nn.Module):\n",
        "    def __init__(self, encoder, processor):\n",
        "        super().__init__()\n",
        "        self.encoder, self.processor = encoder, processor\n",
        "        # Removed or commented out the line freezing encoder parameters to enable fine-tuning\n",
        "        # for p in self.encoder.parameters(): p.requires_grad = False\n",
        "        self.seg_head = DinoTinyDecoder(768,1)\n",
        "    def forward_features(self,x):\n",
        "        # Ensure input to processor is a list of PIL Images (or similar format it expects)\n",
        "        # x is already a tensor [B, C, H, W] ranging from 0-1 from ToTensor()\n",
        "        # Need to convert it back to a list of PIL images or numpy arrays for the processor.\n",
        "        # This part requires careful handling based on AutoImageProcessor's specific expectations.\n",
        "        # Assuming processor can take a list of numpy arrays [H,W,C] or PIL images.\n",
        "        # The `x*255.clamp(0,255).byte().permute(0,2,3,1).cpu().numpy()` converts a batch of float tensors to a batch of uint8 numpy arrays.\n",
        "        imgs = (x*255).clamp(0,255).byte().permute(0,2,3,1).cpu().numpy()\n",
        "        # The processor expects a list of images (PIL Image, NumPy array, or PyTorch tensor)\n",
        "        inputs = self.processor(images=[Image.fromarray(img) for img in imgs], return_tensors=\"pt\").to(x.device)\n",
        "\n",
        "        feats = self.encoder(**inputs).last_hidden_state\n",
        "        B,N,C = feats.shape\n",
        "        fmap = feats[:,1:,:].permute(0,2,1)\n",
        "        s = int(math.sqrt(N-1))\n",
        "        fmap = fmap.reshape(B,C,s,s)\n",
        "        return fmap\n",
        "    def forward_seg(self,x):\n",
        "        fmap = self.forward_features(x)\n",
        "        return self.seg_head(fmap,(IMG_SIZE,IMG_SIZE))\n",
        "\n",
        "\n",
        "# --- MODIFIED PART TO HANDLE MISSING BASE_DIR AND TRAINING LOOP ---\n",
        "# Initialize data-related variables as empty lists or None\n",
        "auth_imgs, forg_imgs, train_auth, val_auth, train_forg, val_forg = [], [], [], [], [], []\n",
        "train_loader, val_loader = None, None\n",
        "\n",
        "if not os.path.exists(BASE_DIR):\n",
        "    print(f\"Warning: Competition data not found at '{BASE_DIR}'. Skipping data loading, training setup, and grid search.\")\n",
        "    print(\"Please ensure the 'recodai-luc-scientific-image-forgery-detection' dataset is available or adjust BASE_DIR.\")\n",
        "    GRID_SEARCH_ENABLED = False # Disable grid search if data is not available\n",
        "else:\n",
        "    auth_imgs = sorted([str(Path(AUTH_DIR)/f) for f in os.listdir(AUTH_DIR)])\n",
        "    forg_imgs = sorted([str(Path(FORG_DIR)/f) for f in os.listdir(FORG_DIR)])\n",
        "    train_auth, val_auth = train_test_split(auth_imgs, test_size=0.2, random_state=42)\n",
        "    train_forg, val_forg = train_test_split(forg_imgs, test_size=0.2, random_state=42)\n",
        "\n",
        "    train_loader = DataLoader(ForgerySegDataset(train_auth, train_forg, MASK_DIR, transform=train_transforms),\n",
        "                              batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
        "    val_loader = DataLoader(ForgerySegDataset(val_auth, val_forg, MASK_DIR, transform=val_transforms),\n",
        "                            batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "model_seg = DinoSegmenter(encoder, processor).to(device)\n",
        "\n",
        "# Load pretrained weights if MODEL_LOC is specified\n",
        "if MODEL_LOC is not None and os.path.exists(MODEL_LOC):\n",
        "    model_seg.load_state_dict(torch.load(MODEL_LOC, map_location=device))\n",
        "    print(f\"âœ… Loaded pretrained model from: {MODEL_LOC}\")\n",
        "    # model_seg.eval()  # Do not set to eval mode if we are training\n",
        "else:\n",
        "    print(f\"âš ï¸ Warning: Model weights not found at '{MODEL_LOC}'. Model will use randomly initialized weights (if applicable).\")\n",
        "\n",
        "\n",
        "# --- Training Loop Implementation ---\n",
        "if train_loader and val_loader and len(train_loader.dataset) > 0 and len(val_loader.dataset) > 0:\n",
        "    print(\"\\nStarting model training...\")\n",
        "    # Define Loss Function and Optimizer\n",
        "    criterion = nn.BCEWithLogitsLoss() # For binary segmentation\n",
        "    optimizer = optim.Adam(model_seg.parameters(), lr=1e-5) # Fine-tune encoder as well\n",
        "    num_epochs = 5 # Example number of epochs\n",
        "    best_val_f1 = -1.0\n",
        "    best_model_path = \"best_model_segmenter.pt\"\n",
        "\n",
        "    # Helper function to calculate F1-score for validation\n",
        "    def calculate_f1_score(preds, masks):\n",
        "        # preds: logits, masks: ground truth (0/1)\n",
        "        probs = torch.sigmoid(preds)\n",
        "        # Threshold at 0.5 for binary prediction\n",
        "        binary_preds = (probs > 0.5).float()\n",
        "        # Flatten tensors for F1 score calculation\n",
        "        return f1_score(masks.cpu().numpy().flatten(), binary_preds.cpu().numpy().flatten(), zero_division=0)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model_seg.train() # Set model to training mode\n",
        "        running_loss = 0.0\n",
        "        for batch_idx, (images, masks) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1} Training\")):\n",
        "            images, masks = images.to(device), masks.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model_seg.forward_seg(images)\n",
        "            loss = criterion(outputs, masks)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = running_loss / len(train_loader)\n",
        "\n",
        "        # Validation phase\n",
        "        model_seg.eval() # Set model to evaluation mode\n",
        "        val_loss = 0.0\n",
        "        val_f1 = 0.0\n",
        "        with torch.no_grad():\n",
        "            for images, masks in tqdm(val_loader, desc=f\"Epoch {epoch+1} Validation\"):\n",
        "                images, masks = images.to(device), masks.to(device)\n",
        "                outputs = model_seg.forward_seg(images)\n",
        "                loss = criterion(outputs, masks)\n",
        "                val_loss += loss.item()\n",
        "                val_f1 += calculate_f1_score(outputs, masks)\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        avg_val_f1 = val_f1 / len(val_loader)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val F1: {avg_val_f1:.4f}\")\n",
        "\n",
        "        # Save best model\n",
        "        if avg_val_f1 > best_val_f1:\n",
        "            best_val_f1 = avg_val_f1\n",
        "            torch.save(model_seg.state_dict(), best_model_path)\n",
        "            print(f\"â­ New best model saved with Val F1: {best_val_f1:.4f}\")\n",
        "    print(\"Training complete.\")\n",
        "else:\n",
        "    print(\"\\nSkipping model training due to missing data or empty data loaders.\")\n",
        "\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def segment_prob_map(pil):\n",
        "    x = torch.from_numpy(np.array(pil.resize((IMG_SIZE, IMG_SIZE)), np.float32)/255.).permute(2,0,1)[None].to(device)\n",
        "    prob = torch.sigmoid(model_seg.forward_seg(x))[0,0].cpu().numpy()\n",
        "    return prob\n",
        "\n",
        "@torch.no_grad()\n",
        "def segment_prob_map_with_tta(pil):\n",
        "    # 1. Preprocessing: Resize, Normalize, and move to Device\n",
        "    # Use val_transforms for consistency in inference preparation\n",
        "    # pil_tensor = val_transforms(pil).to(device)\n",
        "    # The original code's image preparation for inference: manual resize and to_tensor conversion\n",
        "    x = torch.from_numpy(np.array(pil.resize((IMG_SIZE, IMG_SIZE)), np.float32)/255.).permute(2,0,1)[None].to(device)\n",
        "\n",
        "    predictions = []\n",
        "\n",
        "    # 2. Original Prediction\n",
        "    pred_orig = torch.sigmoid(model_seg.forward_seg(x))\n",
        "    predictions.append(pred_orig)\n",
        "\n",
        "    # 3. Horizontal Flip TTA (dim 3)\n",
        "    # Flip input -> Predict -> Flip output back\n",
        "    pred_h = torch.sigmoid(model_seg.forward_seg(torch.flip(x, dims=[3])))\n",
        "    predictions.append(torch.flip(pred_h, dims=[3]))\n",
        "\n",
        "    # 4. Vertical Flip TTA (dim 2)\n",
        "    # Flip input -> Predict -> Flip output back\n",
        "    pred_v = torch.sigmoid(model_seg.forward_seg(torch.flip(x, dims=[2])))\n",
        "    predictions.append(torch.flip(pred_v, dims=[2]))\n",
        "\n",
        "    # 5. Average the predictions and format as numpy\n",
        "    # We stack the 3 predictions and take the mean across the stack dimension (0)\n",
        "    prob = torch.stack(predictions).mean(0)[0, 0].cpu().numpy()\n",
        "\n",
        "    return prob\n",
        "\n",
        "def enhanced_adaptive_mask(prob, alpha_grad=0.45):\n",
        "    gx = cv2.Sobel(prob, cv2.CV_32F, 1, 0, ksize=3)\n",
        "    gy = cv2.Sobel(prob, cv2.CV_32F, 0, 1, ksize=3)\n",
        "    grad_mag = np.sqrt(gx**2 + gy**2)\n",
        "    grad_norm = grad_mag / (grad_mag.max() + 1e-6)\n",
        "    enhanced = (1 - alpha_grad) * prob + alpha_grad * grad_norm\n",
        "    enhanced = cv2.GaussianBlur(enhanced, (3,3), 0)\n",
        "    thr = np.mean(enhanced) + 0.3 * np.std(enhanced)\n",
        "    mask = (enhanced > thr).astype(np.uint8)\n",
        "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, np.ones((5,5), np.uint8))\n",
        "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, np.ones((3,3), np.uint8))\n",
        "    return mask, thr\n",
        "\n",
        "def finalize_mask(prob, orig_size):\n",
        "    mask, thr = enhanced_adaptive_mask(prob)\n",
        "    mask = cv2.resize(mask, orig_size, interpolation=cv2.INTER_NEAREST)\n",
        "    return mask, thr\n",
        "\n",
        "def pipeline_final(pil):\n",
        "    model_seg.eval() # Ensure model is in eval mode for inference\n",
        "    if USE_TTA:\n",
        "        prob = segment_prob_map_with_tta(pil)\n",
        "    else:\n",
        "        prob = segment_prob_map(pil)\n",
        "    mask, thr = finalize_mask(prob, pil.size)\n",
        "    area = int(mask.sum())\n",
        "    # The mask used here for mean calculation should be resized to IMG_SIZE for proper indexing into 'prob'\n",
        "    mask_for_mean = cv2.resize(mask,(IMG_SIZE,IMG_SIZE),interpolation=cv2.INTER_NEAREST)\n",
        "    mean_inside = float(prob[mask_for_mean==1].mean()) if area>0 else 0.0\n",
        "    if area < AREA_THR or mean_inside < MEAN_THR:\n",
        "        return \"authentic\", None, {\"area\": area, \"mean_inside\": mean_inside, \"thr\": thr}\n",
        "    return \"forged\", mask, {\"area\": area, \"mean_inside\": mean_inside, \"thr\": thr}\n",
        "\n",
        "import itertools\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def grid_search_area_mean(forg_paths, auth_paths, mask_dir):\n",
        "    # Return default values and print message if no valid data for grid search\n",
        "    if not forg_paths and not auth_paths:\n",
        "        print(\"Skipping grid search as validation data is not available.\")\n",
        "        return {'AREA_THR': 200, 'MEAN_THR': 0.22}\n",
        "\n",
        "    mean_range = [round(x, 2) for x in np.arange(0.20, 0.291, 0.01)]\n",
        "    area_range = [200]\n",
        "    val_set = [(p, \"forged\") for p in forg_paths] + [(p, \"authentic\") for p in auth_paths]\n",
        "\n",
        "    if not val_set:\n",
        "        print(\"Skipping grid search as validation set is empty.\")\n",
        "        return {'AREA_THR': 200, 'MEAN_THR': 0.22}\n",
        "\n",
        "    print(f\"ðŸš€ Step 1: Caching probability maps for ALL {len(val_set)} images...\")\n",
        "    cache = []\n",
        "    for p, label in tqdm(val_set):\n",
        "        try:\n",
        "            pil = Image.open(p).convert(\"RGB\")\n",
        "            w, h = pil.size\n",
        "\n",
        "            # Ensure model is in eval mode for caching inference\n",
        "            model_seg.eval()\n",
        "            prob = segment_prob_map_with_tta(pil) if USE_TTA else segment_prob_map(pil)\n",
        "\n",
        "            mask_raw, _ = enhanced_adaptive_mask(prob)\n",
        "            mask_resized = cv2.resize(mask_raw, (w, h), interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "            if label == \"forged\":\n",
        "                m_gt = np.load(Path(mask_dir)/f\"{Path(p).stem}.npy\")\n",
        "                if m_gt.ndim == 3: m_gt = np.max(m_gt, axis=0)\n",
        "                m_gt = (m_gt > 0).astype(np.uint8)\n",
        "            else:\n",
        "                m_gt = np.zeros((h, w), np.uint8)\n",
        "\n",
        "            cache.append({\"prob\": prob, \"mask\": mask_resized, \"gt\": m_gt, \"label\": label})\n",
        "        except FileNotFoundError as e:\n",
        "            print(f\"Skipping {p} due to FileNotFoundError during caching: {e}\")\n",
        "            continue\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping {p} due to unexpected error during caching: {e}\")\n",
        "            continue\n",
        "\n",
        "    if not cache: # If no items were successfully cached, return default params\n",
        "        print(\"No valid images found for grid search caching. Returning default parameters.\")\n",
        "        return {'AREA_THR': 200, 'MEAN_THR': 0.22}\n",
        "\n",
        "    best_f1 = -1\n",
        "    best_params = {}\n",
        "    combinations = list(itertools.product(area_range, mean_range))\n",
        "\n",
        "    for a_thr, m_thr in combinations:\n",
        "        current_f1s = []\n",
        "        for item in cache:\n",
        "            mask = item[\"mask\"]\n",
        "            area = int(mask.sum())\n",
        "\n",
        "            mask_small = cv2.resize(mask, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_NEAREST)\n",
        "            mean_in = float(item[\"prob\"][mask_small == 1].mean()) if area > 0 else 0.0\n",
        "\n",
        "            is_forged = (area >= a_thr and mean_in >= m_thr)\n",
        "            m_pred = (mask > 0).astype(np.uint8) if is_forged else np.zeros_like(item[\"gt\"])\n",
        "\n",
        "            f1 = f1_score(item[\"gt\"].flatten(), m_pred.flatten(),\n",
        "                          zero_division=1 if item[\"label\"] == \"authentic\" else 0)\n",
        "            current_f1s.append(f1)\n",
        "\n",
        "        if current_f1s: # Only calculate average if any F1 scores were computed\n",
        "            avg_f1 = np.mean(current_f1s)\n",
        "            if avg_f1 > best_f1:\n",
        "                best_f1 = avg_f1\n",
        "                best_params = {\"AREA_THR\": a_thr, \"MEAN_THR\": m_thr}\n",
        "                print(f\"â­ New Best F1: {best_f1:.4f} -> AREA: {a_thr}, MEAN: {m_thr}\")\n",
        "\n",
        "    if not best_params: # If no suitable parameters found, return defaults\n",
        "        print(\"No optimal parameters found during grid search. Returning default parameters.\")\n",
        "        return {'AREA_THR': 200, 'MEAN_THR': 0.22}\n",
        "\n",
        "    return best_params\n",
        "\n",
        "if GRID_SEARCH_ENABLED:\n",
        "    if val_forg or val_auth: # Only run grid search if there's actual validation data\n",
        "        print(\"Starting grid search for optimal parameters...\")\n",
        "        # Temporarily set model to eval mode for grid search inference if it was in train mode\n",
        "        current_model_mode = model_seg.training\n",
        "        model_seg.eval()\n",
        "        best_cfg = grid_search_area_mean(val_forg, val_auth, MASK_DIR)\n",
        "        AREA_THR = best_cfg['AREA_THR']\n",
        "        MEAN_THR = best_cfg['MEAN_THR']\n",
        "        print(f\"Final Grid Search Parameters: AREA_THR={AREA_THR}, MEAN_THR={MEAN_THR}\")\n",
        "        # Restore model mode\n",
        "        if current_model_mode: model_seg.train()\n",
        "    else:\n",
        "        print(\"Skipping grid search because validation data lists are empty.\")\n",
        "else:\n",
        "    print(\"Skipping grid search because GRID_SEARCH_ENABLED flag is False.\")\n",
        "\n",
        "\n",
        "# --- FURTHER MODIFICATION FOR VALIDATION F1 SCORE CALCULATION ---\n",
        "from sklearn.metrics import f1_score\n",
        "val_items = [(p, 1) for p in val_forg[:10]] # This will be empty if val_forg is empty\n",
        "results = []\n",
        "if val_items:\n",
        "    print(\"\\nCalculating F1-score for validation forged images:\")\n",
        "    # Ensure model is in eval mode for validation inference\n",
        "    current_model_mode = model_seg.training\n",
        "    model_seg.eval()\n",
        "    for p,_ in tqdm(val_items, desc=\"Validation forged-only\"):\n",
        "        try:\n",
        "            pil = Image.open(p).convert(\"RGB\")\n",
        "            label, m_pred, dbg = pipeline_final(pil)\n",
        "            m_gt = np.load(Path(MASK_DIR)/f\"{Path(p).stem}.npy\")\n",
        "            if m_gt.ndim==3: m_gt=np.max(m_gt,axis=0)\n",
        "            m_gt=(m_gt>0).astype(np.uint8)\n",
        "            m_pred=(m_pred>0).astype(np.uint8) if m_pred is not None else np.zeros_like(m_gt)\n",
        "            f1 = f1_score(m_gt.flatten(), m_pred.flatten(), zero_division=0)\n",
        "            results.append((Path(p).stem, f1, dbg))\n",
        "        except FileNotFoundError as e:\n",
        "            print(f\"Skipping {p} in validation F1 calculation due to FileNotFoundError: {e}\")\n",
        "            continue\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping {p} in validation F1 calculation due to unexpected error: {e}\")\n",
        "            continue\n",
        "    # Restore model mode\n",
        "    if current_model_mode: model_seg.train()\n",
        "\n",
        "    if results:\n",
        "        print(\"\\n F1-score par image falsifiÃ©e:\\n\")\n",
        "        for cid,f1,dbg in results:\n",
        "            print(f\"{cid} â€” F1={f1:.4f} | area={dbg['area']} mean={dbg['mean_inside']:.3f} thr={dbg['thr']:.3f}\")\n",
        "        print(f\"\\n Moyenne F1 (falsifiÃ©es) = {np.mean([r[1] for r in results]):.4f}\")\n",
        "    else:\n",
        "        print(\"\\n No results to display for validation F1-score (forged-only).\")\n",
        "else:\n",
        "    print(\"Skipping validation F1-score calculation (forged-only) because validation data is not available.\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Competition data not found at '/kaggle/input/recodai-luc-scientific-image-forgery-detection'. Skipping data loading, training setup, and grid search.\n",
            "Please ensure the 'recodai-luc-scientific-image-forgery-detection' dataset is available or adjust BASE_DIR.\n",
            "âœ… Loaded pretrained model from: cnndinov2_a78_model/model_seg_final.pt\n",
            "\n",
            "Skipping model training due to missing data or empty data loaders.\n",
            "Skipping grid search because GRID_SEARCH_ENABLED flag is False.\n",
            "Skipping validation F1-score calculation (forged-only) because validation data is not available.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4476b7c"
      },
      "source": [
        "## Evaluate and Refine Post-processing Thresholds\n",
        "\n",
        "### Subtask:\n",
        "After training the model and loading the data, re-evaluate and potentially refine the AREA_THR and MEAN_THR parameters for optimal F1-score on the validation set using grid search.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d81772cb"
      },
      "source": [
        "## Evaluate and Refine Post-processing Thresholds\n",
        "\n",
        "### Subtask:\n",
        "After training the model and loading the data, re-evaluate and potentially refine the AREA_THR and MEAN_THR parameters for optimal F1-score on the validation set using grid search.\n",
        "\n",
        "### Current Status & Next Steps for Data Availability:\n",
        "\n",
        "**Issue**: The previous attempts to access the competition data from `/kaggle/input/recodai-luc-scientific-image-forgery-detection` resulted in a `FileNotFoundError` or `401 Client Error` during `kagglehub.competition_download`.\n",
        "\n",
        "**Reason**: This typically means the Kaggle API credentials are not correctly set up, or the competition rules have not been accepted, or the data is simply not available in the current environment.\n",
        "\n",
        "**Action Required (Manual Intervention)**:\n",
        "\n",
        "To proceed with model training and threshold refinement, the competition data must be accessible. Please ensure *one* of the following is true:\n",
        "\n",
        "1.  **Kaggle API Authentication & Rules Acceptance**: Go to the competition page on Kaggle (https://www.kaggle.com/competitions/recodai-luc-scientific-image-forgery-detection/rules), make sure you have accepted the rules, and ensure your Kaggle API credentials are correctly configured in your environment. Then, re-run the `kagglehub.login()` and data download cells (`aooYS7254ZsS` and `m63MTQec4ZsV`).\n",
        "2.  **Manual Data Upload**: If programmatic download continues to fail, manually download the `recodai-luc-scientific-image-forgery-detection` dataset from Kaggle to your local machine. Then, upload it to your Colab environment (e.g., into `/content/kaggle/input/recodai-luc-scientific-image-forgery-detection` or another suitable path).\n",
        "\n",
        "**Once the data is accessible, re-execute all relevant code cells starting from the data loading and model setup (`HLCSDp1l4ZsW` or `cd109df5`, then `bac9eadd` or `4633118c`) to allow the training process and the grid search for `AREA_THR` and `MEAN_THR` to run successfully.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b399a4c6"
      },
      "source": [
        "## Evaluate and Refine Post-processing Thresholds\n",
        "\n",
        "### Subtask:\n",
        "After training the model and loading the data, re-evaluate and potentially refine the AREA_THR and MEAN_THR parameters for optimal F1-score on the validation set using grid search.\n",
        "\n",
        "### Current Status & Next Steps for Data Availability:\n",
        "\n",
        "**Issue**: The previous attempts to access the competition data from `/kaggle/input/recodai-luc-scientific-image-forgery-detection` resulted in a `FileNotFoundError` or `401 Client Error` during `kagglehub.competition_download`.\n",
        "\n",
        "**Reason**: This typically means the Kaggle API credentials are not correctly set up, or the competition rules have not been accepted, or the data is simply not available in the current environment.\n",
        "\n",
        "**Action Required (Manual Intervention)**:\n",
        "\n",
        "To proceed with model training and threshold refinement, the competition data must be accessible. Please ensure *one* of the following is true:\n",
        "\n",
        "1.  **Kaggle API Authentication & Rules Acceptance**: Go to the competition page on Kaggle (https://www.kaggle.com/competitions/recodai-luc-scientific-image-forgery-detection/rules), make sure you have accepted the rules, and ensure your Kaggle API credentials are correctly configured in your environment. Then, re-run the `kagglehub.login()` and data download cells (`aooYS7254ZsS` and `m63MTQec4ZsV`).\n",
        "2.  **Manual Data Upload**: If programmatic download continues to fail, manually download the `recodai-luc-scientific-image-forgery-detection` dataset from Kaggle to your local machine. Then, upload it to your Colab environment (e.g., into `/content/kaggle/input/recodai-luc-scientific-image-forgery-detection` or another suitable path).\n",
        "\n",
        "**Once the data is accessible, re-execute all relevant code cells starting from the data loading and model setup (`HLCSDp1l4ZsW` or `cd109df5`, then `bac9eadd` or `4633118c`) to allow the training process and the grid search for `AREA_THR` and `MEAN_THR` to run successfully.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c17ddba8"
      },
      "source": [
        "## Evaluate and Refine Post-processing Thresholds\n",
        "\n",
        "### Subtask:\n",
        "After training the model and loading the data, re-evaluate and potentially refine the AREA_THR and MEAN_THR parameters for optimal F1-score on the validation set using grid search.\n",
        "\n",
        "#### Instructions\n",
        "1.  **Address Missing Competition Data**: The previous executions indicated that the competition data required for training and evaluation is not available (`/kaggle/input/recodai-luc-scientific-image-forgery-detection`). The initial attempt to download the data via `kagglehub.competition_download` failed due to a `401 Client Error` (permission denied).\n",
        "2.  **Ensure Data Availability**: To proceed with model training and threshold refinement, you must ensure the competition data (`recodai-luc-scientific-image-forgery-detection`) is accessible. This typically involves:\n",
        "    *   **Kaggle API Authentication**: Verify your Kaggle API credentials are correctly set up and authenticated in your environment.\n",
        "    *   **Accepting Competition Rules**: Visit the competition page (`https://www.kaggle.com/competitions/recodai-luc-scientific-image-forgery-detection/rules`) and ensure you have accepted its rules.\n",
        "    *   **Downloading Data**: If `kagglehub.competition_download` still fails, you may need to manually download the dataset from Kaggle and upload it to your Colab environment, then adjust the `BASE_DIR` variable to point to its location.\n",
        "3.  **Re-run the Code**: Once the competition data is available (i.e., the path `/kaggle/input/recodai-luc-scientific-image-forgery-detection` exists and contains the necessary subdirectories), re-execute the code cells related to data loading, model setup, and training.\n",
        "4.  **Execute Grid Search**: The provided code already includes the `grid_search_area_mean` function to refine `AREA_THR` and `MEAN_THR`. This function will automatically run after training if the `BASE_DIR` is found and `GRID_SEARCH_ENABLED` is `True`.\n",
        "5.  **Analyze Results**: Review the output from the grid search, specifically the `â­ New Best F1` messages, to understand the optimal `AREA_THR` and `MEAN_THR` values found. These values will then be used by `pipeline_final` for inference.\n",
        "6.  **Review Validation F1-score**: Examine the final average F1-score for forged images on the validation set to assess the model's performance with the refined thresholds."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "323fac0d"
      },
      "source": [
        "## Evaluate and Refine Post-processing Thresholds\n",
        "\n",
        "### Subtask:\n",
        "After training the model and loading the data, re-evaluate and potentially refine the AREA_THR and MEAN_THR parameters for optimal F1-score on the validation set using grid search.\n",
        "\n",
        "#### Instructions\n",
        "1.  **Address Missing Competition Data**: The previous executions indicated that the competition data required for training and evaluation is not available (`/kaggle/input/recodai-luc-scientific-image-forgery-detection`). The initial attempt to download the data via `kagglehub.competition_download` failed due to a `401 Client Error` (permission denied).\n",
        "2.  **Ensure Data Availability**: To proceed with model training and threshold refinement, you must ensure the competition data (`recodai-luc-scientific-image-forgery-detection`) is accessible. This typically involves:\n",
        "    *   **Kaggle API Authentication**: Verify your Kaggle API credentials are correctly set up and authenticated in your environment.\n",
        "    *   **Accepting Competition Rules**: Visit the competition page (`https://www.kaggle.com/competitions/recodai-luc-scientific-image-forgery-detection/rules`) and ensure you have accepted its rules.\n",
        "    *   **Downloading Data**: If `kagglehub.competition_download` still fails, you may need to manually download the dataset from Kaggle and upload it to your Colab environment, then adjust the `BASE_DIR` variable to point to its location.\n",
        "3.  **Re-run the Code**: Once the competition data is available (i.e., the path `/kaggle/input/recodai-luc-scientific-image-forgery-detection` exists and contains the necessary subdirectories), re-execute the code cells related to data loading, model setup, and training.\n",
        "4.  **Execute Grid Search**: The provided code already includes the `grid_search_area_mean` function to refine `AREA_THR` and `MEAN_THR`. This function will automatically run after training if the `BASE_DIR` is found and `GRID_SEARCH_ENABLED` is `True`.\n",
        "5.  **Analyze Results**: Review the output from the grid search, specifically the `â­ New Best F1` messages, to understand the optimal `AREA_THR` and `MEAN_THR` values found. These values will then be used by `pipeline_final` for inference.\n",
        "6.  **Review Validation F1-score**: Examine the final average F1-score for forged images on the validation set to assess the model's performance with the refined thresholds."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a7cb311"
      },
      "source": [
        "## Evaluate and Refine Post-processing Thresholds\n",
        "\n",
        "### Subtask:\n",
        "After training the model and loading the data, re-evaluate and potentially refine the AREA_THR and MEAN_THR parameters for optimal F1-score on the validation set using grid search.\n",
        "\n",
        "#### Instructions\n",
        "1.  **Address Missing Competition Data**: The previous executions indicated that the competition data required for training and evaluation is not available (`/kaggle/input/recodai-luc-scientific-image-forgery-detection`). The initial attempt to download the data via `kagglehub.competition_download` failed due to a `401 Client Error` (permission denied).\n",
        "2.  **Ensure Data Availability**: To proceed with model training and threshold refinement, you must ensure the competition data (`recodai-luc-scientific-image-forgery-detection`) is accessible. This typically involves:\n",
        "    *   **Kaggle API Authentication**: Verify your Kaggle API credentials are correctly set up and authenticated in your environment.\n",
        "    *   **Accepting Competition Rules**: Visit the competition page (`https://www.kaggle.com/competitions/recodai-luc-scientific-image-forgery-detection/rules`) and ensure you have accepted its rules.\n",
        "    *   **Downloading Data**: If `kagglehub.competition_download` still fails, you may need to manually download the dataset from Kaggle and upload it to your Colab environment, then adjust the `BASE_DIR` variable to point to its location.\n",
        "3.  **Re-run the Code**: Once the competition data is available (i.e., the path `/kaggle/input/recodai-luc-scientific-image-forgery-detection` exists and contains the necessary subdirectories), re-execute the code cells related to data loading, model setup, and training.\n",
        "4.  **Execute Grid Search**: The provided code already includes the `grid_search_area_mean` function to refine `AREA_THR` and `MEAN_THR`. This function will automatically run after training if the `BASE_DIR` is found and `GRID_SEARCH_ENABLED` is `True`.\n",
        "5.  **Analyze Results**: Review the output from the grid search, specifically the `â­ New Best F1` messages, to understand the optimal `AREA_THR` and `MEAN_THR` values found. These values will then be used by `pipeline_final` for inference.\n",
        "6.  **Review Validation F1-score**: Examine the final average F1-score for forged images on the validation set to assess the model's performance with the refined thresholds."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2c0d7af7"
      },
      "source": [
        "## Evaluate and Refine Post-processing Thresholds\n",
        "\n",
        "### Subtask:\n",
        "After training the model and loading the data, re-evaluate and potentially refine the AREA_THR and MEAN_THR parameters for optimal F1-score on the validation set using grid search.\n",
        "\n",
        "#### Instructions\n",
        "1.  **Address Missing Competition Data**: The previous executions indicated that the competition data required for training and evaluation is not available (`/kaggle/input/recodai-luc-scientific-image-forgery-detection`). The initial attempt to download the data via `kagglehub.competition_download` failed due to a `401 Client Error` (permission denied).\n",
        "2.  **Ensure Data Availability**: To proceed with model training and threshold refinement, you must ensure the competition data (`recodai-luc-scientific-image-forgery-detection`) is accessible. This typically involves:\n",
        "    *   **Kaggle API Authentication**: Verify your Kaggle API credentials are correctly set up and authenticated in your environment.\n",
        "    *   **Accepting Competition Rules**: Visit the competition page (`https://www.kaggle.com/competitions/recodai-luc-scientific-image-forgery-detection/rules`) and ensure you have accepted its rules.\n",
        "    *   **Downloading Data**: If `kagglehub.competition_download` still fails, you may need to manually download the dataset from Kaggle and upload it to your Colab environment, then adjust the `BASE_DIR` variable to point to its location.\n",
        "3.  **Re-run the Code**: Once the competition data is available (i.e., the path `/kaggle/input/recodai-luc-scientific-image-forgery-detection` exists and contains the necessary subdirectories), re-execute the code cells related to data loading, model setup, and training.\n",
        "4.  **Execute Grid Search**: The provided code already includes the `grid_search_area_mean` function to refine `AREA_THR` and `MEAN_THR`. This function will automatically run after training if the `BASE_DIR` is found and `GRID_SEARCH_ENABLED` is `True`.\n",
        "5.  **Analyze Results**: Review the output from the grid search, specifically the `â­ New Best F1` messages, to understand the optimal `AREA_THR` and `MEAN_THR` values found. These values will then be used by `pipeline_final` for inference.\n",
        "6.  **Review Validation F1-score**: Examine the final average F1-score for forged images on the validation set to assess the model's performance with the refined thresholds."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1611e7d8"
      },
      "source": [
        "## Evaluate and Refine Post-processing Thresholds\n",
        "\n",
        "### Subtask:\n",
        "Re-evaluate and potentially refine the AREA_THR and MEAN_THR parameters for optimal F1-score on the validation set using grid search, assuming the competition data is now available.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7fc496a"
      },
      "source": [
        "```markdown\n",
        "**Data Availability Check and Grid Search Implications**\n",
        "\n",
        "Based on previous execution logs, the competition data for `recodai-luc-scientific-image-forgery-detection` is currently *not* available at the specified `BASE_DIR` (`/kaggle/input/recodai-luc-scientific-image-forgery-detection`). The `kagglehub.competition_download` in cell `m63MTQec4ZsV` failed with a 401 Unauthorized error, preventing the data from being downloaded or mounted.\n",
        "\n",
        "As a result, the subsequent steps that rely on this data, specifically the data loading for `ForgerySegDataset`, the training loop, and the grid search for `AREA_THR` and `MEAN_THR` parameters, will be skipped or will use default values due to empty data loaders. While the model components (DINOv2 and CNNDINOv2) can still be loaded from their local extractions, the core task of *refining* the post-processing thresholds using the validation set cannot be performed until the competition data is made accessible.\n",
        "\n",
        "I will now proceed to execute cell `4633118c` as instructed, but please be aware that the grid search will not run, and `AREA_THR` and `MEAN_THR` will retain their default or last-set values (200 and 0.22 respectively) due to the missing data.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e232977c"
      },
      "source": [
        "**Reasoning**:\n",
        "As requested, I will now execute cell `4633118c`. This cell contains the complete setup including the corrected `DinoSegmenter` class, data augmentation, and the training loop. Based on the previous data availability checks, the training and grid search components are expected to be skipped due to the missing `BASE_DIR`, but model loading should proceed successfully.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdaf26f7",
        "outputId": "3c5ca588-9ae8-4502-fea5-e877564b2ab3"
      },
      "source": [
        "import os, cv2, json, math, random, torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn, torch.nn.functional as F, torch.optim as optim\n",
        "from transformers import AutoImageProcessor, AutoModel\n",
        "import torchvision.transforms as transforms # Import torchvision transforms\n",
        "\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    # This forces CUDA to use deterministic algorithms (slower but consistent)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed_everything(42)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "BASE_DIR  = \"/kaggle/input/recodai-luc-scientific-image-forgery-detection\"\n",
        "AUTH_DIR  = f\"{BASE_DIR}/train_images/authentic\"\n",
        "FORG_DIR  = f\"{BASE_DIR}/train_images/forged\"\n",
        "MASK_DIR  = f\"{BASE_DIR}/train_masks\"\n",
        "TEST_DIR  = f\"{BASE_DIR}/test_images\"\n",
        "\n",
        "# Updated paths from previous extraction steps\n",
        "DINO_PATH = 'dinov2_base' # Updated to local extraction directory\n",
        "MODEL_LOC = 'cnndinov2_a78_model/model_seg_final.pt' # Corrected path based on directory listing\n",
        "\n",
        "IMG_SIZE = 518\n",
        "BATCH_SIZE = 2\n",
        "\n",
        "# INFERENCE UTILS\n",
        "AREA_THR = 200\n",
        "MEAN_THR = 0.22\n",
        "USE_TTA = True\n",
        "GRID_SEARCH_ENABLED = True # Renamed to avoid conflict if GRID_SEARCH is set to False later\n",
        "\n",
        "# --- Data Augmentation Transforms ---\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomVerticalFlip(p=0.5),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.ToTensor(), # Converts PIL Image to Tensor, normalizes to [0,1]\n",
        "    # No normalization here as the original code scales to /255. manually\n",
        "])\n",
        "\n",
        "val_transforms = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "class ForgerySegDataset(Dataset):\n",
        "    def __init__(self, auth_paths, forg_paths, mask_dir, transform=None):\n",
        "        self.samples = []\n",
        "        for p in forg_paths:\n",
        "            m = os.path.join(mask_dir, Path(p).stem + \".npy\")\n",
        "            if os.path.exists(m):\n",
        "                self.samples.append((p, m, 'forged')) # Added 'forged' label\n",
        "            else:\n",
        "                # If mask is missing for a forged image, treat as authentic for now\n",
        "                # Or skip, depending on desired behavior. For training, we need masks.\n",
        "                # Here, we skip if no mask for forged image (as it would be useless for segmentation training)\n",
        "                print(f\"Warning: Mask not found for forged image {p}. Skipping this sample.\")\n",
        "        for p in auth_paths:\n",
        "            self.samples.append((p, None, 'authentic')) # Added 'authentic' label\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self): return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, mask_path, label_type = self.samples[idx]\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        if mask_path is None: # Authentic image\n",
        "            # Create an empty mask for authentic images, scaled to IMG_SIZE\n",
        "            mask = np.zeros((IMG_SIZE, IMG_SIZE), np.uint8)\n",
        "        else: # Forged image with a mask\n",
        "            m = np.load(mask_path)\n",
        "            if m.ndim == 3: m = np.max(m, axis=0)\n",
        "            mask = (m > 0).astype(np.uint8)\n",
        "            # Resize mask to IMG_SIZE before applying transforms if any spatial transforms are used\n",
        "            mask = Image.fromarray(mask).resize((IMG_SIZE, IMG_SIZE), Image.NEAREST)\n",
        "            mask = np.array(mask)\n",
        "\n",
        "        # Apply image transformations\n",
        "        # Note: torchvision.transforms expects PIL Image or Tensor.\n",
        "        # Custom transforms for both image and mask should be handled carefully\n",
        "        # Here, we apply image transforms and then process mask for consistency.\n",
        "        # For random operations, it's better to combine image and mask transforms.\n",
        "        # For simplicity, we resize mask earlier and then apply 'ToTensor' logic.\n",
        "\n",
        "        # Convert image to tensor using transform\n",
        "        if self.transform:\n",
        "            img_t = self.transform(img)\n",
        "        else:\n",
        "            # Default processing if no transform (e.g., for validation/inference)\n",
        "            img_r = img.resize((IMG_SIZE, IMG_SIZE))\n",
        "            img_t = torch.from_numpy(np.array(img_r, np.float32)/255.).permute(2,0,1)\n",
        "\n",
        "        # Convert mask to tensor (already resized to IMG_SIZE in previous logic)\n",
        "        mask_t = torch.from_numpy(mask[None, ...].astype(np.float32))\n",
        "\n",
        "        return img_t, mask_t\n",
        "\n",
        "\n",
        "\n",
        "#  MODEL (DINOv2 + Decoder)\n",
        "\n",
        "from transformers import AutoImageProcessor, AutoModel\n",
        "processor = AutoImageProcessor.from_pretrained(DINO_PATH, local_files_only=True, use_fast=False)\n",
        "encoder = AutoModel.from_pretrained(DINO_PATH, local_files_only=True).eval().to(device)\n",
        "\n",
        "class DinoTinyDecoder(nn.Module):\n",
        "    def __init__(self, in_ch=768, out_ch=1):\n",
        "        super().__init__()\n",
        "        # Block 1: 768 -> 384\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, 384, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout2d(0.1)\n",
        "        )\n",
        "        # Block 2: 384 -> 192\n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.Conv2d(384, 192, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout2d(0.1)\n",
        "        )\n",
        "        # Block 3: 192 -> 96\n",
        "        self.block3 = nn.Sequential(\n",
        "            nn.Conv2d(192, 96, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        # Final Output: 96 -> 1\n",
        "        self.conv_out = nn.Conv2d(96, out_ch, kernel_size=1)\n",
        "\n",
        "    def forward(self, f, target_size):\n",
        "        # f: [B, 768, 37, 37]\n",
        "\n",
        "        # Step 1: Up to ~74x74\n",
        "        x = F.interpolate(self.block1(f), size=(74, 74), mode='bilinear', align_corners=False)\n",
        "\n",
        "        # Step 2: Up to ~148x148\n",
        "        x = F.interpolate(self.block2(x), size=(148, 148), mode='bilinear', align_corners=False)\n",
        "\n",
        "        # Step 3: Up to ~296x296\n",
        "        x = F.interpolate(self.block3(x), size=(296, 296), mode='bilinear', align_corners=False)\n",
        "\n",
        "        # Step 4: Final jump to 518x518\n",
        "        x = self.conv_out(x)\n",
        "        x = F.interpolate(x, size=target_size, mode='bilinear', align_corners=False)\n",
        "\n",
        "        return x\n",
        "\n",
        "class DinoSegmenter(nn.Module):\n",
        "    def __init__(self, encoder, processor):\n",
        "        super().__init__()\n",
        "        self.encoder, self.processor = encoder, processor\n",
        "        # Removed or commented out the line freezing encoder parameters to enable fine-tuning\n",
        "        # for p in self.encoder.parameters(): p.requires_grad = False\n",
        "        self.seg_head = DinoTinyDecoder(768,1)\n",
        "    def forward_features(self,x):\n",
        "        # Ensure input to processor is a list of PIL Images (or similar format it expects)\n",
        "        # x is already a tensor [B, C, H, W] ranging from 0-1 from ToTensor()\n",
        "        # Need to convert it back to a list of PIL images or numpy arrays for the processor.\n",
        "        # This part requires careful handling based on AutoImageProcessor's specific expectations.\n",
        "        # Assuming processor can take a list of numpy arrays [H,W,C] or PIL images.\n",
        "        # The `x*255.clamp(0,255).byte().permute(0,2,3,1).cpu().numpy()` converts a batch of float tensors to a batch of uint8 numpy arrays.\n",
        "        imgs = (x*255).clamp(0,255).byte().permute(0,2,3,1).cpu().numpy()\n",
        "        # The processor expects a list of images (PIL Image, NumPy array, or PyTorch tensor)\n",
        "        inputs = self.processor(images=[Image.fromarray(img) for img in imgs], return_tensors=\"pt\").to(x.device)\n",
        "\n",
        "        feats = self.encoder(**inputs).last_hidden_state\n",
        "        B,N,C = feats.shape\n",
        "        fmap = feats[:,1:,:].permute(0,2,1)\n",
        "        s = int(math.sqrt(N-1))\n",
        "        fmap = fmap.reshape(B,C,s,s)\n",
        "        return fmap\n",
        "    def forward_seg(self,x):\n",
        "        fmap = self.forward_features(x)\n",
        "        return self.seg_head(fmap,(IMG_SIZE,IMG_SIZE))\n",
        "\n",
        "\n",
        "# --- MODIFIED PART TO HANDLE MISSING BASE_DIR AND TRAINING LOOP ---\n",
        "# Initialize data-related variables as empty lists or None\n",
        "auth_imgs, forg_imgs, train_auth, val_auth, train_forg, val_forg = [], [], [], [], [], []\n",
        "train_loader, val_loader = None, None\n",
        "\n",
        "if not os.path.exists(BASE_DIR):\n",
        "    print(f\"Warning: Competition data not found at '{BASE_DIR}'. Skipping data loading, training setup, and grid search.\")\n",
        "    print(\"Please ensure the 'recodai-luc-scientific-image-forgery-detection' dataset is available or adjust BASE_DIR.\")\n",
        "    GRID_SEARCH_ENABLED = False # Disable grid search if data is not available\n",
        "else:\n",
        "    auth_imgs = sorted([str(Path(AUTH_DIR)/f) for f in os.listdir(AUTH_DIR)])\n",
        "    forg_imgs = sorted([str(Path(FORG_DIR)/f) for f in os.listdir(FORG_DIR)])\n",
        "    train_auth, val_auth = train_test_split(auth_imgs, test_size=0.2, random_state=42)\n",
        "    train_forg, val_forg = train_test_split(forg_imgs, test_size=0.2, random_state=42)\n",
        "\n",
        "    train_loader = DataLoader(ForgerySegDataset(train_auth, train_forg, MASK_DIR, transform=train_transforms),\n",
        "                              batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
        "    val_loader = DataLoader(ForgerySegDataset(val_auth, val_forg, MASK_DIR, transform=val_transforms),\n",
        "                            batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "model_seg = DinoSegmenter(encoder, processor).to(device)\n",
        "\n",
        "# Load pretrained weights if MODEL_LOC is specified\n",
        "if MODEL_LOC is not None and os.path.exists(MODEL_LOC):\n",
        "    model_seg.load_state_dict(torch.load(MODEL_LOC, map_location=device))\n",
        "    print(f\"âœ… Loaded pretrained model from: {MODEL_LOC}\")\n",
        "    # model_seg.eval()  # Do not set to eval mode if we are training\n",
        "else:\n",
        "    print(f\"âš ï¸ Warning: Model weights not found at '{MODEL_LOC}'. Model will use randomly initialized weights (if applicable).\")\n",
        "\n",
        "\n",
        "# --- Training Loop Implementation ---\n",
        "if train_loader and val_loader and len(train_loader.dataset) > 0 and len(val_loader.dataset) > 0:\n",
        "    print(\"\\nStarting model training...\")\n",
        "    # Define Loss Function and Optimizer\n",
        "    criterion = nn.BCEWithLogitsLoss() # For binary segmentation\n",
        "    optimizer = optim.Adam(model_seg.parameters(), lr=1e-5) # Fine-tune encoder as well\n",
        "    num_epochs = 5 # Example number of epochs\n",
        "    best_val_f1 = -1.0\n",
        "    best_model_path = \"best_model_segmenter.pt\"\n",
        "\n",
        "    # Helper function to calculate F1-score for validation\n",
        "    def calculate_f1_score(preds, masks):\n",
        "        # preds: logits, masks: ground truth (0/1)\n",
        "        probs = torch.sigmoid(preds)\n",
        "        # Threshold at 0.5 for binary prediction\n",
        "        binary_preds = (probs > 0.5).float()\n",
        "        # Flatten tensors for F1 score calculation\n",
        "        return f1_score(masks.cpu().numpy().flatten(), binary_preds.cpu().numpy().flatten(), zero_division=0)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model_seg.train() # Set model to training mode\n",
        "        running_loss = 0.0\n",
        "        for batch_idx, (images, masks) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1} Training\")):\n",
        "            images, masks = images.to(device), masks.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model_seg.forward_seg(images)\n",
        "            loss = criterion(outputs, masks)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = running_loss / len(train_loader)\n",
        "\n",
        "        # Validation phase\n",
        "        model_seg.eval() # Set model to evaluation mode\n",
        "        val_loss = 0.0\n",
        "        val_f1 = 0.0\n",
        "        with torch.no_grad():\n",
        "            for images, masks in tqdm(val_loader, desc=f\"Epoch {epoch+1} Validation\"):\n",
        "                images, masks = images.to(device), masks.to(device)\n",
        "                outputs = model_seg.forward_seg(images)\n",
        "                loss = criterion(outputs, masks)\n",
        "                val_loss += loss.item()\n",
        "                val_f1 += calculate_f1_score(outputs, masks)\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        avg_val_f1 = val_f1 / len(val_loader)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val F1: {avg_val_f1:.4f}\")\n",
        "\n",
        "        # Save best model\n",
        "        if avg_val_f1 > best_val_f1:\n",
        "            best_val_f1 = avg_val_f1\n",
        "            torch.save(model_seg.state_dict(), best_model_path)\n",
        "            print(f\"â­ New best model saved with Val F1: {best_val_f1:.4f}\")\n",
        "    print(\"Training complete.\")\n",
        "else:\n",
        "    print(\"\\nSkipping model training due to missing data or empty data loaders.\")\n",
        "\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def segment_prob_map(pil):\n",
        "    x = torch.from_numpy(np.array(pil.resize((IMG_SIZE, IMG_SIZE)), np.float32)/255.).permute(2,0,1)[None].to(device)\n",
        "    prob = torch.sigmoid(model_seg.forward_seg(x))[0,0].cpu().numpy()\n",
        "    return prob\n",
        "\n",
        "@torch.no_grad()\n",
        "def segment_prob_map_with_tta(pil):\n",
        "    # 1. Preprocessing: Resize, Normalize, and move to Device\n",
        "    # Use val_transforms for consistency in inference preparation\n",
        "    # pil_tensor = val_transforms(pil).to(device)\n",
        "    # The original code's image preparation for inference: manual resize and to_tensor conversion\n",
        "    x = torch.from_numpy(np.array(pil.resize((IMG_SIZE, IMG_SIZE)), np.float32)/255.).permute(2,0,1)[None].to(device)\n",
        "\n",
        "    predictions = []\n",
        "\n",
        "    # 2. Original Prediction\n",
        "    pred_orig = torch.sigmoid(model_seg.forward_seg(x))\n",
        "    predictions.append(pred_orig)\n",
        "\n",
        "    # 3. Horizontal Flip TTA (dim 3)\n",
        "    # Flip input -> Predict -> Flip output back\n",
        "    pred_h = torch.sigmoid(model_seg.forward_seg(torch.flip(x, dims=[3])))\n",
        "    predictions.append(torch.flip(pred_h, dims=[3]))\n",
        "\n",
        "    # 4. Vertical Flip TTA (dim 2)\n",
        "    # Flip input -> Predict -> Flip output back\n",
        "    pred_v = torch.sigmoid(model_seg.forward_seg(torch.flip(x, dims=[2])))\n",
        "    predictions.append(torch.flip(pred_v, dims=[2]))\n",
        "\n",
        "    # 5. Average the predictions and format as numpy\n",
        "    # We stack the 3 predictions and take the mean across the stack dimension (0)\n",
        "    prob = torch.stack(predictions).mean(0)[0, 0].cpu().numpy()\n",
        "\n",
        "    return prob\n",
        "\n",
        "def enhanced_adaptive_mask(prob, alpha_grad=0.45):\n",
        "    gx = cv2.Sobel(prob, cv2.CV_32F, 1, 0, ksize=3)\n",
        "    gy = cv2.Sobel(prob, cv2.CV_32F, 0, 1, ksize=3)\n",
        "    grad_mag = np.sqrt(gx**2 + gy**2)\n",
        "    grad_norm = grad_mag / (grad_mag.max() + 1e-6)\n",
        "    enhanced = (1 - alpha_grad) * prob + alpha_grad * grad_norm\n",
        "    enhanced = cv2.GaussianBlur(enhanced, (3,3), 0)\n",
        "    thr = np.mean(enhanced) + 0.3 * np.std(enhanced)\n",
        "    mask = (enhanced > thr).astype(np.uint8)\n",
        "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, np.ones((5,5), np.uint8))\n",
        "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, np.ones((3,3), np.uint8))\n",
        "    return mask, thr\n",
        "\n",
        "def finalize_mask(prob, orig_size):\n",
        "    mask, thr = enhanced_adaptive_mask(prob)\n",
        "    mask = cv2.resize(mask, orig_size, interpolation=cv2.INTER_NEAREST)\n",
        "    return mask, thr\n",
        "\n",
        "def pipeline_final(pil):\n",
        "    model_seg.eval() # Ensure model is in eval mode for inference\n",
        "    if USE_TTA:\n",
        "        prob = segment_prob_map_with_tta(pil)\n",
        "    else:\n",
        "        prob = segment_prob_map(pil)\n",
        "    mask, thr = finalize_mask(prob, pil.size)\n",
        "    area = int(mask.sum())\n",
        "    # The mask used here for mean calculation should be resized to IMG_SIZE for proper indexing into 'prob'\n",
        "    mask_for_mean = cv2.resize(mask,(IMG_SIZE,IMG_SIZE),interpolation=cv2.INTER_NEAREST)\n",
        "    mean_inside = float(prob[mask_for_mean==1].mean()) if area>0 else 0.0\n",
        "    if area < AREA_THR or mean_inside < MEAN_THR:\n",
        "        return \"authentic\", None, {\"area\": area, \"mean_inside\": mean_inside, \"thr\": thr}\n",
        "    return \"forged\", mask, {\"area\": area, \"mean_inside\": mean_inside, \"thr\": thr}\n",
        "\n",
        "import itertools\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def grid_search_area_mean(forg_paths, auth_paths, mask_dir):\n",
        "    # Return default values and print message if no valid data for grid search\n",
        "    if not forg_paths and not auth_paths:\n",
        "        print(\"Skipping grid search as validation data is not available.\")\n",
        "        return {'AREA_THR': 200, 'MEAN_THR': 0.22}\n",
        "\n",
        "    mean_range = [round(x, 2) for x in np.arange(0.20, 0.291, 0.01)]\n",
        "    area_range = [200]\n",
        "    val_set = [(p, \"forged\") for p in forg_paths] + [(p, \"authentic\") for p in auth_paths]\n",
        "\n",
        "    if not val_set:\n",
        "        print(\"Skipping grid search as validation set is empty.\")\n",
        "        return {'AREA_THR': 200, 'MEAN_THR': 0.22}\n",
        "\n",
        "    print(f\"ðŸš€ Step 1: Caching probability maps for ALL {len(val_set)} images...\")\n",
        "    cache = []\n",
        "    for p, label in tqdm(val_set):\n",
        "        try:\n",
        "            pil = Image.open(p).convert(\"RGB\")\n",
        "            w, h = pil.size\n",
        "\n",
        "            # Ensure model is in eval mode for caching inference\n",
        "            model_seg.eval()\n",
        "            prob = segment_prob_map_with_tta(pil) if USE_TTA else segment_prob_map(pil)\n",
        "\n",
        "            mask_raw, _ = enhanced_adaptive_mask(prob)\n",
        "            mask_resized = cv2.resize(mask_raw, (w, h), interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "            if label == \"forged\":\n",
        "                m_gt = np.load(Path(mask_dir)/f\"{Path(p).stem}.npy\")\n",
        "                if m_gt.ndim == 3: m_gt = np.max(m_gt, axis=0)\n",
        "                m_gt = (m_gt > 0).astype(np.uint8)\n",
        "            else:\n",
        "                m_gt = np.zeros((h, w), np.uint8)\n",
        "\n",
        "            cache.append({\"prob\": prob, \"mask\": mask_resized, \"gt\": m_gt, \"label\": label})\n",
        "        except FileNotFoundError as e:\n",
        "            print(f\"Skipping {p} due to FileNotFoundError during caching: {e}\")\n",
        "            continue\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping {p} due to unexpected error during caching: {e}\")\n",
        "            continue\n",
        "\n",
        "    if not cache: # If no items were successfully cached, return default params\n",
        "        print(\"No valid images found for grid search caching. Returning default parameters.\")\n",
        "        return {'AREA_THR': 200, 'MEAN_THR': 0.22}\n",
        "\n",
        "    best_f1 = -1\n",
        "    best_params = {}\n",
        "    combinations = list(itertools.product(area_range, mean_range))\n",
        "\n",
        "    for a_thr, m_thr in combinations:\n",
        "        current_f1s = []\n",
        "        for item in cache:\n",
        "            mask = item[\"mask\"]\n",
        "            area = int(mask.sum())\n",
        "\n",
        "            mask_small = cv2.resize(mask, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_NEAREST)\n",
        "            mean_in = float(item[\"prob\"][mask_small == 1].mean()) if area > 0 else 0.0\n",
        "\n",
        "            is_forged = (area >= a_thr and mean_in >= m_thr)\n",
        "            m_pred = (mask > 0).astype(np.uint8) if is_forged else np.zeros_like(item[\"gt\"])\n",
        "\n",
        "            f1 = f1_score(item[\"gt\"].flatten(), m_pred.flatten(),\n",
        "                          zero_division=1 if item[\"label\"] == \"authentic\" else 0)\n",
        "            current_f1s.append(f1)\n",
        "\n",
        "        if current_f1s: # Only calculate average if any F1 scores were computed\n",
        "            avg_f1 = np.mean(current_f1s)\n",
        "            if avg_f1 > best_f1:\n",
        "                best_f1 = avg_f1\n",
        "                best_params = {\"AREA_THR\": a_thr, \"MEAN_THR\": m_thr}\n",
        "                print(f\"â­ New Best F1: {best_f1:.4f} -> AREA: {a_thr}, MEAN: {m_thr}\")\n",
        "\n",
        "    if not best_params: # If no suitable parameters found, return defaults\n",
        "        print(\"No optimal parameters found during grid search. Returning default parameters.\")\n",
        "        return {'AREA_THR': 200, 'MEAN_THR': 0.22}\n",
        "\n",
        "    return best_params\n",
        "\n",
        "if GRID_SEARCH_ENABLED:\n",
        "    if val_forg or val_auth: # Only run grid search if there's actual validation data\n",
        "        print(\"Starting grid search for optimal parameters...\")\n",
        "        # Temporarily set model to eval mode for grid search inference if it was in train mode\n",
        "        current_model_mode = model_seg.training\n",
        "        model_seg.eval()\n",
        "        best_cfg = grid_search_area_mean(val_forg, val_auth, MASK_DIR)\n",
        "        AREA_THR = best_cfg['AREA_THR']\n",
        "        MEAN_THR = best_cfg['MEAN_THR']\n",
        "        print(f\"Final Grid Search Parameters: AREA_THR={AREA_THR}, MEAN_THR={MEAN_THR}\")\n",
        "        # Restore model mode\n",
        "        if current_model_mode: model_seg.train()\n",
        "    else:\n",
        "        print(\"Skipping grid search because validation data lists are empty.\")\n",
        "else:\n",
        "    print(\"Skipping grid search because GRID_SEARCH_ENABLED flag is False.\")\n",
        "\n",
        "\n",
        "# --- FURTHER MODIFICATION FOR VALIDATION F1 SCORE CALCULATION ---\n",
        "from sklearn.metrics import f1_score\n",
        "val_items = [(p, 1) for p in val_forg[:10]] # This will be empty if val_forg is empty\n",
        "results = []\n",
        "if val_items:\n",
        "    print(\"\\nCalculating F1-score for validation forged images:\")\n",
        "    # Ensure model is in eval mode for validation inference\n",
        "    current_model_mode = model_seg.training\n",
        "    model_seg.eval()\n",
        "    for p,_ in tqdm(val_items, desc=\"Validation forged-only\"):\n",
        "        try:\n",
        "            pil = Image.open(p).convert(\"RGB\")\n",
        "            label, m_pred, dbg = pipeline_final(pil)\n",
        "            m_gt = np.load(Path(MASK_DIR)/f\"{Path(p).stem}.npy\")\n",
        "            if m_gt.ndim==3: m_gt=np.max(m_gt,axis=0)\n",
        "            m_gt=(m_gt>0).astype(np.uint8)\n",
        "            m_pred=(m_pred>0).astype(np.uint8) if m_pred is not None else np.zeros_like(m_gt)\n",
        "            f1 = f1_score(m_gt.flatten(), m_pred.flatten(), zero_division=0)\n",
        "            results.append((Path(p).stem, f1, dbg))\n",
        "        except FileNotFoundError as e:\n",
        "            print(f\"Skipping {p} in validation F1 calculation due to FileNotFoundError: {e}\")\n",
        "            continue\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping {p} in validation F1 calculation due to unexpected error: {e}\")\n",
        "            continue\n",
        "    # Restore model mode\n",
        "    if current_model_mode: model_seg.train()\n",
        "\n",
        "    if results:\n",
        "        print(\"\\n F1-score par image falsifiÃ©e:\\n\")\n",
        "        for cid,f1,dbg in results:\n",
        "            print(f\"{cid} â€” F1={f1:.4f} | area={dbg['area']} mean={dbg['mean_inside']:.3f} thr={dbg['thr']:.3f}\")\n",
        "        print(f\"\\n Moyenne F1 (falsifiÃ©es) = {np.mean([r[1] for r in results]):.4f}\")\n",
        "    else:\n",
        "        print(\"\\n No results to display for validation F1-score (forged-only).\")\n",
        "else:\n",
        "    print(\"Skipping validation F1-score calculation (forged-only) because validation data is not available.\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Competition data not found at '/kaggle/input/recodai-luc-scientific-image-forgery-detection'. Skipping data loading, training setup, and grid search.\n",
            "Please ensure the 'recodai-luc-scientific-image-forgery-detection' dataset is available or adjust BASE_DIR.\n",
            "âœ… Loaded pretrained model from: cnndinov2_a78_model/model_seg_final.pt\n",
            "\n",
            "Skipping model training due to missing data or empty data loaders.\n",
            "Skipping grid search because GRID_SEARCH_ENABLED flag is False.\n",
            "Skipping validation F1-score calculation (forged-only) because validation data is not available.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19e41e05"
      },
      "source": [
        "## Evaluate and Refine Post-processing Thresholds\n",
        "\n",
        "### Subtask:\n",
        "Re-evaluate and potentially refine the AREA_THR and MEAN_THR parameters for optimal F1-score on the validation set using grid search, assuming the competition data is now available.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "226f576b"
      },
      "source": [
        "```markdown\n",
        "**Current Data Availability Status:**\n",
        "\n",
        "As noted in previous steps, the competition data (`recodai-luc-scientific-image-forgery-detection`) is still reported as *not found* at the specified `BASE_DIR` (`/kaggle/input/recodai-luc-scientific-image-forgery-detection`). The `kagglehub.competition_download` operation failed earlier with a `401 Client Error`, indicating permission issues or unaccepted competition rules.\n",
        "\n",
        "**Implications for this Subtask:**\n",
        "\n",
        "Because the training and validation data are not accessible, the `grid_search_area_mean` function (which is part of the `cdaf26f7` cell) will be skipped, and `AREA_THR` and `MEAN_THR` will retain their default values (200 and 0.22, respectively). The model training loop will also be skipped. The AI cannot resolve this data access issue automatically.\n",
        "\n",
        "**To successfully complete this subtask and allow the grid search to run, manual intervention is required:**\n",
        "\n",
        "1.  **Ensure Kaggle API Authentication**: Confirm your Kaggle API credentials are correctly configured and you have accepted the competition rules on the Kaggle website.\n",
        "2.  **Verify Data Download/Upload**: Ensure the dataset is either successfully downloaded via `kagglehub` or manually uploaded to your Colab environment, and that `BASE_DIR` points to its correct location.\n",
        "\n",
        "Despite the data unavailability, the following step will attempt to execute cell `cdaf26f7` as instructed. Please observe the output to confirm if the data issue persists and if the training/grid search components are indeed skipped.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65965ac1"
      },
      "source": [
        "**Reasoning**:\n",
        "As requested, I will now execute cell `cdaf26f7`. This cell contains the complete setup including the corrected `DinoSegmenter` class, data augmentation, and the training loop. Based on the previous data availability checks, the training and grid search components are expected to be skipped due to the missing `BASE_DIR`, but model loading should proceed successfully.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5abfac18",
        "outputId": "aef3957c-7920-49e8-e7bd-5dd9d5d4805c"
      },
      "source": [
        "import os, cv2, json, math, random, torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn, torch.nn.functional as F, torch.optim as optim\n",
        "from transformers import AutoImageProcessor, AutoModel\n",
        "import torchvision.transforms as transforms # Import torchvision transforms\n",
        "\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    # This forces CUDA to use deterministic algorithms (slower but consistent)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed_everything(42)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "BASE_DIR  = \"/kaggle/input/recodai-luc-scientific-image-forgery-detection\"\n",
        "AUTH_DIR  = f\"{BASE_DIR}/train_images/authentic\"\n",
        "FORG_DIR  = f\"{BASE_DIR}/train_images/forged\"\n",
        "MASK_DIR  = f\"{BASE_DIR}/train_masks\"\n",
        "TEST_DIR  = f\"{BASE_DIR}/test_images\"\n",
        "\n",
        "# Updated paths from previous extraction steps\n",
        "DINO_PATH = 'dinov2_base' # Updated to local extraction directory\n",
        "MODEL_LOC = 'cnndinov2_a78_model/model_seg_final.pt' # Corrected path based on directory listing\n",
        "\n",
        "IMG_SIZE = 518\n",
        "BATCH_SIZE = 2\n",
        "\n",
        "# INFERENCE UTILS\n",
        "AREA_THR = 200\n",
        "MEAN_THR = 0.22\n",
        "USE_TTA = True\n",
        "GRID_SEARCH_ENABLED = True # Renamed to avoid conflict if GRID_SEARCH is set to False later\n",
        "\n",
        "# --- Data Augmentation Transforms ---\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomVerticalFlip(p=0.5),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.ToTensor(), # Converts PIL Image to Tensor, normalizes to [0,1]\n",
        "    # No normalization here as the original code scales to /255. manually\n",
        "])\n",
        "\n",
        "val_transforms = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "class ForgerySegDataset(Dataset):\n",
        "    def __init__(self, auth_paths, forg_paths, mask_dir, transform=None):\n",
        "        self.samples = []\n",
        "        for p in forg_paths:\n",
        "            m = os.path.join(mask_dir, Path(p).stem + \".npy\")\n",
        "            if os.path.exists(m):\n",
        "                self.samples.append((p, m, 'forged')) # Added 'forged' label\n",
        "            else:\n",
        "                # If mask is missing for a forged image, treat as authentic for now\n",
        "                # Or skip, depending on desired behavior. For training, we need masks.\n",
        "                # Here, we skip if no mask for forged image (as it would be useless for segmentation training)\n",
        "                print(f\"Warning: Mask not found for forged image {p}. Skipping this sample.\")\n",
        "        for p in auth_paths:\n",
        "            self.samples.append((p, None, 'authentic')) # Added 'authentic' label\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self): return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, mask_path, label_type = self.samples[idx]\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        if mask_path is None: # Authentic image\n",
        "            # Create an empty mask for authentic images, scaled to IMG_SIZE\n",
        "            mask = np.zeros((IMG_SIZE, IMG_SIZE), np.uint8)\n",
        "        else: # Forged image with a mask\n",
        "            m = np.load(mask_path)\n",
        "            if m.ndim == 3: m = np.max(m, axis=0)\n",
        "            mask = (m > 0).astype(np.uint8)\n",
        "            # Resize mask to IMG_SIZE before applying transforms if any spatial transforms are used\n",
        "            mask = Image.fromarray(mask).resize((IMG_SIZE, IMG_SIZE), Image.NEAREST)\n",
        "            mask = np.array(mask)\n",
        "\n",
        "        # Apply image transformations\n",
        "        # Note: torchvision.transforms expects PIL Image or Tensor.\n",
        "        # Custom transforms for both image and mask should be handled carefully\n",
        "        # Here, we apply image transforms and then process mask for consistency.\n",
        "        # For random operations, it's better to combine image and mask transforms.\n",
        "        # For simplicity, we resize mask earlier and then apply 'ToTensor' logic.\n",
        "\n",
        "        # Convert image to tensor using transform\n",
        "        if self.transform:\n",
        "            img_t = self.transform(img)\n",
        "        else:\n",
        "            # Default processing if no transform (e.g., for validation/inference)\n",
        "            img_r = img.resize((IMG_SIZE, IMG_SIZE))\n",
        "            img_t = torch.from_numpy(np.array(img_r, np.float32)/255.).permute(2,0,1)\n",
        "\n",
        "        # Convert mask to tensor (already resized to IMG_SIZE in previous logic)\n",
        "        mask_t = torch.from_numpy(mask[None, ...].astype(np.float32))\n",
        "\n",
        "        return img_t, mask_t\n",
        "\n",
        "\n",
        "\n",
        "#  MODEL (DINOv2 + Decoder)\n",
        "\n",
        "from transformers import AutoImageProcessor, AutoModel\n",
        "processor = AutoImageProcessor.from_pretrained(DINO_PATH, local_files_only=True, use_fast=False)\n",
        "encoder = AutoModel.from_pretrained(DINO_PATH, local_files_only=True).eval().to(device)\n",
        "\n",
        "class DinoTinyDecoder(nn.Module):\n",
        "    def __init__(self, in_ch=768, out_ch=1):\n",
        "        super().__init__()\n",
        "        # Block 1: 768 -> 384\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, 384, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout2d(0.1)\n",
        "        )\n",
        "        # Block 2: 384 -> 192\n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.Conv2d(384, 192, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout2d(0.1)\n",
        "        )\n",
        "        # Block 3: 192 -> 96\n",
        "        self.block3 = nn.Sequential(\n",
        "            nn.Conv2d(192, 96, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        # Final Output: 96 -> 1\n",
        "        self.conv_out = nn.Conv2d(96, out_ch, kernel_size=1)\n",
        "\n",
        "    def forward(self, f, target_size):\n",
        "        # f: [B, 768, 37, 37]\n",
        "\n",
        "        # Step 1: Up to ~74x74\n",
        "        x = F.interpolate(self.block1(f), size=(74, 74), mode='bilinear', align_corners=False)\n",
        "\n",
        "        # Step 2: Up to ~148x148\n",
        "        x = F.interpolate(self.block2(x), size=(148, 148), mode='bilinear', align_corners=False)\n",
        "\n",
        "        # Step 3: Up to ~296x296\n",
        "        x = F.interpolate(self.block3(x), size=(296, 296), mode='bilinear', align_corners=False)\n",
        "\n",
        "        # Step 4: Final jump to 518x518\n",
        "        x = self.conv_out(x)\n",
        "        x = F.interpolate(x, size=target_size, mode='bilinear', align_corners=False)\n",
        "\n",
        "        return x\n",
        "\n",
        "class DinoSegmenter(nn.Module):\n",
        "    def __init__(self, encoder, processor):\n",
        "        super().__init__()\n",
        "        self.encoder, self.processor = encoder, processor\n",
        "        # Removed or commented out the line freezing encoder parameters to enable fine-tuning\n",
        "        # for p in self.encoder.parameters(): p.requires_grad = False\n",
        "        self.seg_head = DinoTinyDecoder(768,1)\n",
        "    def forward_features(self,x):\n",
        "        # Ensure input to processor is a list of PIL Images (or similar format it expects)\n",
        "        # x is already a tensor [B, C, H, W] ranging from 0-1 from ToTensor()\n",
        "        # Need to convert it back to a list of PIL images or numpy arrays for the processor.\n",
        "        # This part requires careful handling based on AutoImageProcessor's specific expectations.\n",
        "        # Assuming processor can take a list of numpy arrays [H,W,C] or PIL images.\n",
        "        # The `x*255.clamp(0,255).byte().permute(0,2,3,1).cpu().numpy()` converts a batch of float tensors to a batch of uint8 numpy arrays.\n",
        "        imgs = (x*255).clamp(0,255).byte().permute(0,2,3,1).cpu().numpy()\n",
        "        # The processor expects a list of images (PIL Image, NumPy array, or PyTorch tensor)\n",
        "        inputs = self.processor(images=[Image.fromarray(img) for img in imgs], return_tensors=\"pt\").to(x.device)\n",
        "\n",
        "        feats = self.encoder(**inputs).last_hidden_state\n",
        "        B,N,C = feats.shape\n",
        "        fmap = feats[:,1:,:].permute(0,2,1)\n",
        "        s = int(math.sqrt(N-1))\n",
        "        fmap = fmap.reshape(B,C,s,s)\n",
        "        return fmap\n",
        "    def forward_seg(self,x):\n",
        "        fmap = self.forward_features(x)\n",
        "        return self.seg_head(fmap,(IMG_SIZE,IMG_SIZE))\n",
        "\n",
        "\n",
        "# --- MODIFIED PART TO HANDLE MISSING BASE_DIR AND TRAINING LOOP ---\n",
        "# Initialize data-related variables as empty lists or None\n",
        "auth_imgs, forg_imgs, train_auth, val_auth, train_forg, val_forg = [], [], [], [], [], []\n",
        "train_loader, val_loader = None, None\n",
        "\n",
        "if not os.path.exists(BASE_DIR):\n",
        "    print(f\"Warning: Competition data not found at '{BASE_DIR}'. Skipping data loading, training setup, and grid search.\")\n",
        "    print(\"Please ensure the 'recodai-luc-scientific-image-forgery-detection' dataset is available or adjust BASE_DIR.\")\n",
        "    GRID_SEARCH_ENABLED = False # Disable grid search if data is not available\n",
        "else:\n",
        "    auth_imgs = sorted([str(Path(AUTH_DIR)/f) for f in os.listdir(AUTH_DIR)])\n",
        "    forg_imgs = sorted([str(Path(FORG_DIR)/f) for f in os.listdir(FORG_DIR)])\n",
        "    train_auth, val_auth = train_test_split(auth_imgs, test_size=0.2, random_state=42)\n",
        "    train_forg, val_forg = train_test_split(forg_imgs, test_size=0.2, random_state=42)\n",
        "\n",
        "    train_loader = DataLoader(ForgerySegDataset(train_auth, train_forg, MASK_DIR, transform=train_transforms),\n",
        "                              batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
        "    val_loader = DataLoader(ForgerySegDataset(val_auth, val_forg, MASK_DIR, transform=val_transforms),\n",
        "                            batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "model_seg = DinoSegmenter(encoder, processor).to(device)\n",
        "\n",
        "# Load pretrained weights if MODEL_LOC is specified\n",
        "if MODEL_LOC is not None and os.path.exists(MODEL_LOC):\n",
        "    model_seg.load_state_dict(torch.load(MODEL_LOC, map_location=device))\n",
        "    print(f\"âœ… Loaded pretrained model from: {MODEL_LOC}\")\n",
        "    # model_seg.eval()  # Do not set to eval mode if we are training\n",
        "else:\n",
        "    print(f\"âš ï¸ Warning: Model weights not found at '{MODEL_LOC}'. Model will use randomly initialized weights (if applicable).\")\n",
        "\n",
        "\n",
        "# --- Training Loop Implementation ---\n",
        "if train_loader and val_loader and len(train_loader.dataset) > 0 and len(val_loader.dataset) > 0:\n",
        "    print(\"\\nStarting model training...\")\n",
        "    # Define Loss Function and Optimizer\n",
        "    criterion = nn.BCEWithLogitsLoss() # For binary segmentation\n",
        "    optimizer = optim.Adam(model_seg.parameters(), lr=1e-5) # Fine-tune encoder as well\n",
        "    num_epochs = 5 # Example number of epochs\n",
        "    best_val_f1 = -1.0\n",
        "    best_model_path = \"best_model_segmenter.pt\"\n",
        "\n",
        "    # Helper function to calculate F1-score for validation\n",
        "    def calculate_f1_score(preds, masks):\n",
        "        # preds: logits, masks: ground truth (0/1)\n",
        "        probs = torch.sigmoid(preds)\n",
        "        # Threshold at 0.5 for binary prediction\n",
        "        binary_preds = (probs > 0.5).float()\n",
        "        # Flatten tensors for F1 score calculation\n",
        "        return f1_score(masks.cpu().numpy().flatten(), binary_preds.cpu().numpy().flatten(), zero_division=0)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model_seg.train() # Set model to training mode\n",
        "        running_loss = 0.0\n",
        "        for batch_idx, (images, masks) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1} Training\")):\n",
        "            images, masks = images.to(device), masks.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model_seg.forward_seg(images)\n",
        "            loss = criterion(outputs, masks)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = running_loss / len(train_loader)\n",
        "\n",
        "        # Validation phase\n",
        "        model_seg.eval() # Set model to evaluation mode\n",
        "        val_loss = 0.0\n",
        "        val_f1 = 0.0\n",
        "        with torch.no_grad():\n",
        "            for images, masks in tqdm(val_loader, desc=f\"Epoch {epoch+1} Validation\"):\n",
        "                images, masks = images.to(device), masks.to(device)\n",
        "                outputs = model_seg.forward_seg(images)\n",
        "                loss = criterion(outputs, masks)\n",
        "                val_loss += loss.item()\n",
        "                val_f1 += calculate_f1_score(outputs, masks)\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        avg_val_f1 = val_f1 / len(val_loader)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val F1: {avg_val_f1:.4f}\")\n",
        "\n",
        "        # Save best model\n",
        "        if avg_val_f1 > best_val_f1:\n",
        "            best_val_f1 = avg_val_f1\n",
        "            torch.save(model_seg.state_dict(), best_model_path)\n",
        "            print(f\"â­ New best model saved with Val F1: {best_val_f1:.4f}\")\n",
        "    print(\"Training complete.\")\n",
        "else:\n",
        "    print(\"\\nSkipping model training due to missing data or empty data loaders.\")\n",
        "\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def segment_prob_map(pil):\n",
        "    x = torch.from_numpy(np.array(pil.resize((IMG_SIZE, IMG_SIZE)), np.float32)/255.).permute(2,0,1)[None].to(device)\n",
        "    prob = torch.sigmoid(model_seg.forward_seg(x))[0,0].cpu().numpy()\n",
        "    return prob\n",
        "\n",
        "@torch.no_grad()\n",
        "def segment_prob_map_with_tta(pil):\n",
        "    # 1. Preprocessing: Resize, Normalize, and move to Device\n",
        "    # Use val_transforms for consistency in inference preparation\n",
        "    # pil_tensor = val_transforms(pil).to(device)\n",
        "    # The original code's image preparation for inference: manual resize and to_tensor conversion\n",
        "    x = torch.from_numpy(np.array(pil.resize((IMG_SIZE, IMG_SIZE)), np.float32)/255.).permute(2,0,1)[None].to(device)\n",
        "\n",
        "    predictions = []\n",
        "\n",
        "    # 2. Original Prediction\n",
        "    pred_orig = torch.sigmoid(model_seg.forward_seg(x))\n",
        "    predictions.append(pred_orig)\n",
        "\n",
        "    # 3. Horizontal Flip TTA (dim 3)\n",
        "    # Flip input -> Predict -> Flip output back\n",
        "    pred_h = torch.sigmoid(model_seg.forward_seg(torch.flip(x, dims=[3])))\n",
        "    predictions.append(torch.flip(pred_h, dims=[3]))\n",
        "\n",
        "    # 4. Vertical Flip TTA (dim 2)\n",
        "    # Flip input -> Predict -> Flip output back\n",
        "    pred_v = torch.sigmoid(model_seg.forward_seg(torch.flip(x, dims=[2])))\n",
        "    predictions.append(torch.flip(pred_v, dims=[2]))\n",
        "\n",
        "    # 5. Average the predictions and format as numpy\n",
        "    # We stack the 3 predictions and take the mean across the stack dimension (0)\n",
        "    prob = torch.stack(predictions).mean(0)[0, 0].cpu().numpy()\n",
        "\n",
        "    return prob\n",
        "\n",
        "def enhanced_adaptive_mask(prob, alpha_grad=0.45):\n",
        "    gx = cv2.Sobel(prob, cv2.CV_32F, 1, 0, ksize=3)\n",
        "    gy = cv2.Sobel(prob, cv2.CV_32F, 0, 1, ksize=3)\n",
        "    grad_mag = np.sqrt(gx**2 + gy**2)\n",
        "    grad_norm = grad_mag / (grad_mag.max() + 1e-6)\n",
        "    enhanced = (1 - alpha_grad) * prob + alpha_grad * grad_norm\n",
        "    enhanced = cv2.GaussianBlur(enhanced, (3,3), 0)\n",
        "    thr = np.mean(enhanced) + 0.3 * np.std(enhanced)\n",
        "    mask = (enhanced > thr).astype(np.uint8)\n",
        "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, np.ones((5,5), np.uint8))\n",
        "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, np.ones((3,3), np.uint8))\n",
        "    return mask, thr\n",
        "\n",
        "def finalize_mask(prob, orig_size):\n",
        "    mask, thr = enhanced_adaptive_mask(prob)\n",
        "    mask = cv2.resize(mask, orig_size, interpolation=cv2.INTER_NEAREST)\n",
        "    return mask, thr\n",
        "\n",
        "def pipeline_final(pil):\n",
        "    model_seg.eval() # Ensure model is in eval mode for inference\n",
        "    if USE_TTA:\n",
        "        prob = segment_prob_map_with_tta(pil)\n",
        "    else:\n",
        "        prob = segment_prob_map(pil)\n",
        "    mask, thr = finalize_mask(prob, pil.size)\n",
        "    area = int(mask.sum())\n",
        "    # The mask used here for mean calculation should be resized to IMG_SIZE for proper indexing into 'prob'\n",
        "    mask_for_mean = cv2.resize(mask,(IMG_SIZE,IMG_SIZE),interpolation=cv2.INTER_NEAREST)\n",
        "    mean_inside = float(prob[mask_for_mean==1].mean()) if area>0 else 0.0\n",
        "    if area < AREA_THR or mean_inside < MEAN_THR:\n",
        "        return \"authentic\", None, {\"area\": area, \"mean_inside\": mean_inside, \"thr\": thr}\n",
        "    return \"forged\", mask, {\"area\": area, \"mean_inside\": mean_inside, \"thr\": thr}\n",
        "\n",
        "import itertools\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def grid_search_area_mean(forg_paths, auth_paths, mask_dir):\n",
        "    # Return default values and print message if no valid data for grid search\n",
        "    if not forg_paths and not auth_paths:\n",
        "        print(\"Skipping grid search as validation data is not available.\")\n",
        "        return {'AREA_THR': 200, 'MEAN_THR': 0.22}\n",
        "\n",
        "    mean_range = [round(x, 2) for x in np.arange(0.20, 0.291, 0.01)]\n",
        "    area_range = [200]\n",
        "    val_set = [(p, \"forged\") for p in forg_paths] + [(p, \"authentic\") for p in auth_paths]\n",
        "\n",
        "    if not val_set:\n",
        "        print(\"Skipping grid search as validation set is empty.\")\n",
        "        return {'AREA_THR': 200, 'MEAN_THR': 0.22}\n",
        "\n",
        "    print(f\"ðŸš€ Step 1: Caching probability maps for ALL {len(val_set)} images...\")\n",
        "    cache = []\n",
        "    for p, label in tqdm(val_set):\n",
        "        try:\n",
        "            pil = Image.open(p).convert(\"RGB\")\n",
        "            w, h = pil.size\n",
        "\n",
        "            # Ensure model is in eval mode for caching inference\n",
        "            model_seg.eval()\n",
        "            prob = segment_prob_map_with_tta(pil) if USE_TTA else segment_prob_map(pil)\n",
        "\n",
        "            mask_raw, _ = enhanced_adaptive_mask(prob)\n",
        "            mask_resized = cv2.resize(mask_raw, (w, h), interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "            if label == \"forged\":\n",
        "                m_gt = np.load(Path(mask_dir)/f\"{Path(p).stem}.npy\")\n",
        "                if m_gt.ndim == 3: m_gt = np.max(m_gt, axis=0)\n",
        "                m_gt = (m_gt > 0).astype(np.uint8)\n",
        "            else:\n",
        "                m_gt = np.zeros((h, w), np.uint8)\n",
        "\n",
        "            cache.append({\"prob\": prob, \"mask\": mask_resized, \"gt\": m_gt, \"label\": label})\n",
        "        except FileNotFoundError as e:\n",
        "            print(f\"Skipping {p} due to FileNotFoundError during caching: {e}\")\n",
        "            continue\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping {p} due to unexpected error during caching: {e}\")\n",
        "            continue\n",
        "\n",
        "    if not cache: # If no items were successfully cached, return default params\n",
        "        print(\"No valid images found for grid search caching. Returning default parameters.\")\n",
        "        return {'AREA_THR': 200, 'MEAN_THR': 0.22}\n",
        "\n",
        "    best_f1 = -1\n",
        "    best_params = {}\n",
        "    combinations = list(itertools.product(area_range, mean_range))\n",
        "\n",
        "    for a_thr, m_thr in combinations:\n",
        "        current_f1s = []\n",
        "        for item in cache:\n",
        "            mask = item[\"mask\"]\n",
        "            area = int(mask.sum())\n",
        "\n",
        "            mask_small = cv2.resize(mask, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_NEAREST)\n",
        "            mean_in = float(item[\"prob\"][mask_small == 1].mean()) if area > 0 else 0.0\n",
        "\n",
        "            is_forged = (area >= a_thr and mean_in >= m_thr)\n",
        "            m_pred = (mask > 0).astype(np.uint8) if is_forged else np.zeros_like(item[\"gt\"])\n",
        "\n",
        "            f1 = f1_score(item[\"gt\"].flatten(), m_pred.flatten(),\n",
        "                          zero_division=1 if item[\"label\"] == \"authentic\" else 0)\n",
        "            current_f1s.append(f1)\n",
        "\n",
        "        if current_f1s: # Only calculate average if any F1 scores were computed\n",
        "            avg_f1 = np.mean(current_f1s)\n",
        "            if avg_f1 > best_f1:\n",
        "                best_f1 = avg_f1\n",
        "                best_params = {\"AREA_THR\": a_thr, \"MEAN_THR\": m_thr}\n",
        "                print(f\"â­ New Best F1: {best_f1:.4f} -> AREA: {a_thr}, MEAN: {m_thr}\")\n",
        "\n",
        "    if not best_params: # If no suitable parameters found, return defaults\n",
        "        print(\"No optimal parameters found during grid search. Returning default parameters.\")\n",
        "        return {'AREA_THR': 200, 'MEAN_THR': 0.22}\n",
        "\n",
        "    return best_params\n",
        "\n",
        "if GRID_SEARCH_ENABLED:\n",
        "    if val_forg or val_auth: # Only run grid search if there's actual validation data\n",
        "        print(\"Starting grid search for optimal parameters...\")\n",
        "        # Temporarily set model to eval mode for grid search inference if it was in train mode\n",
        "        current_model_mode = model_seg.training\n",
        "        model_seg.eval()\n",
        "        best_cfg = grid_search_area_mean(val_forg, val_auth, MASK_DIR)\n",
        "        AREA_THR = best_cfg['AREA_THR']\n",
        "        MEAN_THR = best_cfg['MEAN_THR']\n",
        "        print(f\"Final Grid Search Parameters: AREA_THR={AREA_THR}, MEAN_THR={MEAN_THR}\")\n",
        "        # Restore model mode\n",
        "        if current_model_mode: model_seg.train()\n",
        "    else:\n",
        "        print(\"Skipping grid search because validation data lists are empty.\")\n",
        "else:\n",
        "    print(\"Skipping grid search because GRID_SEARCH_ENABLED flag is False.\")\n",
        "\n",
        "\n",
        "# --- FURTHER MODIFICATION FOR VALIDATION F1 SCORE CALCULATION ---\n",
        "from sklearn.metrics import f1_score\n",
        "val_items = [(p, 1) for p in val_forg[:10]] # This will be empty if val_forg is empty\n",
        "results = []\n",
        "if val_items:\n",
        "    print(\"\\nCalculating F1-score for validation forged images:\")\n",
        "    # Ensure model is in eval mode for validation inference\n",
        "    current_model_mode = model_seg.training\n",
        "    model_seg.eval()\n",
        "    for p,_ in tqdm(val_items, desc=\"Validation forged-only\"):\n",
        "        try:\n",
        "            pil = Image.open(p).convert(\"RGB\")\n",
        "            label, m_pred, dbg = pipeline_final(pil)\n",
        "            m_gt = np.load(Path(MASK_DIR)/f\"{Path(p).stem}.npy\")\n",
        "            if m_gt.ndim==3: m_gt=np.max(m_gt,axis=0)\n",
        "            m_gt=(m_gt>0).astype(np.uint8)\n",
        "            m_pred=(m_pred>0).astype(np.uint8) if m_pred is not None else np.zeros_like(m_gt)\n",
        "            f1 = f1_score(m_gt.flatten(), m_pred.flatten(), zero_division=0)\n",
        "            results.append((Path(p).stem, f1, dbg))\n",
        "        except FileNotFoundError as e:\n",
        "            print(f\"Skipping {p} in validation F1 calculation due to FileNotFoundError: {e}\")\n",
        "            continue\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping {p} in validation F1 calculation due to unexpected error: {e}\")\n",
        "            continue\n",
        "    # Restore model mode\n",
        "    if current_model_mode: model_seg.train()\n",
        "\n",
        "    if results:\n",
        "        print(\"\\n F1-score par image falsifiÃ©e:\\n\")\n",
        "        for cid,f1,dbg in results:\n",
        "            print(f\"{cid} â€” F1={f1:.4f} | area={dbg['area']} mean={dbg['mean_inside']:.3f} thr={dbg['thr']:.3f}\")\n",
        "        print(f\"\\n Moyenne F1 (falsifiÃ©es) = {np.mean([r[1] for r in results]):.4f}\")\n",
        "    else:\n",
        "        print(\"\\n No results to display for validation F1-score (forged-only).\")\n",
        "else:\n",
        "    print(\"Skipping validation F1-score calculation (forged-only) because validation data is not available.\")\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Competition data not found at '/kaggle/input/recodai-luc-scientific-image-forgery-detection'. Skipping data loading, training setup, and grid search.\n",
            "Please ensure the 'recodai-luc-scientific-image-forgery-detection' dataset is available or adjust BASE_DIR.\n",
            "âœ… Loaded pretrained model from: cnndinov2_a78_model/model_seg_final.pt\n",
            "\n",
            "Skipping model training due to missing data or empty data loaders.\n",
            "Skipping grid search because GRID_SEARCH_ENABLED flag is False.\n",
            "Skipping validation F1-score calculation (forged-only) because validation data is not available.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec83f95f"
      },
      "source": [
        "### Subtask Completion Status: Failed - Data Not Available\n",
        "\n",
        "The subtask was to re-evaluate and refine `AREA_THR` and `MEAN_THR` parameters using grid search, assuming the competition data is now available.\n",
        "\n",
        "**Current Status:**\n",
        "\n",
        "*   The execution of cell `cdaf26f7` re-confirmed that the competition data at `BASE_DIR = '/kaggle/input/recodai-luc-scientific-image-forgery-detection'` is **still not found**.\n",
        "*   As a direct result of missing data, the data loading, model training, and the `grid_search_area_mean` function were all **skipped**, as indicated by the output:\n",
        "    *   `Warning: Competition data not found... Skipping data loading, training setup, and grid search.`\n",
        "    *   `Skipping model training due to missing data or empty data loaders.`\n",
        "    *   `Skipping grid search because GRID_SEARCH_ENABLED flag is False.`\n",
        "    *   `Skipping validation F1-score calculation (forged-only) because validation data is not available.`\n",
        "\n",
        "**Conclusion:**\n",
        "\n",
        "Since the grid search relies entirely on the availability of training and validation data, it could not be performed. Therefore, the `AREA_THR` and `MEAN_THR` parameters could not be re-evaluated or refined in this step.\n",
        "\n",
        "**Next Steps Required (Manual Intervention):**\n",
        "\n",
        "To successfully complete this subtask in the future, you *must* ensure the competition data (`recodai-luc-scientific-image-forgery-detection`) is accessible in the environment. This involves:\n",
        "\n",
        "1.  **Kaggle API Authentication & Rules Acceptance**: Re-verify your Kaggle API credentials are correctly set up and you have accepted the competition rules on the Kaggle website.\n",
        "2.  **Verify Data Download/Upload**: Ensure the dataset is either successfully downloaded via `kagglehub` or manually uploaded to your Colab environment, and that `BASE_DIR` points to its correct location.\n",
        "\n",
        "Once the data is accessible, re-execute the relevant code cells to allow the training process and the grid search for `AREA_THR` and `MEAN_THR` to run successfully."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cea7f491"
      },
      "source": [
        "### Subtask Completion Status: Failed - Data Not Available\n",
        "\n",
        "The subtask was to re-evaluate and refine `AREA_THR` and `MEAN_THR` parameters using grid search, assuming the competition data is now available.\n",
        "\n",
        "**Current Status:**\n",
        "\n",
        "*   The execution of cell `cdaf26f7` re-confirmed that the competition data at `BASE_DIR = '/kaggle/input/recodai-luc-scientific-image-forgery-detection'` is **still not found**.\n",
        "*   As a direct result of missing data, the data loading, model training, and the `grid_search_area_mean` function were all **skipped**, as indicated by the output:\n",
        "    *   `Warning: Competition data not found... Skipping data loading, training setup, and grid search.`\n",
        "    *   `Skipping model training due to missing data or empty data loaders.`\n",
        "    *   `Skipping grid search because GRID_SEARCH_ENABLED flag is False.`\n",
        "    *   `Skipping validation F1-score calculation (forged-only) because validation data is not available.`\n",
        "\n",
        "**Conclusion:**\n",
        "\n",
        "Since the grid search relies entirely on the availability of training and validation data, it could not be performed. Therefore, the `AREA_THR` and `MEAN_THR` parameters could not be re-evaluated or refined in this step.\n",
        "\n",
        "**Next Steps Required (Manual Intervention):**\n",
        "\n",
        "To successfully complete this subtask in the future, you *must* ensure the competition data (`recodai-luc-scientific-image-forgery-detection`) is accessible in the environment. This involves:\n",
        "\n",
        "1.  **Kaggle API Authentication & Rules Acceptance**: Re-verify your Kaggle API credentials are correctly set up and you have accepted the competition rules on the Kaggle website.\n",
        "2.  **Verify Data Download/Upload**: Ensure the dataset is either successfully downloaded via `kagglehub` or manually uploaded to your Colab environment, and that `BASE_DIR` points to its correct location.\n",
        "\n",
        "Once the data is accessible, re-execute the relevant code cells to allow the training process and the grid search for `AREA_THR` and `MEAN_THR` to run successfully."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27e27ff8"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   **Model Fine-tuning Enabled**: The DINOv2 encoder parameters within the `DinoSegmenter` model were successfully configured to be trainable by removing the parameter freezing line, allowing for fine-tuning during the training process.\n",
        "*   **Data Augmentation Integrated**: `torchvision.transforms` were successfully integrated into the `ForgerySegDataset` to apply data augmentation techniques, including random horizontal/vertical flips and color jitter, which would typically enhance model generalization during training.\n",
        "*   **Complete Training Loop Designed**: A comprehensive training loop was implemented, featuring `nn.BCEWithLogitsLoss` as the loss function, `optim.Adam` as the optimizer, and a mechanism to save the best model based on validation F1-score.\n",
        "*   **Persistent Data Inaccessibility**: The primary and recurring issue was the inability to access the competition dataset (`recodai-luc-scientific-image-forgery-detection`) at the specified `BASE_DIR`. Attempts to download the data via `kagglehub.competition_download` consistently resulted in a `401 Unauthorized` error or `FileNotFoundError`.\n",
        "*   **Skipped Training and Grid Search**: Due to the critical data unavailability, the actual model training, validation, and the intended grid search for `AREA_THR` and `MEAN_THR` parameters were consistently skipped across multiple execution attempts. The model was initialized and pretrained weights were loaded, but no further learning or parameter optimization could occur.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   **Resolve Data Access**: The immediate next step is to manually ensure the `recodai-luc-scientific-image-forgery-detection` dataset is accessible in the environment. This requires verifying Kaggle API credentials, accepting competition rules, and potentially manually uploading the data to the correct `BASE_DIR`.\n",
        "*   **Execute Full Pipeline**: Once the data is available, re-execute the complete pipeline to allow the model to train, apply data augmentation, and then perform the grid search to refine `AREA_THR` and `MEAN_THR` for optimal performance on the validation set.\n"
      ]
    }
  ]
}